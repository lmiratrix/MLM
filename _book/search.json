[
  {
    "objectID": "faraway_ex.html#r-setup",
    "href": "faraway_ex.html#r-setup",
    "title": "30  Faraway Example",
    "section": "30.1 R Setup",
    "text": "30.1 R Setup\n\nlibrary( arm )\nlibrary( ggplot2 )\nlibrary( plyr )\n\n# Install package from textbook to get the data by \n# running this line once.\n#install.packages( \"faraway\" )"
  },
  {
    "objectID": "faraway_ex.html#first-example",
    "href": "faraway_ex.html#first-example",
    "title": "30  Faraway Example",
    "section": "30.2 First Example",
    "text": "30.2 First Example\n\n# load the data\nlibrary(faraway)\ndata(psid)\nhead(psid)\n\n  age educ sex income year person\n1  31   12   M   6000   68      1\n2  31   12   M   5300   69      1\n3  31   12   M   5200   70      1\n4  31   12   M   6900   71      1\n5  31   12   M   7500   72      1\n6  31   12   M   8000   73      1\n\n# Make log-transform of income\npsid$log_income = with( psid, log( income + 100 ) )\n                            \n                            \n# Look at some plots\npsid.sub = subset( psid, person &lt; 21 )\nggplot( data=psid.sub, aes( x=year, y=income ) ) +\n    facet_wrap( ~ person ) +\n    geom_line()\n\n\n\nggplot( data=psid.sub, aes( x=year, y=log_income, group=person ) ) +\n    facet_wrap( ~ sex ) +\n    geom_line()\n\n\n\n# Simple regression on a single person\nlmod &lt;- lm( log_income ~ I(year-78), subset=(person==1), psid)\ncoef(lmod)\n\n (Intercept) I(year - 78) \n  9.40910950   0.08342068 \n\n# Now do linear regression on everyone\nsum.stat = ddply( psid, .(person), function( dat ) {\n    lmod &lt;- lm(log(income) ~ I(year-78), data=dat )\n    cc = coef(lmod)\n    names(cc) = c(\"intercept\",\"slope\")\n    c( cc, sex=dat$sex[[1]] )\n} )\nhead( sum.stat )\n\n  person intercept       slope sex\n1      1  9.399957  0.08426670   2\n2      2  9.819091  0.08281031   2\n3      3  7.893863  0.03131149   1\n4      4  7.853027  0.07585135   1\n5      5  8.033453 -0.04738677   1\n6      6  9.673443  0.08953380   2\n\nplot( slope ~ intercept, data=sum.stat, xlab=\"Intercept\",ylab=\"Slope\")\n\n\n\nboxplot( slope ~ sex, data=sum.stat )\n\n\n\n# Is rate of income growth different by sex?\nt.test( slope ~ sex, data=sum.stat )\n\n\n    Welch Two Sample t-test\n\ndata:  slope by sex\nt = 2.3786, df = 56.736, p-value = 0.02077\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n 0.00507729 0.05916871\nsample estimates:\nmean in group 1 mean in group 2 \n     0.08903346      0.05691046 \n\n# Is initial income different by sex?\nt.test( intercept ~ sex, data=sum.stat )\n\n\n    Welch Two Sample t-test\n\ndata:  intercept by sex\nt = -8.2199, df = 79.719, p-value = 3.065e-12\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -1.4322218 -0.8738792\nsample estimates:\nmean in group 1 mean in group 2 \n       8.229275        9.382325 \n\n# Fitting our model\nlibrary(lme4)\npsid$cyear &lt;- psid$year-78\nmmod &lt;- lmer(log(income) ~ cyear*sex + age + educ + (cyear|person), psid)\ndisplay(mmod)\n\nlmer(formula = log(income) ~ cyear * sex + age + educ + (cyear | \n    person), data = psid)\n            coef.est coef.se\n(Intercept)  6.67     0.54  \ncyear        0.09     0.01  \nsexM         1.15     0.12  \nage          0.01     0.01  \neduc         0.10     0.02  \ncyear:sexM  -0.03     0.01  \n\nError terms:\n Groups   Name        Std.Dev. Corr \n person   (Intercept) 0.53          \n          cyear       0.05     0.19 \n Residual             0.68          \n---\nnumber of obs: 1661, groups: person, 85\nAIC = 3839.8, DIC = 3751.2\ndeviance = 3785.5 \n\n# refit with the lmerTest library to get p-values\nlibrary( lmerTest )\nmmod &lt;- lmer(log(income) ~ cyear*sex + age + educ + (cyear|person), psid)\nsummary(mmod)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: log(income) ~ cyear * sex + age + educ + (cyear | person)\n   Data: psid\n\nREML criterion at convergence: 3819.8\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-10.2310  -0.2134   0.0795   0.4147   2.8254 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n person   (Intercept) 0.2817   0.53071      \n          cyear       0.0024   0.04899  0.19\n Residual             0.4673   0.68357      \nNumber of obs: 1661, groups:  person, 85\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)  6.674211   0.543323 81.176969  12.284  &lt; 2e-16 ***\ncyear        0.085312   0.008999 78.915123   9.480 1.14e-14 ***\nsexM         1.150312   0.121292 81.772542   9.484 8.06e-15 ***\nage          0.010932   0.013524 80.837433   0.808   0.4213    \neduc         0.104209   0.021437 80.722317   4.861 5.65e-06 ***\ncyear:sexM  -0.026306   0.012238 77.995359  -2.150   0.0347 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n           (Intr) cyear  sexM   age    educ  \ncyear       0.020                            \nsexM       -0.104 -0.098                     \nage        -0.874  0.002 -0.026              \neduc       -0.597  0.000  0.008  0.167       \ncyear:sexM -0.003 -0.735  0.156 -0.010 -0.011"
  },
  {
    "objectID": "faraway_ex.html#diagnostics",
    "href": "faraway_ex.html#diagnostics",
    "title": "30  Faraway Example",
    "section": "30.3 Diagnostics",
    "text": "30.3 Diagnostics\n\n# First add our residuals and fitted values to our original data\n# (We can do this since we have no missing data so the rows will line up\n# correctly)\npsid = transform( psid,  resid=resid( mmod ),\n                  fit = fitted( mmod ) )\nhead( psid )\n\n  age educ sex income year person log_income cyear       resid      fit\n1  31   12   M   6000   68      1   8.716044   -10  0.06719915 8.632316\n2  31   12   M   5300   69      1   8.594154    -9 -0.13201639 8.707478\n3  31   12   M   5200   70      1   8.575462    -8 -0.22622748 8.782641\n4  31   12   M   6900   71      1   8.853665    -7 -0.01852759 8.857804\n5  31   12   M   7500   72      1   8.935904    -6 -0.01030887 8.932967\n6  31   12   M   8000   73      1   8.999619    -5 -0.02093325 9.008130\n\n# Here is a qqplot for each sex\nggplot( data=psid ) +\n    facet_wrap( ~ sex ) +\n    stat_qq( aes( sample=resid ) )\n\n\n\n# If you want to add the lines, you have to do a little more work\nslopes = ddply( psid, .(sex), function( dat ) {\n    y &lt;- quantile(dat$resid, c(0.25, 0.75))\n    x &lt;- qnorm(c(0.25, 0.75))\n    slope &lt;- as.numeric( diff(y)/diff(x) )\n    int &lt;- y[[1]] - slope * x[[1]]\n    c( slope=slope, int=int )\n} )\nslopes\n\n  sex     slope        int\n1   F 0.4324568 0.10579138\n2   M 0.2473357 0.03321435\n\nggplot( data=psid ) +\n    facet_wrap( ~ sex ) +\n    stat_qq( aes( sample=resid ) ) +\n    geom_abline( data=slopes, aes( slope=slope, intercept=int ) )\n\n\n\n# This is doing it from the lattice pacage\nlibrary( lattice )\nqqmath(~resid(mmod) | sex, psid)\n\n\n\n# fancier with some lines.  The points should lie on the line\n# if we have normal residuals.  (We don't.)\nqqmath(~ resid(mmod)  | sex, data = psid,\n       panel = function(x, ...) {\n           panel.qqmathline(x, ...)\n           panel.qqmath(x, ...)\n       })\n\n\n\npsid$educ_levels = cut(psid$educ, c(0,8.5,12.5,20), labels=c( \"Less than HS\", \"HS\", \"Beyond HS\" ) )\nggplot( data=psid, aes( x=fit, y=resid ) ) +\n    facet_wrap( ~ educ_levels ) +\n    geom_point()"
  },
  {
    "objectID": "perf_ex.html#load-the-data",
    "href": "perf_ex.html#load-the-data",
    "title": "31  Example of a three-level model of clustered data",
    "section": "31.1 Load the data",
    "text": "31.1 Load the data\nWe first load the data. This is a dataset extensively discussed in Rabe-Hesketh and Skrondal. I am replicating the model they propose in chapter 8.4. This story is as follows: the data set is a collection of measurements for a test-retest of two peak expiratory flow measurement devices (in English, patients were told to exhale into a device to measure their lung capacity, and they did so twice for two different measurement devices). We want to understand whether the types of meter are different, and also understand variation in subjects lung capacities, and variation in the measurement error of the meters.\nIn the following we load the data and look at the first few lines. We see that each subject had two measurements from the standard and the mini Wright flow meter.\n\npefr = read.dta( \"data/pefr.dta\" )\n\nhead( pefr )\n\n  id wp1 wp2 wm1 wm2\n1  1 494 490 512 525\n2  2 395 397 430 415\n3  3 516 512 520 508\n4  4 434 401 428 444\n5  5 476 470 500 500\n6  6 557 611 600 625\n\n\nWe are going to view this as three-level data. We have multiple measurements nested inside device type nested inside subject. We might imagine that different subjects have different lung capacities. We also might imagine that different subjects are going to have different biases when using the two different meters. The two observations for each meter allows us to understand the variability of measurements for a single meter for a given subject, and looking at how these vary across subjects allows us to understand how much the biases move across individuals."
  },
  {
    "objectID": "perf_ex.html#reshape-the-data-optional-section",
    "href": "perf_ex.html#reshape-the-data-optional-section",
    "title": "31  Example of a three-level model of clustered data",
    "section": "31.2 Reshape the data (Optional section)",
    "text": "31.2 Reshape the data (Optional section)\nThis section illustrates some advanced reshaping techniques. In particular we reshape the data twice to deal with the time and the device as different levels.\nHere we go:\n\ndat = reshape( pefr, direction=\"long\", idvar = \"id\", \n               varying=list( c(\"wp1\",\"wm1\"), c(\"wp2\",\"wm2\") ),\n               times=c(\"wp\",\"wm\"),\n               timevar=\"device\",\n               v.names=c(\"time1\",\"time2\") )\n\nLet’s see what we got:\n\nhead( dat )\n\n     id device time1 time2\n1.wp  1     wp   494   490\n2.wp  2     wp   395   397\n3.wp  3     wp   516   512\n4.wp  4     wp   434   401\n5.wp  5     wp   476   470\n6.wp  6     wp   557   611\n\nsubset( dat, id==1 )\n\n     id device time1 time2\n1.wp  1     wp   494   490\n1.wm  1     wm   512   525\n\n\nThe second line above shows us our first person now as two new lines, one for each device. We see the measurements correspond to the first row of the original pefr data.\nNow we have a row for each person for each device. Next we unstack the time (and then look at what we got):\n\ndat = reshape( dat, direction=\"long\", idvar=c(\"id\",\"device\"),\n               varying=list( c(\"time1\",\"time2\") ),\n               v.names=c(\"flow\") )\nhead( dat )\n\n       id device time flow\n1.wp.1  1     wp    1  494\n2.wp.1  2     wp    1  395\n3.wp.1  3     wp    1  516\n4.wp.1  4     wp    1  434\n5.wp.1  5     wp    1  476\n6.wp.1  6     wp    1  557\n\n\nWe look at our second person to see if the measurements have the appropriate labels. They do.\n\nsubset( dat, id==2 )\n\n       id device time flow\n2.wp.1  2     wp    1  395\n2.wm.1  2     wm    1  430\n2.wp.2  2     wp    2  397\n2.wm.2  2     wm    2  415\n\nsubset( pefr, id==2 )\n\n  id wp1 wp2 wm1 wm2\n2  2 395 397 430 415\n\n\nAnother sanity check:\n\ntable( dat$id )\n\n\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 \n 4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4 \n\n\nWe have four measurements, still, for each person.\nWhen reshaping data, one typically has to fiddle with all of the commands and check the results a few times to get it right."
  },
  {
    "objectID": "perf_ex.html#plot-the-data",
    "href": "perf_ex.html#plot-the-data",
    "title": "31  Example of a three-level model of clustered data",
    "section": "31.3 Plot the data",
    "text": "31.3 Plot the data\nWe can look at the data. The following illustrates getting different colors and symbols depending on covariate information:\n\ndat$id = as.factor( dat$id )\ndat$device = as.factor( dat$device )\ndat$time = as.factor( dat$time )\nggplot( data=dat, aes( x=id, y=flow, col=device, pch=time ) ) + \n    geom_jitter( width=0.2, height=0 )"
  },
  {
    "objectID": "perf_ex.html#the-mathematical-model",
    "href": "perf_ex.html#the-mathematical-model",
    "title": "31  Example of a three-level model of clustered data",
    "section": "31.4 The mathematical model",
    "text": "31.4 The mathematical model\nLevel 1: We have for individual \\(i\\) using machine \\(j\\) at time \\(t\\): [ Y_{ijt} = +* t + _{ijt} ] The \\(\\beta_{1}\\) allows for a time effect of the second measurement being systematically lower or higher than the first. We pool this across all subjects and machines.\nLevel 2: Our machine-level intercepts for each subject are [ =* + D_j + u*{ij} ] with \\(D_j = 1\\{ j = wp \\}\\) being an indicator (dummy variable) for the second machine. The \\(\\gamma_1\\) allows a systematic bias for the two machines (so the wp machine could tend to give larger readings than the wm machine, for example). Overall, the above says each machine expected reading varies around the subject’s lung capacity, but that these expected readings will vary around the subjects true capacity by the \\(u_{ij}\\). Actual readings for subject \\(i\\) on machine \\(j\\) will hover around \\(\\beta_{ij}\\) if we had the subject test over and over, according to our model (not including fatigue captured by the time coefficient).\nLevel 3: Finally our subject intercepts are [ =* + w{i} . ] The overall population lung capacity is \\(\\mu\\). Subjects have larger or smaller lung capacity depending on their \\(w_{i}\\).\nThe \\(u_{ij}\\) and \\(w_i\\) are each normally distributed, and independent from each other.\nThe \\(w_i\\) are how the subjects vary (i.e., their different lung capacities). The \\(u_{ij}\\) are the individual biases of a machine for a given subject. Looking at our plot, we see that subjects vary a lot, and machines vary sometimes within a subject (the centers of the pairs of colored points tend to be close, but not always), and the residual variance tends to be small (colored points are close together)."
  },
  {
    "objectID": "perf_ex.html#fit-the-model",
    "href": "perf_ex.html#fit-the-model",
    "title": "31  Example of a three-level model of clustered data",
    "section": "31.5 Fit the model",
    "text": "31.5 Fit the model\n\nlibrary( lme4 )\nM1 = lmer( flow ~ device + time + (1|id) + (1|device:id), data=dat )\ndisplay( M1 )\n\nlmer(formula = flow ~ device + time + (1 | id) + (1 | device:id), \n    data = dat)\n            coef.est coef.se\n(Intercept) 454.43    27.84 \ndevicewp     -6.03     8.05 \ntime2        -1.03     4.37 \n\nError terms:\n Groups    Name        Std.Dev.\n device:id (Intercept)  19.72  \n id        (Intercept) 111.99  \n Residual               18.01  \n---\nnumber of obs: 68, groups: device:id, 34; id, 17\nAIC = 682.8, DIC = 709.1\ndeviance = 689.9 \n\n\nNow let’s connect some pieces:\n\nThe main effects estimate \\(\\mu = 455.46\\) and \\(\\gamma_1 = -6.03\\) and \\(\\beta_1 = -1.03\\).\nThe z-score of \\(z = -6.03 / 8.05 &lt; 1\\) means there is no evidence of systematic bias of one machine compared to the other.\nThe estimated standard deviation of actual lung capacity is 112.\nThe estimated standard deviation of how two different machines will measure the same person is \\(19.72\\). Different machines will tend to give different average measurements for the same subject.\nThe estimated standard deviation of how much a repeated measurement of the same machine on the same person will vary is 18. The machines are relatively precise, given the variation in the population.\nThe amount of variance explained by lung variation is \\(112^2 / (19.72^2 + 111.99^2 + 18.01^2) = 0.94636\\), i.e., most of it."
  },
  {
    "objectID": "perf_ex.html#appendix-optional-base-plot-package",
    "href": "perf_ex.html#appendix-optional-base-plot-package",
    "title": "31  Example of a three-level model of clustered data",
    "section": "31.6 Appendix (Optional): base plot package",
    "text": "31.6 Appendix (Optional): base plot package\nHere is how to build the plot without ggplot:\n\nplot( flow ~ as.numeric(id), data=dat, pch=ifelse( time==1, 21, 22 ),\n      col=ifelse( device==\"wp\", \"red\", \"green\" ) )\nlegend( \"bottomleft\", legend=c(\"WP-1\", \"WP-2\",\"WM-1\", \"WM-2\"),\n        pch=c(21,22,21,22),\n        col=c(\"red\",\"red\",\"green\",\"green\") )"
  },
  {
    "objectID": "kenya_ex.html#load-the-data",
    "href": "kenya_ex.html#load-the-data",
    "title": "32  Example of a three-level longitudinal model",
    "section": "32.1 Load the data",
    "text": "32.1 Load the data\nWe first load the data. Shoving a lot of things under the rug, we have five measurements on a collection of kids in Kenya across time. We are interested in the impact of improved nutrition. The children are clustered in schools. This gives a three-level structure. The schools were treated with different nutrition programs.\nIn the following we load the data and look at the first few lines. Lots of variables! The main ones are id (the identifier of the kid), treatment (the kind of treatment given to the school), schoolid (the identifier of the school), gender (the gender of the kid), and rn (the time variable). Our outcome is ravens (Raven’s colored progressive matrices asssessment).\n\nkenya = read.dta( \"data/kenya.dta\" )\n\n# look at first 9 variables\nhead( kenya[1:9], 3 )\n\n  id schoolid rn relyear ravens arithmetic vmeaning dstotal age_at_time0\n1  1        2  1   -0.15     15          5       25       6         7.19\n2  1        2  2    0.14     19          7       39       8         7.19\n3  1        2  3    0.46     21          7       33       7         7.19\n\n# what times do we have?\ntable( kenya$rn ) #time\n\n\n  1   2   3   4   5 \n546 546 546 546 546 \n\nlength( unique( kenya$id ) )\n\n[1] 546\n\nlength( unique( kenya$schoolid) )\n\n[1] 12\n\n\nWe see we have 546 kids and 12 schools."
  },
  {
    "objectID": "kenya_ex.html#plot-the-data",
    "href": "kenya_ex.html#plot-the-data",
    "title": "32  Example of a three-level longitudinal model",
    "section": "32.2 Plot the data",
    "text": "32.2 Plot the data\nWe can look at the data.\n\nggplot( data=kenya, aes( x=rn, y=ravens, group=id )  )+ \n            facet_wrap( ~ gender ) + \n            geom_line( alpha=0.3 )\n\nWarning: Removed 114 rows containing missing values (`geom_line()`).\n\n\n\n\n\nor\n\nggplot( data=kenya, aes( x=rn, y=ravens, group=id )  )+ \n            facet_wrap( ~ schoolid ) + \n            geom_line( alpha=0.3 )\n\nWarning: Removed 114 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\nid.sub = sample( unique( kenya$id), 12 )\nken.sub = subset( kenya, id %in% id.sub )\nggplot( data=ken.sub, aes( x=rn, y=ravens, group=id )  )+ \n            facet_wrap( ~ id ) + \n            geom_line( alpha=0.3 )\n\n\n\n\nWe have lots of noise! But there is also a trend.\nThe progression of marginal means show there is growth over time, on average:\n\nmosaic::favstats( ravens ~ rn, data=kenya )\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\n\n  rn min Q1 median Q3 max mean   sd   n missing\n1  1   1 16     17 19  28 17.3 2.56 537       9\n2  2   0 16     17 19  28 17.7 2.79 529      17\n3  3   0 16     18 20  30 18.3 3.04 523      23\n4  4   4 17     18 20  30 18.6 2.97 513      33\n5  5   7 17     19 21  31 19.5 3.10 496      50\n\n\n(Using the mosaic package we can do this.)\nThe above also shows that we have some missing data, more as the study progresses.\nWe drop these missing observations:\n\nkenya = subset( kenya, !is.na( ravens ) & !is.na( rn ) )\n\nWe have some treatments, which we order so control is first\n\nstr( kenya$treatment )\n\n Factor w/ 4 levels \"meat\",\"milk\",..: 1 1 1 1 1 4 4 4 4 4 ...\n\nlevels( kenya$treatment )\n\n[1] \"meat\"    \"milk\"    \"calorie\" \"control\"\n\nkenya$treatment = relevel( kenya$treatment, ref = \"control\" )\nlevels( kenya$treatment )\n\n[1] \"control\" \"meat\"    \"milk\"    \"calorie\""
  },
  {
    "objectID": "kenya_ex.html#the-mathematical-model",
    "href": "kenya_ex.html#the-mathematical-model",
    "title": "32  Example of a three-level longitudinal model",
    "section": "32.3 The mathematical model",
    "text": "32.3 The mathematical model\nLet’s fit a random slope model.\nLevel 1: We have for individual \\(i\\) in school \\(j\\) at time \\(t\\): [ Y_{ijt} = +* (t-L) + _{ijt} ]\nLevel 2: Each individual has their own growth curve. Their curve’s slope and intercepts varies around the school means: [ =* + gender{ij} + u_{0ij} ] [ = + gender*{ij} + u_{1ij} ] We also have that \\((u_{0ij}, u_{1ij})\\) are normally distributed with some 2x2 covariance matrix. We are forcing the impact of gender to be constant across schools.\nLevel 3: Finally our school mean slope and intercepts are [ =* + w_{0i} ] [ =* + meat_j +* milk_j + calorie_j + w*{1i} ] For the rate of growth at a school we allow different slopes for different treatments (compared to baseline). The milk, meat, and calorie are the three different treatments applied. We also have that \\((w_{0j}, w_{1j})\\) are normally distributed with some 2x2 covariance matrix: [\n\\[\\begin{pmatrix}w_{j0}\\\\\nw_{j1}\n\\end{pmatrix}\\]\nN((\n\\[\\begin{array}{c}\n0 \\\\\n0\n\\end{array}\\]\n), ) = N((\n\\[\\begin{array}{c}\n0 \\\\\n0\n\\end{array}\\]\n), _{sch} ) ]\nThe \\(\\mu_0\\) and \\(\\mu_1\\) are the slope and intercept for the overall population growth (this is what defines our marginal model).\nWe will use \\(L = 1\\) to center the data at the first time point (so our intercept is expected ravens score at onset of the study).\nConceptual question: Why do we not have treatment in the intercept for school? What would changing \\(L\\) do to our model and this reasoning?"
  },
  {
    "objectID": "kenya_ex.html#fit-the-model",
    "href": "kenya_ex.html#fit-the-model",
    "title": "32  Example of a three-level longitudinal model",
    "section": "32.4 Fit the model",
    "text": "32.4 Fit the model\n\nlibrary( lme4 )\nkenya$rn = kenya$rn - 1 # center by L=1\nM1 = lmer( ravens ~ 1 + rn + gender*rn + treatment:rn + (1+rn|schoolid) + (1+rn|id:schoolid), \n           data=kenya )\ndisplay( M1 )\n\nlmer(formula = ravens ~ 1 + rn + gender * rn + treatment:rn + \n    (1 + rn | schoolid) + (1 + rn | id:schoolid), data = kenya)\n                    coef.est coef.se\n(Intercept)         17.41     0.19  \nrn                   0.59     0.08  \ngendergirl          -0.30     0.20  \nrn:gendergirl       -0.14     0.08  \nrn:treatmentmeat     0.17     0.09  \nrn:treatmentmilk    -0.13     0.09  \nrn:treatmentcalorie -0.02     0.09  \n\nError terms:\n Groups      Name        Std.Dev. Corr  \n id:schoolid (Intercept) 1.40           \n             rn          0.43     -0.09 \n schoolid    (Intercept) 0.45           \n             rn          0.09     -0.99 \n Residual                2.31           \n---\nnumber of obs: 2598, groups: id:schoolid, 546; schoolid, 12\nAIC = 12545.9, DIC = 12474\ndeviance = 12496.0 \n\n\nNow let’s connect some pieces:\n\n\\(\\mu_{00} = 17.41\\) and \\(\\mu_{11} = 0.59\\). The initial score for boys is 17.4, on average, with an average gain of 0.59 per year for control schools.\n\\(\\gamma_{01} = -0.30\\) and \\(\\gamma_{11} = -0.14\\), giving estimates that girls score lower and gain slower than boys.\nThe school-level variation in initial expected Raven scores is 0.45 (this is the standard deviation of \\(w_{0i}\\)), relatively small compared to the individual variation of 1.40 (this is the standard deviation of \\(u_{0ij}\\)).\nThe correlation of the \\(u_{0ij}\\) and \\(u_{1ij}\\) is basically zero (estimated at -0.09).\nThe random effects for school has a covariance matrix \\(\\Sigma_{sch}\\) of [ _{sch} = ] The very negative correlation suggests an extrapolation effect, and that perhaps we could drop the random slope for schools.\nThe treatment effects are estimated as \\(\\mu_{11}=0.17, \\mu_{12}=-0.13\\), and \\(\\mu_{13}=-0.02\\).\n\nP-values for these will not be small, however, as the standard errors are all 0.09.\n\nWe could try to look at uncertainty on our parameters using the confint( M1 ) command, but it turns out that it crashes for this model. This can happen, and our -0.99 correlation gives a hint as to why. Let’s first drop the random slope and then try:\n\nM1B = lmer( ravens ~ rn + gender*rn + treatment:rn + (1|schoolid) + (1+rn|id:schoolid), \n           data=kenya )\ndisplay( M1B )\n\nlmer(formula = ravens ~ rn + gender * rn + treatment:rn + (1 | \n    schoolid) + (1 + rn | id:schoolid), data = kenya)\n                    coef.est coef.se\n(Intercept)         17.39     0.17  \nrn                   0.57     0.08  \ngendergirl          -0.30     0.20  \nrn:gendergirl       -0.14     0.08  \nrn:treatmentmeat     0.22     0.10  \nrn:treatmentmilk    -0.09     0.10  \nrn:treatmentcalorie  0.02     0.10  \n\nError terms:\n Groups      Name        Std.Dev. Corr  \n id:schoolid (Intercept) 1.42           \n             rn          0.44     -0.11 \n schoolid    (Intercept) 0.33           \n Residual                2.31           \n---\nnumber of obs: 2598, groups: id:schoolid, 546; schoolid, 12\nAIC = 12544.4, DIC = 12478\ndeviance = 12498.9 \n\nconfint( M1B )\n\n                      2.5 %   97.5 %\n.sig01               1.1657  1.65436\n.sig02              -0.3545  0.32858\n.sig03               0.3076  0.54180\n.sig04               0.0000  0.60033\n.sigma               2.2312  2.39580\n(Intercept)         17.0605 17.71696\nrn                   0.4091  0.72814\ngendergirl          -0.6870  0.09145\nrn:gendergirl       -0.2879  0.00772\nrn:treatmentmeat     0.0164  0.40811\nrn:treatmentmilk    -0.2876  0.09811\nrn:treatmentcalorie -0.1775  0.20453\n\n\nWe then have to puzzle out which confidence interval goes with what. The .sig01 is the variance of the kid (id:schoolid), which we can tell by the range it covers. Then the next must be correlation, and then the slope. This tells us we have no confidence the school random intercept is away from 0 (.sig04)."
  },
  {
    "objectID": "kenya_ex.html#some-quick-plots",
    "href": "kenya_ex.html#some-quick-plots",
    "title": "32  Example of a three-level longitudinal model",
    "section": "32.5 Some quick plots",
    "text": "32.5 Some quick plots\nWe can look at the empirical bayes intercepts:\n\nschools = data.frame( resid = ranef( M1 )$schoolid$`(Intercept)` )\nkids = data.frame( resid = ranef( M1 )$id$`(Intercept)` )\nresid = data.frame( resid = resid( M1 ) )\nresids = bind_rows( school=schools, child=kids, residual=resid, .id=\"type\" )\nresids$type = factor( resids$type, levels = c(\"school\",\"child\", \"residual\" ) )\n\nggplot( resids, aes( x = type, y = resid ) ) +\n  geom_boxplot() +\n  coord_flip()\n\n\n\n\nThis shows that the variation in occasion is much larger than kid, which is much larger than school.\nWe can calculate all the individual growth curves and plot those:\n\nkenya$predictions = predict( M1 )\nggplot( data=kenya, aes( x=rn, y=predictions, group=id ) ) +\n    facet_wrap( ~ schoolid, labeller = label_both) +\n    geom_line( alpha=0.3 )\n\n\n\n\nGenerally individual curves are estimated to have positive slopes. The schools visually look quite similar; any school variation is small compared to individual variation."
  },
  {
    "objectID": "derivations.html",
    "href": "derivations.html",
    "title": "Derivations and Technical Details",
    "section": "",
    "text": "This section contains derivations and other technical details."
  },
  {
    "objectID": "icc_derivation.html",
    "href": "icc_derivation.html",
    "title": "37  ICC Derivation",
    "section": "",
    "text": "Often, the ICC is described as the estimated correlation between pairs of points drawn from the same cluster. While you can look at the visualizer to get some intuition on what this means, here is a short proof adapted from S52 materials.\nConsider the variance components model:\n\\[\ny_{ij} = \\beta_0 + \\zeta_j + \\varepsilon_{ij}\n\\]\nThe correlation between an observation \\(y\\) and an observation from the same group \\(y'\\) is the standardized covariance:\n\\[\n\\rho(y, y') = \\frac{cov(y,y')}{\\sqrt{var(y)var(y')}}\n\\]\nWe can expand the numerator, the covariance between \\(y\\) and \\(y'\\) and substitute in the definition of \\(y\\) from our model:\n\\[\ncov(y,y') = cov(\\beta_0 + \\zeta_j + \\varepsilon_{ij}, \\beta_0 + \\zeta_j + \\varepsilon'_{ij})\n\\]\nBy definition, \\(\\beta_0\\) is the same for everyone (i.e., the “constant” term), and \\(\\zeta_j\\) will be the same for both observations because we are looking within a single cluster. The only difference between the two groups are the individual level error terms, \\(\\varepsilon_{ij}\\). The rules of covariance tell us that the constant drops out and the \\(\\varepsilon\\) too because it is independent of \\(\\zeta_j\\), we can simplify our equation:\n\\[\ncov(y,y') = cov(\\zeta_j, \\zeta_j)\n\\]\nThe covariance of a variable with itself is the variance:\n\\[\ncov(y,y') = cov(\\zeta_j, \\zeta_j) = var(\\zeta_j) = \\sigma^2_\\zeta\n\\]\nConceptually, the \\(\\zeta_j\\) represents the shared influences on \\(y\\) that would cause the similarity between observations in the same group.\nWe know from our variance decomposition in the ICC formula that \\(var(y)\\) is the sum of the between-group and within-group variance components:\n\\[\nvar(y) = var(y') = \\sigma^2_\\zeta + \\sigma^2_\\varepsilon\n\\]\nWe can substitute these quantities into the original formula:\n\\[\n\\rho(y, y') = \\frac{cov(y,y')}{\\sqrt{var(y)var(y')}} = \\frac{\\sigma^2_\\zeta}{\\sigma^2_\\zeta + \\sigma^2_\\varepsilon} = ICC\n\\] Thus, the ICC is both the proportion of total variance accounted for by group membership and the correlation between pairs of observations drawn from the same group. QED!"
  },
  {
    "objectID": "cov_matrix_derivation.html#calculating-the-delta_tt",
    "href": "cov_matrix_derivation.html#calculating-the-delta_tt",
    "title": "38  Covariance Derivation",
    "section": "Calculating the \\(\\delta_{tt'}\\)",
    "text": "Calculating the \\(\\delta_{tt'}\\)\nLet’s calculate \\(\\delta_{13} = cov( \\epsilon_{i1}, \\epsilon_{i2} )\\).\nFirst we need a math fact about random quantities \\(A\\), \\(B\\), and \\(C\\): \\[cov( A + B, C ) = cov( A, C ) + cov( B, C ) .\\] Also if you multiply something by a constant \\(k\\) you have \\[cov( k_1 A, k_2 B ) = k_1 k_2 cov( A, B ) .\\]\nAlso note that \\(a_1 = 0\\) and \\(a_3 = 2\\). Then we have: \\[\\begin{aligned}\n\\delta_{13} &= cov( \\epsilon_{i1}, \\epsilon_{i3} ) \\\\\n   &= cov(  u_{0i} + u_{1i} a_1 + \\tilde{\\epsilon}_{1i},  u_{0i} + u_{1i} a_3 + \\tilde{\\epsilon}_{3i} ) \\\\\n   &= cov(  u_{0i}  + \\tilde{\\epsilon}_{1i},  u_{0i} + 2 u_{1i} + \\tilde{\\epsilon}_{3i} ) \\\\\n   &= cov(  u_{0i}, u_{0i} ) + cov( u_{0i}, 2 u_{1i} ) + cov( u_{0i}, \\tilde{\\epsilon}_{3i} ) + cov( \\tilde{\\epsilon}_{1i}, u_{0i}) + cov( \\tilde{\\epsilon}_{1i}, 2 u_{1i} )  + cov( \\tilde{\\epsilon}_{1i}, \\tilde{\\epsilon}_{3i}) \\\\\n   &= \\tau_{00} + 2\\tau_{01} + 0 + 0 + 0 + 0 \\\\\n   &= \\tau_{00} + 2\\tau_{01}\n\\end{aligned}\\]\nNote how we multiple out the individual components, and this gives an expression for the overall covariance of our two residuals. If we did this for each \\(\\delta_{tt'}\\) we could fill in our \\(5 \\times 5\\) matrix. Fun!\nA core idea here is the independence of the different residual pieces makes a lot of the terms go to 0, giving short(er) expressions than we might have otherwise. The random slope model dictates the overall covariance of the residuals."
  },
  {
    "objectID": "cov_matrix_derivation.html#calculating-the-diagonal-terms.",
    "href": "cov_matrix_derivation.html#calculating-the-diagonal-terms.",
    "title": "38  Covariance Derivation",
    "section": "38.1 Calculating the diagonal terms.",
    "text": "38.1 Calculating the diagonal terms.\nFor the variances, you would just calculate covariance of a quantity with itself. Let’s do \\(\\delta_{11}\\), the variance of timepoint 1: \\[\\begin{aligned}\n\\delta_{11} &= var( \\epsilon_{i1} ) = cov( \\epsilon_{i1}, \\epsilon_{i1} ) \\\\\n   &= cov(  u_{0i} + u_{1i} a_1 + \\tilde{\\epsilon}_{1i},  u_{0i} + u_{1i} a_1 + \\tilde{\\epsilon}_{1i} ) \\\\\n   &= cov(  u_{0i}  + \\tilde{\\epsilon}_{1i},  u_{0i} + \\tilde{\\epsilon}_{1i} ) \\\\\n   &= cov(  u_{0i}, u_{0i} ) + cov( u_{0i},  \\tilde{\\epsilon}_{1i} ) + cov( \\tilde{\\epsilon}_{1i}, u_{0i}) + cov( \\tilde{\\epsilon}_{1i},\\tilde{\\epsilon}_{1i} )  \\\\\n   &= \\tau_{00} + 0 + 0 + \\sigma^2 =  \\tau_{00} + \\sigma^2\n\\end{aligned}\\]\nNow let’s do \\(\\delta_{55}\\), the variance of timepoint 5: \\[\\begin{aligned}\n\\delta_{55} &= var( \\epsilon_{i5} ) = cov( \\epsilon_{i5}, \\epsilon_{i5} ) \\\\\n   &= cov(  u_{0i} + u_{1i} a_5 + \\tilde{\\epsilon}_{5i},  u_{0i} + u_{5i} a_5 + \\tilde{\\epsilon}_{5i} ) \\\\\n   &= cov(  u_{0i} + 4 u_{1i} + \\tilde{\\epsilon}_{5i},  u_{0i} + 4 u_{1i} + \\tilde{\\epsilon}_{5i} ) \\\\\n   &= cov(  u_{0i}, u_{0i} ) + cov(  u_{0i}, 4 u_{1i} )  + cov( u_{0i},  \\tilde{\\epsilon}_{5i} ) + \\\\\n    &\\qquad cov( 4 u_{1i}, u_{0i} ) + cov( 4 u_{1i}, 4 u_{1i} )  + cov( 4 u_{1i}, \\tilde{\\epsilon}_{5i} ) \\\\\n    & \\qquad cov( \\tilde{\\epsilon}_{1i}, u_{0i}) + cov( \\tilde{\\epsilon}_{1i}, 4 u_{1i} )  + cov( \\tilde{\\epsilon}_{1i},\\tilde{\\epsilon}_{1i} )  \\\\\n   &= \\tau_{00} + 4 \\tau_{01} + 0 + 4 \\tau_{01} + 16 \\tau_{11} + 0 + 0 + 0 + \\sigma^2 \\\\\n   &= \\tau_{00} + 16 \\tau_{11} + 8 \\tau_{01} + \\sigma^2 .\n\\end{aligned}\\]\nNote how the variance around the intercept (at time 1 where \\(a_1 = 0\\)) looks like it would be smaller than far out. That being said, the covariance \\(\\tau_{01}\\) could be large and negative, causing the variance at the intercept to be less. But, \\(\\tau_{01}\\) is positive, the overall variance increases as we move away from the intercept point.\nOne interesting aspect of random slope models is the marginal (at each time point) has heteroskedasticity: the variances are each time point can be different because the lines can spread or gather."
  },
  {
    "objectID": "inflated_variance.html",
    "href": "inflated_variance.html",
    "title": "39  Inflated Variance Derivation",
    "section": "",
    "text": "Say we want to estimate the variability of mean math achievement across schools. I.e., each school has some average math achievement of its students, and we want to know how different schools are.\nThe naïve way of doing this is to estimate the mean math achievement for a sample of schools, and take the standard deviation (square root of variance) of this sample as a reasonable estimate. In math terms, we would calculate \\(\\bar{Y}_j\\) for each school \\(j\\) and then use as our estimate \\[\\widehat{\\tau^2} = var( \\bar{Y}_j ) = \\frac{1}{n-1} \\sum_{j=1}^J \\bar{Y}_j .\\] This will give a number that is too big. The following is a math derivation on a simple scenario that illustrates why.\nFirst, pretend our Data Generation Process (DGP) is Mother Nature making a bunch of schools, and then for each school making a bunch of kids. Our model is that the schools are represented by school-level true mean math achievement, and the kids are made by adding an individual kid effect to the mean math achievement of their schools.\nSo we have \\[\\alpha_j \\sim N( \\mu, \\tau^2 )\\] meaning each school is a random draw from a normal distribution with a mean \\(\\mu\\) and a standard deviation \\(\\tau\\). These are the true means of the schools. We wish we knew them, but we do not. Instead we see a sample of kids from the school and we hope the mean of the kids is close to this true mean \\(\\alpha_j\\).\nFor any kid \\(i\\) we have \\[Y_i  = \\alpha_{j[i]} + \\epsilon_i\\] with \\[\\epsilon_i \\sim N( 0, \\sigma^2 ).\\] These \\(\\epsilon_i\\) are the classic residuals we are used to.\nFor the moment, assume each school \\(j\\) has \\(n\\) kids. Then the average observed math achievement is \\[\\bar{Y}_j = \\frac{1}{n} \\sum_{i : j[i] = j} Y_i ,\\] the average of all kids in the school. Note the “\\(i : j[i] = j\\)” term, which reads as “\\(i\\) for those \\(i\\) where \\(j[i] = j\\)” meaning “sum over all students which go to school \\(j\\).”\nOk, so now we have math achievement for school \\(j\\). We then have \\[\\bar{Y}_j = \\frac{1}{n} \\sum_{i : j[i] = j} Y_i  = \\frac{1}{n} \\sum_{i : j[i] = j} \\alpha_{j[i]} + \\epsilon_i =   \\frac{1}{n} \\sum_{i : j[i] = j} \\alpha_j +  \\frac{1}{n} \\sum_{i : j[i] = j} \\epsilon_i = \\alpha_j +  \\frac{1}{n} \\sum_{i : j[i] = j}\\epsilon_i = \\alpha_j + \\bar{\\epsilon}_j .\\] Here we have \\(\\bar{\\epsilon}_j = \\bar{Y}_j - \\alpha_j\\), i.e., we have a school-level residual, the error in our estimate of \\(\\alpha_j\\) using \\(\\bar{Y}_j\\). This residual is the sum of a bunch of student residuals, which we assume are all independent of each other. When you average a bunch of independent, identically distributed (i.i.d.) residuals, each with variance \\(\\sigma^2\\), you get something which still has the same mean (of 0) but a smaller variance by a factor of \\(n\\): \\[\\var{ \\bar{\\epsilon_j} } = \\var{  \\frac{1}{n} \\sum_{i : j[i] = j}\\epsilon_i } = \\frac{1}{n^2} \\sum_{i : j[i] = j} \\var{ \\epsilon_i } =  \\frac{1}{n} \\sigma^2\\] This is the familiar result that the mean of a bunch of variables has a standard deviation \\(1/\\sqrt{n}\\) of the original standard deviation (part of the Central Limit Theorem).\nWe can think of \\(\\bar{Y}_j\\) as a random quantity, random for two reasons: school \\(j\\) is a randomly made school, and the students in school \\(j\\) are randomly made students. Under the assumption that the students’ error terms are independent of the school’s mean math achievement we can easily calculate the variance of our estimator: \\[\\var{ \\bar{Y}_j } = \\var{ \\alpha_j } + \\var{ \\bar{\\epsilon}_j } =   \\tau^2 +  \\frac{1}{n} \\sigma^2\\]\nThis is bigger than our target of \\(\\tau^2\\), the true variability in mean math achievement across schools. The uncertainty in estimating the \\(\\alpha_j\\) has entered into the variability.\nOur estimate \\(\\widehat{\\tau^2}\\) will be an unbiased estimate of \\(\\tau^2 + \\frac{1}{n} \\sigma^2\\). One way to fix is to estimate \\(\\sigma^2\\) and then adjust our estimate of the variance of \\(\\tau^2\\) by subtracting \\(\\frac{1}{n} \\hat{\\sigma}^2\\). Another is to use multilevel modeling, which does this for us, in effect."
  },
  {
    "objectID": "pooling.html#pooledunpooled-v.s.-fixedrandom-effects",
    "href": "pooling.html#pooledunpooled-v.s.-fixedrandom-effects",
    "title": "40  Pooling",
    "section": "40.1 Pooled/unpooled v.s. fixed/random effects",
    "text": "40.1 Pooled/unpooled v.s. fixed/random effects\nYou may have noticed that we use a couple of different terms interchangeably in this class when it comes to models. Sometimes we talk about coefficients as being completely pooled/partially pooled/unpooled, and sometimes we talk about coefficients as being random or fixed. Yikes, so confusing! Here’s a quick document explaining what these various terms mean and what sorts of models they represent. We’re only going to be talking about models where the pooling applied to the intercept and slope is the same; most models look like this, and these models are easier to talk about. You should be able to see how you might pool different coefficients differently, though the R code for that can be challenging. We’ll use the HSB data, and all of the models we’ll consider will look at regressions of math achievement on SES.\n\n40.1.1 Completely pooled\nA completely pooled model is a model where we assume that every second-level unit (school) has the same intercept and slope (slopes and intercepts are both completely pooled). This doesn’t really have an analog in the fixed/random effects world.\nA completely pooled model in this setting might look like\n\\[\\begin{aligned}\n    mathach_i &= \\beta_0 + \\beta_1SES_i + \\varepsilon_i \\\\\n    \\varepsilon_i &\\sim Normal(0, \\sigma^2)\n\\end{aligned}\\]\nIn a completely pooled model we’re basically assuming that every school has the same intercept and slope, so we just ignore school membership; notice that we don’t even include the \\(j\\) subscript because we’re ignoring schools completely. How rude!\nWe would fit this model with\nlm(mathach ~ ses).\n\n\n40.1.2 Partially pooled\nA partially pooled model allows for the possibility that different schools might have different slopes and intercepts, but assumes that these slopes and intercepts come from a Normal distribution, which has the effect of pulling them all in towards a grand mean (or partially pooling them). This model can also be called a model with random slopes and random intercepts, since we assume that school intercepts and residuals are random draws from a multivariate distribution with means equal to the grand means (and some possibly non-0 correlation). We don’t try to estimate these by themselves, only their variances and covariance.\nThis model can be represented as\n\\[\\begin{aligned}\nmathach_{ij} &= \\beta_{0j[i]} + \\beta_{1j[i]}SES_i + \\varepsilon_i, \\\\\n    \\beta_{0j} &= \\gamma_{00} + u_{0j},\\\\\n    \\beta_{1j} &= \\gamma_{10} + u_{1j},\\\\\n    \\varepsilon_i &\\sim Normal(0, \\sigma^2_\\varepsilon) \\\\\n    \\begin{pmatrix}\n        u_{0j}\\\\\n        u_{1j}\\\\\n    \\end{pmatrix} &\\sim  N\n    \\begin{bmatrix}\n        \\begin{pmatrix}\n            0\\\\\n            0\n        \\end{pmatrix}\\!\\!,&\n        \\begin{pmatrix}\n            \\sigma^2_0 & \\rho\\sigma_0\\sigma_1\\\\\n            \\rho\\sigma_0\\sigma_1 & \\sigma^2_1\n        \\end{pmatrix}\n    \\end{bmatrix}\n\\end{aligned}\\]\nWe would fit this model with\nlmer(mathach ~ ses + (ses|school))\n\n\n40.1.3 Unpooled\nIn an unpooled model, we don’t share any information across schools about the slopes and intercepts. Instead, we estimate each one separately in each higher-order unit. This is a fixed-effects model, because the model treats each school-level slope and intercept as a fixed quantity in the population to be estimated directly. In general parlance, to be a little more precise, a fixed-effects model is a model with unpooled intercepts and completely pooled slopes (although in theory the completely pooled model also has only fixed effects, it’s just that those effects are the same in every school; this is why the language of completely pooled, partially pooled, and unpooled coefficients is a little more precise, though it’s also less popular).\nWe could represent an unpooled model as\n\\[\\begin{aligned}\n    mathach_{ij} &= \\beta_{0j[i]} + \\beta_{1j[i]}SES_i + \\varepsilon_i \\\\\n    \\varepsilon_i &\\sim Normal(0, \\sigma^2)\n\\end{aligned}\\]\nWe would fit the model with\nlm(mathach ~ ses*school)\nalthough we might get our estimates in a more useful way by specifying an (identical) model which has no reference school, i.e.,\nlm(mathach ~ 0 + ses + ses:school)\nFor either of these models to fit you need to ensure that school is coded as a factor and not a number; this is not a concern for lmer."
  },
  {
    "objectID": "survey_weights.html#multilevel-modeling-and-survey-weights",
    "href": "survey_weights.html#multilevel-modeling-and-survey-weights",
    "title": "41  Survey Weights",
    "section": "41.1 Multilevel modeling and survey weights",
    "text": "41.1 Multilevel modeling and survey weights\nIn many circumstances you may be faced with a multilevel modeling project where you also have survey weights. Unfortunately R does not have good support for this hybrid of two worlds (although if you go the econometric direction you can incorporate weights into your robust standard errors).\nYou basically have two options at this point: you can ignore the weights (defendable, but often upsetting to reviewers and colleagues), or switch to Stata, which allows for both."
  },
  {
    "objectID": "survey_weights.html#topline-advice",
    "href": "survey_weights.html#topline-advice",
    "title": "41  Survey Weights",
    "section": "41.2 Topline advice",
    "text": "41.2 Topline advice\nIgnore the weights for your final project and worry about extending to your general population later on, depending on where your research takes you. More important than the weights is making sure you have random effects corresponding to all clustering involved in the way the data were collected. For example, if the program was a sample of states and then a sample of villages, and then households, you would have a 4-level model: states, villages, households, and individuals. You would want a random effect for each level. If you had few states, you could back off and have fixed effects for state and generalize only to the sampled states rather than the full country."
  },
  {
    "objectID": "survey_weights.html#what-are-survey-weights",
    "href": "survey_weights.html#what-are-survey-weights",
    "title": "41  Survey Weights",
    "section": "41.3 What are survey weights?",
    "text": "41.3 What are survey weights?\nThe easy way to think of sample weights (survey weights) is the answer to “how many people does this individual represent in the population?” (Although note that weights will generally be proportional to the answer to this question rather than literally that value.)\nFor example, if you had three people, the first with a weight of 1, the second with a weight of 0.5 and the third with a weight of 3, then we would think of our population as being \\(1 + 0.5 + 3 = 4.5\\) people, 3 of whom are people similar to our third sampled person, 0.5 of which our second sampled person, and 1 of whom is similar to our first person.\nIn other words, our first person is sampled proportional to their prevalence in the population. The 0.5 weight person is “oversampled”—we have too many people like this in our sample, as compared to the population so we “downweight” them. The third person is underrepresented, by contrast. We should have had three times as many of these types of people in our sample as we have.\nThus, sampling weights adjust for the probability of selecting an individual from the population when that probability is not constant (this could be due either by design or by chance). For nationally representative data surveys often select a sample where individuals have an unequal probability of being selected. This is done to increase the number of individuals and reduce sampling variability, particularly for certain areas or subgroups of the population. In some cases, corrections for non-response are also built into the weights. Sampling weights are then inversely proportional to the probability of selection.\nIn some complex surveys, there may be more than one sampling weight when different subsamples are selected. For example, the Demographic and Health Surveys (DHS)1 select a subsample of adults to be tested for HIV. If 1 in 5 households is selected for HIV testing, then no weighting is needed. But if 1 in 5 urban households and 1 in 2 rural households are selected, then sampling weights need to be applied to both descriptive statistics and model estimates to estimate at national level."
  },
  {
    "objectID": "survey_weights.html#what-happens-if-you-ignore-the-weights",
    "href": "survey_weights.html#what-happens-if-you-ignore-the-weights",
    "title": "41  Survey Weights",
    "section": "41.4 What happens if you ignore the weights?",
    "text": "41.4 What happens if you ignore the weights?\nIn this case (as long as you are modeling the clustering correctly) you are estimating relationships on your sample, rather than the target population. This can be a totally fine thing: if you are interested in how some variables interrelate you might reasonably believe that a found pattern of relationships in the sample is very indicative of how things may play out on a wider stage. It would be odd for (statistically significant) relationships in the sample to not be at least somewhat similar to the population the sample came from. The true magnitudes may shift, but the story should be the same.\nFor example, if, after ignoring weights, you find an impact of some treatment, then you know the treatment works, at least for those in your sample. Even if your sample is a nonrepresentative sample of your population, it is still some sort of representation, and thus you would believe that your treatment would work to some degree more generally. In this case, any differences between your sample and population would be due to treatment variation, i.e., some in your sample respond differently than some in the population, and so your results in the sample would be weighting some people more than we “should,” causing the discrepancy.\nSurvey weights are usually much more important when trying to estimate level, or prevalence, of an outcome. If, for example, you are attempting to measure the average literacy in a population, then survey weights will be very important: if the weights of those systematically more (or less) literate are different, then ignoring the weights can cause bias. In addition, if some types of groups or areas are oversampled, then your estimates will tend to be biased towards the levels and relationships in the oversampled group/area. But if you are interested in the relationship between literacy and some covariate, the weights will matter less: it is only if the relationship between these variables is different in your high weight and low weight individuals where you will get bias. This is arguably a less natural phenomenon."
  },
  {
    "objectID": "survey_weights.html#how-to-apply-weights",
    "href": "survey_weights.html#how-to-apply-weights",
    "title": "41  Survey Weights",
    "section": "41.5 How to apply weights?",
    "text": "41.5 How to apply weights?\nAs a rule of thumb, you want to first read any available documentation for the data you are using. You want to understand how the sample was obtained and how the weights were calculated. Publicly available data often comes with manuals on how to handle weights. Some manuals even come with R and Stata code! This is a very important step as sometimes you have to manipulate the weights before you can use them! For example, when using DHS data, you have to divide the weight by 1,000,000 before use. This is a function of how the weights are calculated. In addition, many complex surveys that use weights may also have stratified the sample, and that is also something to account for in your analysis.\nWhen using survey weights it is always advisable to compare the results that include weights with those without them. In general, one should not expect see substantive changes in the point estimates of regression coefficients to the point of dramatically changing one’s interpretation of one’s results. The model itself is supposed to capture structural relationships between covariates and outcomes, and under correct model specification the weights are superfluous with regards to these coefficients. Where weights could cause change is with descriptives such as the overall averages (e.g., the intercepts, in particular, could be different. We may also see changes in the variance parameters. Finally, with weights, one usually sees an increase in the the standard errors.\nA limitation with some packages is one might not have an easy way to obtain model fit statistics to help compare models. A clean way to avoid this is to go through the process of model selection using the data in the sample (ignoring the weights) and using the packages and approaches we have seen in class. The findings from such an exploration would be valid for the structure of the data in our sample. Then, as a second step, include the survey weights to move to inference to a larger population (for which the sample is supposed to be representative), taking the preferred model choices from step one and fitting them through a package that allows for survey weights."
  },
  {
    "objectID": "survey_weights.html#further-references",
    "href": "survey_weights.html#further-references",
    "title": "41  Survey Weights",
    "section": "41.6 Further references",
    "text": "41.6 Further references\nFor some good resources see (asparouhov2006?; carle2009?; rabe-hesketh2006?). Prior students previously also used (laukaityte2018importance?) and (lorah2020estimating?) for some guidance. They then worked with the BIFIEsurvey R package to fit multi-level models with survey weights to account for the complex sampling design in their data."
  },
  {
    "objectID": "survey_weights.html#footnotes",
    "href": "survey_weights.html#footnotes",
    "title": "41  Survey Weights",
    "section": "",
    "text": "These are nationally representative surveys conducted in low- and middle-income countries collecting data primarily on maternal and child health.↩︎"
  },
  {
    "objectID": "complex_error.html#national-youth-survey-running-example",
    "href": "complex_error.html#national-youth-survey-running-example",
    "title": "42  An overview of complex error structures",
    "section": "42.1 National Youth Survey running example",
    "text": "42.1 National Youth Survey running example\nOur running example is the data as described in Raudenbush and Bryk, and we follow the discussion on page 190. These data are the first cohort of the National Youth Survey (NYS). This data comes from a survey in which the same students were asked yearly about their acceptance of 9 “deviant” behaviors (such as smoking marijuana, stealing, etc.). The study began in 1976, and followed two cohorts of children, starting at ages 11 and 14 respectively. We will analyze the first 5 years of data.\nAt each time point, we have measures of:\n\nATTIT, the attitude towards deviance, with higher numbers implying higher tolerance for deviant behaviors.\nEXPO, the “exposure”, based on asking the children how many friends they had who had engaged in each of the behaviors. Both of these numbers have been transformed to a logarithmic scale to reduce skew.\n\nFor each student, we have:\n\nGender (binary)\nMinority status (binary)\nFamily income, in units of $10K.\n\nOne reasonable research question would to describe how the cohort evolved. For this question, the parameters of interest would be the average attitudes at each age. Standard deviations and intrasubject correlations are, as is often but not always the case, simply nuisance parameters. Still, the better we can do at realistically modeling these nuisance parameters, the more precision we will have for the measures of interest, and the power we will have to test relevant hypotheses.\n\n42.1.1 Getting the data ready\nWe’ll focus on the first cohort, from ages 11-15. First, let’s read the data.\nNote that this table is in “wide format”. That is, there is only one row for each student, with all the different observations for that student in different columns of that one row.\n\nnyswide = read.csv(\"data/nyswide.csv\")\nhead(nyswide)\n\n  ID ATTIT.11 EXPO.11 ATTIT.12 EXPO.12 ATTIT.13 EXPO.13 ATTIT.14 EXPO.14\n1  3     0.11   -0.37     0.20   -0.27     0.00   -0.37     0.00   -0.27\n2  8     0.29    0.42     0.29    0.20     0.11    0.42     0.51    0.20\n3  9     0.80    0.47     0.58    0.52     0.64    0.20     0.75    0.47\n4 15     0.44    0.07     0.44    0.32     0.89    0.47     0.75    0.26\n5 33     0.20   -0.27     0.64   -0.27     0.69   -0.27       NA      NA\n6 45     0.11    0.26     0.37   -0.17     0.37    0.14     0.37    0.14\n  ATTIT.15 EXPO.15 FEMALE MINORITY INCOME\n1     0.11   -0.17      1        0      3\n2     0.69    0.20      0        0      4\n3     0.98    0.47      0        0      3\n4     0.80    0.47      0        0      4\n5     0.11    0.07      1        0      4\n6     0.69    0.32      1        0      4\n\n\nFor our purposes, we want it in “long format”. The reshape() command does this for us (reshape allows us to reshape two variables at once, as compared to gather() from tidyverse:\n\nnys1.na = reshape(nyswide, direction=\"long\", #we want it in long format\n              varying=list(ATTIT=paste(\"ATTIT\",11:15,sep=\".\"), \n                          EXPO=paste(\"EXPO\",11:15,sep=\".\") ),\n              v.names=c(\"ATTIT\",\"EXPO\"), idvar=\"ID\", timevar=\"AGE\", times=11:15)\n\n## Drop missing ATTIT values\nnys1 = nys1.na[!is.na(nys1.na$ATTIT),] \n\nhead( nys1 )\n\n      ID FEMALE MINORITY INCOME AGE ATTIT  EXPO\n3.11   3      1        0      3  11  0.11 -0.37\n8.11   8      0        0      4  11  0.29  0.42\n9.11   9      0        0      3  11  0.80  0.47\n15.11 15      0        0      4  11  0.44  0.07\n33.11 33      1        0      4  11  0.20 -0.27\n45.11 45      1        0      4  11  0.11  0.26\n\n\nNote, the paste command makes the sequence c(\"ATTIT.12\", \"ATTIT.13\", ...) to autogenerate our variable names to reshape:\n\npaste(\"ATTIT\",11:15,sep=\".\")\n\n[1] \"ATTIT.11\" \"ATTIT.12\" \"ATTIT.13\" \"ATTIT.14\" \"ATTIT.15\"\n\n\nWe also need to make our age a factor so it is treated appropriately as an indicator of what wave the data was collected in.\n\nnys1$agefac = as.factor(nys1$AGE) \n\nJust to get a sense of the data, let’s plot each age as a boxplot\n\nggplot( nys1, aes( agefac, ATTIT ) ) +\n  geom_boxplot()\n\n\n\n\nNote some features of the data: First, we see that ATTIT goes up over time. Second, we see the variation of points also goes up over time. This is heteroskedasticity.\nIf we plot individual lines we have\n\nnys1$AGEjit = jitter(nys1$AGE)\nnys1$ATTITjit = jitter(nys1$ATTIT, amount=0.05)\nggplot( filter( nys1, complete.cases(nys1) ), aes( AGEjit, ATTITjit, group=ID ) ) +\n  geom_line( alpha=0.2 )\n\n\n\n\nNote how we have correlation of residuals, in that some students are systematically low and some are systematically higher (although there is a lot of bouncing around)."
  },
  {
    "objectID": "complex_error.html#representation-of-error-structure",
    "href": "complex_error.html#representation-of-error-structure",
    "title": "42  An overview of complex error structures",
    "section": "42.2 Representation of error structure",
    "text": "42.2 Representation of error structure\nIn our data, we have 5 observations \\(y_{it}\\) for each subject i at 5 fixed times \\(t=1\\) through \\(t=5\\). Within each person \\(i\\) (where person is our Level-2 group, and time is our Level-1),\n\\[\\begin{pmatrix}y_{i1}\\\\\ny_{i2}\\\\\ny_{i3}\\\\\ny_{i4}\\\\\ny_{i5}\n\\end{pmatrix} \\sim N\\left[\\left(\\begin{array}{c}\n\\mu_1\\\\\n\\mu_2\\\\\n\\mu_3\\\\\n\\mu_4\\\\\n\\mu_5\n\\end{array}\\right),\\left(\\begin{array}{ccccc}\n\\tau_{11} & \\tau_{12} & \\tau_{13} & \\tau_{14} & \\tau_{15}\\\\\n. & \\tau_{22} & \\tau_{23}& \\tau_{24} & \\tau_{25}\\\\\n. & . & \\tau_{33}& \\tau_{34} & \\tau_{35}\\\\\n. & . & . & \\tau_{44} & \\tau_{45}\\\\\n. & . & . & . & \\tau_{52}\n\\end{array}\\right)\\right] = N[ \\mu, \\Sigma ]\\]\nNote that the key parts here are the correlations between the residuals at different times. We call our entire covariance matrix \\(\\Sigma\\). This matrix describes how the residuals within a single individual (with 5 time points of observation) are correlated. The mean vector can easily be separated out:\n$$\n\\[\\begin{pmatrix}y_{i1}\\\\\n\ny_{i2}\\\\\ny_{i3}\\\\\ny_{i4}\\\\\ny_{i5}\n\\end{pmatrix}\\]\n= (\n\\[\\begin{array}{c}\n\\mu_{1i}\\\\\n\\mu_{2i}\\\\\n\\mu_{3i}\\\\\n\\mu_{4i}\\\\\n\\mu_{5i}\n\\end{array}\\]\n) + (\n\\[\\begin{array}{c}\n\\epsilon_{1i}\\\\\n\\epsilon_{2i}\\\\\n\\epsilon_{3i}\\\\\n\\epsilon_{4i}\\\\\n\\epsilon_{5i}\n\\end{array}\\]\n)\\[ \\](\n\\[\\begin{array}{c}\n\\epsilon_{1i}\\\\\n\\epsilon_{2i}\\\\\n\\epsilon_{3i}\\\\\n\\epsilon_{4i}\\\\\n\\epsilon_{5i}\n\\end{array}\\]\n)N$$\nOur regression model would give us the mean vector for any given student (e.g., it would be \\(X'\\beta\\) for some covariate matrix (design matrix) \\(X\\) and fixed effect parameter vector \\(\\beta\\). \\(X\\) would have one row per time point and time would be one of the columns, to give our predictions for our 5 time points.\nOur error structure model gives us the distribution of the \\((\\epsilon_{1i}, \\ldots, \\epsilon_{5i})\\) for that student. Different ideas about the data generating process lead to different correlation structures here. We saw a couple of those in class."
  },
  {
    "objectID": "complex_error.html#reproducing-chapter-6-examples",
    "href": "complex_error.html#reproducing-chapter-6-examples",
    "title": "42  An overview of complex error structures",
    "section": "42.3 Reproducing Chapter 6 examples",
    "text": "42.3 Reproducing Chapter 6 examples\nThe above provides a framework for thinking about grouped data: each group is a small world with a linear prediction line and a collection of residuals around that line. Under this view, we specify a specific structure on how the residuals relate to each other. (E.g., for classic OLS we would have i.i.d. normally distributed residuals, represented as our \\(\\Sigma\\) being a diagonal matrix with \\(\\sigma^2\\) along the diagonal and 0s everywhere else). In R, once we determine what structure we want, we can fit models based on parameterized correlation matrices using the “lme” command from the “nlme” package (You may need to first call install.packages(\"nlme\") to get this package). Let’s load the package now:\n\nlibrary(nlme)\n\nRecall that all of these models include just the single fixed effect (besides intercept) of a linear term on age.\n\n42.3.1 Compound symmetry (random intercept model)\nA “compound symmetry” residual covariance structure (all diagonal elements equal, all off-diagonal elements equal) is actually equivalent to a random intercepts model. Thus, there are 2 ways to get this same model:\n\nmodelRE = lme(ATTIT ~ AGE, \n              data=nys1,\n              random=~1|ID )\n\nand\n\nmodelCompSymm = gls(ATTIT ~ AGE,\n                    data=nys1,\n                    correlation=corCompSymm(form=~AGE|ID) )\n\nFor reference, using the lme4 package we again have (we use lme4:: in front of lmer to avoid loading the lme4 package fully):\n\nmodelRE.lme4 = lme4::lmer(ATTIT ~ AGE + (1|ID), data=nys1 )\n\nWe can get the correlation matrix for individuals #3:\n\nmyVarCovs = getVarCov(modelRE,type=\"marginal\", individual=3)\nmyVarCovs\n\nID 9 \nMarginal variance covariance matrix\n         1        2        3        4        5\n1 0.066450 0.034113 0.034113 0.034113 0.034113\n2 0.034113 0.066450 0.034113 0.034113 0.034113\n3 0.034113 0.034113 0.066450 0.034113 0.034113\n4 0.034113 0.034113 0.034113 0.066450 0.034113\n5 0.034113 0.034113 0.034113 0.034113 0.066450\n  Standard Deviations: 0.25778 0.25778 0.25778 0.25778 0.25778 \n\n\nIf we look at an individual #5, who only has 4 timepoints we get a \\(4 \\times 4\\) matrix:\n\ngetVarCov(modelRE,type=\"marginal\", individual=5)\n\nID 33 \nMarginal variance covariance matrix\n         1        2        3        4\n1 0.066450 0.034113 0.034113 0.034113\n2 0.034113 0.066450 0.034113 0.034113\n3 0.034113 0.034113 0.066450 0.034113\n4 0.034113 0.034113 0.034113 0.066450\n  Standard Deviations: 0.25778 0.25778 0.25778 0.25778 \n\n\n(Other individuals are the same, if they have the same number of time points, given our model.)\n\n42.3.1.1 Comparing the models\nThese are two very different ways of specifying the same thing, and the parameter estimates we get out are also not the same. Compare the two summary printouts:\n\nsummary(modelRE)\n\nLinear mixed-effects model fit by REML\n  Data: nys1 \n        AIC       BIC   logLik\n  -204.9696 -185.0418 106.4848\n\nRandom effects:\n Formula: ~1 | ID\n        (Intercept)  Residual\nStdDev:   0.1846979 0.1798237\n\nFixed effects:  ATTIT ~ AGE \n                 Value  Std.Error  DF   t-value p-value\n(Intercept) -0.5099954 0.05358498 839 -9.517505       0\nAGE          0.0644387 0.00398784 839 16.158810       0\n Correlation: \n    (Intr)\nAGE -0.969\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-2.90522949 -0.64353962 -0.01388485  0.60377631  3.26938845 \n\nNumber of Observations: 1079\nNumber of Groups: 239 \n\n\nand\n\nsummary(modelCompSymm)\n\nGeneralized least squares fit by REML\n  Model: ATTIT ~ AGE \n  Data: nys1 \n        AIC       BIC   logLik\n  -204.9696 -185.0418 106.4848\n\nCorrelation Structure: Compound symmetry\n Formula: ~AGE | ID \n Parameter estimate(s):\n      Rho \n0.5133692 \n\nCoefficients:\n                 Value  Std.Error   t-value p-value\n(Intercept) -0.5099954 0.05358498 -9.517505       0\nAGE          0.0644387 0.00398784 16.158810       0\n\n Correlation: \n    (Intr)\nAGE -0.969\n\nStandardized residuals:\n        Min          Q1         Med          Q3         Max \n-1.77123071 -0.77132300 -0.06434029  0.71151900  3.38387884 \n\nResidual standard error: 0.2577787 \nDegrees of freedom: 1079 total; 1077 residual\n\n\nThese do not look very similar, do they? But wait:\n\nlogLik(modelCompSymm)\n\n'log Lik.' 106.4848 (df=4)\n\nlogLik(modelRE)\n\n'log Lik.' 106.4848 (df=4)\n\nlogLik(modelRE.lme4)\n\n'log Lik.' 106.4848 (df=4)\n\nAIC( modelCompSymm )\n\n[1] -204.9696\n\nAIC( modelRE )\n\n[1] -204.9696\n\nAIC( modelRE.lme4 )\n\n[1] -204.9696\n\n\nIn fact, they have the same AIC, etc., because they are equivalent models.\nThe lesson is that it’s actually quite hard to see the correspondence between a familiar random-effects model and an equivalent model expressed in terms of a covariance matrix. Sure, we could do a bunch of math and see that in the end they are the same; but that math is already daunting here, and this is the simplest possible situation. The fitted parameters of a covariance-based model are just really hard to interpret in familiar terms.\n\n\n\n42.3.2 Autoregressive error structure (AR[1])\nOne typical structure used for longitudinal data is the “autoregressive” structure. The idea is threefold:\n\n\\(Var(u_{it}) = \\sigma^2\\) - that is, overall marginal variance is staying constant.\n\\(Cor(u_{it},u_{i(t-1)}) = \\rho\\) - that is, residuals are a little bit “sticky” over time so residuals from nearby time points tend to be similar.\n\\(E(u_{it}|u_{i(t-1)},u_{i(t-2)}) = E(u_{it}|u_{i(t-1)})\\) - that is, the only way the two-periods-ago measurement tells you anything about the current one is through the intermediate one, with no longer-term effects or “momentum”.\n\nIn this case, the unconditional two-step correlation \\(Cor(u_{it},u_{i(t-2)})\\) is also easy to calculate. Intuitively, we can say that a portion \\(\\rho\\) of the residual “is the same” after each step, so that after two steps the portion that “is the same” is \\(\\rho\\) of \\(\\rho\\), or \\(\\rho^2\\). Clearly, then, after three steps the correlation will be \\(\\rho^3\\), and so on. In other words, the part that “is the same” is decaying in an exponential pattern. Indeed, one could show that (3.), above, requires the correlated part to decay in a memoryless pattern, leaving the Exponential and Hypergeometric distributions (which both show exponential decay) among the few options.\nThus, the within-subject correlation structure implied by these postulates is:\n\\[\\left(\\begin{array}{c}\nu_{1i}\\\\\nu_{2i}\\\\\nu_{3i}\\\\\nu_{4i}\\\\\n...\\\\\nu_{ni}\n\\end{array}\\right)\\sim N\\left[\\left(\\begin{array}{c}\n0\\\\\n0\\\\\n0\\\\\n0\\\\\n...\\\\\n0\n\\end{array}\\right),\\sigma^2\\left(\\begin{array}{cccccc}\n1 & \\rho  & \\rho^2 & \\rho^3 & ... & \\rho^{n-1}\\\\\n. & 1 & \\rho & \\rho^2 & ... & \\rho^{n-2}\\\\\n. & . & 1& \\rho  & ... & \\rho^{n-3}\\\\\n. & . & . & 1 & ... & \\rho^{n-4} \\\\\n... & ... & ... & ... & ... & ... \\\\\n. & . & . & . & ... & 1\n\\end{array}\\right)\\right]\\\\\\]\nAs you can see, this structure takes advantage of the temporal nature of the data sequence to parameterize the covariance matrix with only two underlying parameters: \\(\\sigma\\) and \\(\\rho\\). By contrast, a random intercept model needs the overall \\(\\sigma\\) and variance of intercepts \\(\\tau\\)—also two parameters! Same complexity, different structure.\n\n42.3.2.1 Fitting the AR[1] covariance structure\nTo get a true AR[1] residual covariance structure, we need to leave the world of hierarchical models, and thus use the command gls. This is just what we’ve discussed in class. However, later on in this document, we’ll see how to add AR[1] structure on top of a hierarchical model, which is messier from a theoretical point of view, but often more useful and interpretable in practice.\n\nmodelAR1 = gls(ATTIT ~ AGE, \n                    data=nys1,\n                    correlation=corAR1(form=~AGE|ID) )\n\nsummary(modelAR1)\n\nGeneralized least squares fit by REML\n  Model: ATTIT ~ AGE \n  Data: nys1 \n        AIC       BIC   logLik\n  -250.4103 -230.4826 129.2051\n\nCorrelation Structure: ARMA(1,0)\n Formula: ~AGE | ID \n Parameter estimate(s):\n     Phi1 \n0.6159857 \n\nCoefficients:\n                 Value  Std.Error   t-value p-value\n(Intercept) -0.4534647 0.07515703 -6.033564       0\nAGE          0.0601205 0.00569797 10.551218       0\n\n Correlation: \n    (Intr)\nAGE -0.987\n\nStandardized residuals:\n        Min          Q1         Med          Q3         Max \n-1.75013168 -0.81139621 -0.03256558  0.74814629  3.40350724 \n\nResidual standard error: 0.2561765 \nDegrees of freedom: 1079 total; 1077 residual\n\n\nYou have to dig around in the large amount of output to find the parameter estimates, but they are there. Phi1 is the auto-correlation parameter. And the covariance of residuals:\n\ngetVarCov(modelAR1,type=\"marginal\")\n\nMarginal variance covariance matrix\n          [,1]     [,2]     [,3]     [,4]      [,5]\n[1,] 0.0656260 0.040425 0.024901 0.015339 0.0094485\n[2,] 0.0404250 0.065626 0.040425 0.024901 0.0153390\n[3,] 0.0249010 0.040425 0.065626 0.040425 0.0249010\n[4,] 0.0153390 0.024901 0.040425 0.065626 0.0404250\n[5,] 0.0094485 0.015339 0.024901 0.040425 0.0656260\n  Standard Deviations: 0.25618 0.25618 0.25618 0.25618 0.25618 \n\nsummary(modelAR1)$AIC\n\n[1] -250.4103\n\n\nNote that the AIC of our AR[1] model is lower by about 45 than the random intercept model; clearly far superior because it is getting nearby residuals being more correlated, while the random intercept model does not do this. Also see the banding structure of the residual correlation matrix.\n\n\n\n42.3.3 Random slopes\nIn theory, a random slopes model could be done with gls as well as with lme; in practice, it’s much more practical just to do it as a hierarchical model with lme:\n\nmodelRS = lme(ATTIT ~ 1 + AGE, \n              data=nys1,\n              random=~AGE|ID )\n\nWe have separated our fixed and random components with lme(). We first include a formula with only fixed effects, and then give a right-side-only formula with terms similar to what you’d put in parentheses with lmer() for the random effects.\nOur results:\n\nsummary(modelRS)\n\nLinear mixed-effects model fit by REML\n  Data: nys1 \n       AIC       BIC   logLik\n  -310.125 -280.2334 161.0625\n\nRandom effects:\n Formula: ~AGE | ID\n Structure: General positive-definite, Log-Cholesky parametrization\n            StdDev     Corr  \n(Intercept) 0.51024132 (Intr)\nAGE         0.05038614 -0.98 \nResidual    0.16265429       \n\nFixed effects:  ATTIT ~ 1 + AGE \n                 Value  Std.Error  DF  t-value p-value\n(Intercept) -0.5133250 0.05834087 839 -8.79872       0\nAGE          0.0646849 0.00492904 839 13.12323       0\n Correlation: \n    (Intr)\nAGE -0.981\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-2.87852414 -0.55971196 -0.07521191  0.57495075  3.45648134 \n\nNumber of Observations: 1079\nNumber of Groups: 239 \n\ngetVarCov(modelRS,type=\"marginal\", individual=3)\n\nID 9 \nMarginal variance covariance matrix\n         1        2        3        4        5\n1 0.039649 0.015922 0.018650 0.021379 0.024108\n2 0.015922 0.047646 0.026457 0.031725 0.036992\n3 0.018650 0.026457 0.060720 0.042070 0.049876\n4 0.021379 0.031725 0.042070 0.078872 0.062760\n5 0.024108 0.036992 0.049876 0.062760 0.102100\n  Standard Deviations: 0.19912 0.21828 0.24641 0.28084 0.31953 \n\nsummary(modelRS)$AIC\n\n[1] -310.125\n\n\nThe first thing to note is the residual covariance matrix comes from the structure of the random intercept and random slope. If you squint hard enough at it, you can begin to see the linear structures in its diagonal and off-diagonal elements. If you graphed it, those structures would jump out more clearly. But in practice, it’s much easier to think of things in terms of the hierarchical model, not in terms of linear structures in a covariance matrix.\nNote also that the AIC has dropped by another 60 points or so; we’re continuing to improve the model.\n\n\n42.3.4 Random slopes with heteroskedasticity\nRelaxing the homoskedasticity assumption in the random slopes model leaves us a bit in between worlds. We’re not fully into the world of GLS, because there are still random effects; but we’re not fully in the world of hierarchical models because there is structure in the residuals within groups. We’ll talk more about this compromise below; for now, let’s just do it.\n\nmodelRSH = lme(ATTIT ~ AGE, \n              data=nys1,\n              random=~AGE|ID,\n              weights=varIdent(form=~1|agefac) )\n\nThe key line is the varIdent line: we are saying each age factor level gets its own weight (rescaling) of the residuals—this is heteroskedasticity. In particular, the above says our residual variance will be weighted by a weight for each age factor, so each age level effectively gets its own variance. This is where these models start to get a bit exciting—we have random slopes, and then heteroskedastic residuals (homoskedastic for any given age level), all together. Our fit model:\n\nsummary(modelRSH)\n\nLinear mixed-effects model fit by REML\n  Data: nys1 \n        AIC       BIC   logLik\n  -312.5801 -262.7608 166.2901\n\nRandom effects:\n Formula: ~AGE | ID\n Structure: General positive-definite, Log-Cholesky parametrization\n            StdDev     Corr  \n(Intercept) 0.57693602 (Intr)\nAGE         0.05431367 -0.979\nResidual    0.14054184       \n\nVariance function:\n Structure: Different standard deviations per stratum\n Formula: ~1 | agefac \n Parameter estimates:\n       11        12        13        14        15 \n1.0000000 1.1956071 1.3095864 1.1255177 0.9802311 \nFixed effects:  ATTIT ~ AGE \n                 Value  Std.Error  DF   t-value p-value\n(Intercept) -0.4929012 0.05715889 839 -8.623351       0\nAGE          0.0631404 0.00483385 839 13.062122       0\n Correlation: \n    (Intr)\nAGE -0.981\n\nStandardized Within-Group Residuals:\n       Min         Q1        Med         Q3        Max \n-2.9163540 -0.5498217 -0.0758348  0.5482942  3.2370312 \n\nNumber of Observations: 1079\nNumber of Groups: 239 \n\n\nNote how we have 5 parameter estimates for the residuals, listed under agefac. It appears as if we have more variation in age 13 than other ages. Age 11, the baseline, is 1.0; it is our reference scaling. These numbers are all scaling the overall residual variance parameter \\(\\sigma^2\\) of \\(0.1405^2\\).\nFor looking at the covariance structure of the residuals, at this point we have to warn you: there appears to be a bug in getVarCov which rears its head when you use the weights argument to either lme or gls. It has to do with the order of the rows of the data set, something which obviously should not matter; and it means that you get simply wrong numbers for marginal variances, though correlations should still be correct. Jameson Quinn wrote a fix for this function, which is in the source file getVarCov2.R. This fix is used below, using the source command to load the fixed version of the function. But beware: while we’ve tested that this fix gives the right answers for this data set, and that it gives the right answers when the weights argument is not used (that is, when the old version was already right), we have not done the extensive checking it would take to say we trust it in all cases. This bug was reported back in 2016, and hopefully it will be fixed in later versions of R; but for now, tread with care, and double-check that the numbers you’re getting make sense.\n\nsource('scripts/getVarCov2.R')\nmyVarCov = getVarCovFixedLme(modelRSH,type=\"marginal\", individual=3)\nmyVarCov\n\nID 9 \nMarginal variance covariance matrix\n         1        2        3        4        5\n1 0.034915 0.016947 0.018731 0.020516 0.022300\n2 0.016947 0.049916 0.026415 0.031150 0.035884\n3 0.018731 0.026415 0.067975 0.041784 0.049468\n4 0.020516 0.031150 0.041784 0.077440 0.063052\n5 0.022300 0.035884 0.049468 0.063052 0.095615\n  Standard Deviations: 0.18685 0.22342 0.26072 0.27828 0.30922 \n\n\nWe get lists of matrices back from our call. We can convert any one to a correlation matrix:\n\ncov2cor(myVarCov[[1]])\n\n          1         2         3         4         5\n1 1.0000000 0.4059446 0.3844935 0.3945453 0.3859525\n2 0.4059446 1.0000000 0.4534860 0.5010159 0.5194173\n3 0.3844935 0.4534860 1.0000000 0.5759089 0.6136046\n4 0.3945453 0.5010159 0.5759089 1.0000000 0.7327497\n5 0.3859525 0.5194173 0.6136046 0.7327497 1.0000000\n\n\nAnd our AIC:\n\nsummary(modelRSH)$AIC\n\n[1] -312.5801\n\n\nNo amount of squinting will show the structure in that covariance matrix. But when you convert to a correlation matrix, you can again squint and begin to see the linear structures in its diagonal and off-diagonal elements. The same comment as above still applies: in practice, it’s much easier to think of things in terms of the hierarchical model, and only read the diagonals of the covariance matrix.\nNote that the AIC has dropped by only another 2.5 points or so; that corresponds to the idea that if one of these two models were exactly true, the odds are about \\(e^{2.5/2}\\cong 3.5\\) in favor of the more complex model. Aside from the fact that that premise is silly – we are pretty sure that neither of these models is the exact truth; and in that case, something like BIC would probably be better than AIC – those odds are also pretty weak; the simpler model is probably better here.\nHere’s the reported BICs, by the way: -280.2334145 for the homoskedastic one, and -262.7607678 for the heteroskedastic. As we expected, the simpler model wins that fight. (Though what \\(N\\) to use for BIC is sometimes not obvious with hierarchical models, so you can’t trust those numbers too much. We will discuss AIC and BIC more later in the course.)\n\n\n42.3.5 Fully unrestricted model\nOK, let’s go whole hog, and fit the unrestricted model. Again, this means leaving the world of hierarchical models and using gls.\n\nmodelUnrestricted = gls(ATTIT ~ AGE, \n               data=nys1,\n               correlation=corSymm(form=~1|ID),\n               weights=varIdent(form=~1|agefac) )\n\n\nsummary(modelUnrestricted)\n\nGeneralized least squares fit by REML\n  Model: ATTIT ~ AGE \n  Data: nys1 \n       AIC       BIC  logLik\n  -319.262 -234.5691 176.631\n\nCorrelation Structure: General\n Formula: ~1 | ID \n Parameter estimate(s):\n Correlation: \n  1     2     3     4    \n2 0.458                  \n3 0.372 0.511            \n4 0.441 0.437 0.663      \n5 0.468 0.443 0.597 0.764\nVariance function:\n Structure: Different standard deviations per stratum\n Formula: ~1 | agefac \n Parameter estimates:\n      11       12       13       14       15 \n1.000000 1.118479 1.414269 1.522510 1.560074 \n\nCoefficients:\n                 Value  Std.Error   t-value p-value\n(Intercept) -0.4557090 0.05465564 -8.337822       0\nAGE          0.0597274 0.00458344 13.031145       0\n\n Correlation: \n    (Intr)\nAGE -0.979\n\nStandardized residuals:\n         Min           Q1          Med           Q3          Max \n-1.482606297 -0.809004080 -0.006791942  0.840804584  4.082258054 \n\nResidual standard error: 0.1903187 \nDegrees of freedom: 1079 total; 1077 residual\n\n\nAnd our residual structure:\n\nsource('scripts/getVarCov2.R')\nmyvc = getVarCovFixedGls(modelUnrestricted,type=\"marginal\", individual=3)\nmyvc\n\nMarginal variance covariance matrix\n         [,1]     [,2]     [,3]     [,4]     [,5]\n[1,] 0.036221 0.018541 0.019071 0.024335 0.026466\n[2,] 0.018541 0.045313 0.029251 0.026924 0.027989\n[3,] 0.019071 0.029251 0.072448 0.051703 0.047736\n[4,] 0.024335 0.026924 0.051703 0.083962 0.065764\n[5,] 0.026466 0.027989 0.047736 0.065764 0.088156\n  Standard Deviations: 0.19032 0.21287 0.26916 0.28976 0.29691 \n\n\nAnd AIC:\n\nAIC( modelUnrestricted )\n\n[1] -319.262\n\n\nThis unrestricted covariance and correlation matrices have the same structures discussed in the book and in class. The AIC has improved by another 6 or 7 points; that’s marginally “significant”, but in practice probably not substantial enough to make up for the loss of interpretability. The lesson we should take from that is that there’s not a whole lot of room for improvement just by tinkering with the residual covariance structure; if we want a much better model, we would have to add new fixed or random effects; perhaps other covariates or perhaps a quadratic term in time."
  },
  {
    "objectID": "complex_error.html#mixing-ar1-and-random-slopes",
    "href": "complex_error.html#mixing-ar1-and-random-slopes",
    "title": "42  An overview of complex error structures",
    "section": "42.4 Mixing AR[1] and Random Slopes",
    "text": "42.4 Mixing AR[1] and Random Slopes\nLet’s look at an AR1 residual structure along with some covariates in our main model. The following has AR[1] and also a random intercept and slope:\n\nmodel1 = lme(fixed=ATTIT ~ EXPO + FEMALE + MINORITY + log(INCOME + 1), \n              data=nys1,\n              random=~AGE|ID,\n              correlation=corAR1()  )\n\nCompare to same model without AR1 correlation\n\nmodel1simple = lme(fixed=ATTIT ~ EXPO + FEMALE + MINORITY + log(INCOME + 1), \n             data=nys1,\n             random=~AGE|ID )\nscreenreg( list( model1, model1simple ) )\n\n\n=========================================\n                 Model 1      Model 2    \n-----------------------------------------\n(Intercept)         0.33 ***     0.32 ***\n                   (0.04)       (0.04)   \nEXPO                0.36 ***     0.37 ***\n                   (0.03)       (0.03)   \nFEMALE             -0.01        -0.01    \n                   (0.02)       (0.02)   \nMINORITY           -0.06 *      -0.06 *  \n                   (0.03)       (0.03)   \nlog(INCOME + 1)    -0.01        -0.01    \n                   (0.02)       (0.02)   \n-----------------------------------------\nAIC              -359.46      -350.54    \nBIC              -309.67      -305.73    \nLog Likelihood    189.73       184.27    \nNum. obs.        1079         1079       \nNum. groups: ID   239          239       \n=========================================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n\n\nThe AR1 model has notably lower AIC and thus is better. (A difference of ~9 in AIC, which can be interpreted as a factor of \\(e^{4.5}\\) in odds.) Here are the log likelihoods with degrees of freedom:\n\nlogLik( model1 )\n\n'log Lik.' 189.7308 (df=10)\n\nlogLik( model1simple )\n\n'log Lik.' 184.269 (df=9)\n\n\nOur model is actually kind of mixed up, conceptually. We allowed a random slope on age, and also an autoregressive component by age. Thus, we effectively allowed the covariance matrix to vary in two different ways, at two different levels of our modeling.\nIn fact, as we’ve seen in class, any random effects, whether they be on slope or intercept, are actually equivalent to certain ways of varying the [variance-]covariance matrix of the residuals within each group. For instance, random intercepts are equivalent to compound symmetry. Thus, by including both random intercepts and AR1 correlation in the above model, we’ve effectively fit a model that allows any covariance matrix that can be expressed as a sum of a random slope covariance matrix (with 2 parameters plus a scaling factor) and an AR1 covariance matrix (with 1 parameter plus a scaling factor). That makes 5 degrees of freedom total for our covariance matrix.\nBut conceptually this model is nice: people have linear growth trends, but vary around those growth trends in an autoregressive way."
  },
  {
    "objectID": "complex_error.html#the-kitchen-sink-building-complex-models",
    "href": "complex_error.html#the-kitchen-sink-building-complex-models",
    "title": "42  An overview of complex error structures",
    "section": "42.5 The Kitchen sink: building complex models",
    "text": "42.5 The Kitchen sink: building complex models\nWhich brings us to the next point: how do you actually use this stuff in practice. Ideally, you’d like both the interpretability (and robustness against MAR missingness) of hierarchical models, along with the ability to add additional residual structure such as AR[1] and/or heteroskedastic residuals. The good news is, you can get both. The bad news is, there’s a bit of a potential for bias due to overfitting.\nFor instance, imagine you use both random effects and AR[1]. Say that for a given subject you have 5 time points, and all of them are above the values you would have predicted based on fixed effects alone. That might be explained by an above-average random effect, or by a set of correlated residuals that all came in high. Whichever one of these is the “true” explanation, the MLE will tend to parcel it out between the two. This can lead to downward bias in variance and/or correlation parameter estimates, especially with small numbers of observations per subject.\nStill, as long as your focus is on location parameters such as true means or slopes, this can be a good way to proceed. Let’s explore this by first fitting a “kitchen sink” model for this data, in which we use all available covariates; and seeing how adding heteroskedasticity, AR[1] structure, or both changes it (or doesn’t).\nWhat do we want in this “kitchen sink” model? Let’s first fit a very simple random intercept model with fixed effects for gender, minority status, “exposure”, and log(income), to see which of these covariates to focus on. We use the lmerTest package to get some early \\(p\\)-values for these fixed effects.\n\nmodelKS0 = lmerTest::lmer(ATTIT ~ FEMALE + MINORITY + log(INCOME + 1) + EXPO + (1|ID), data=nys1)\nsummary(modelKS0, correlation=FALSE)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ATTIT ~ FEMALE + MINORITY + log(INCOME + 1) + EXPO + (1 | ID)\n   Data: nys1\n\nREML criterion at convergence: -265.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.1840 -0.5922 -0.0797  0.6043  2.6319 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept) 0.01756  0.1325  \n Residual             0.03444  0.1856  \nNumber of obs: 1079, groups:  ID, 239\n\nFixed effects:\n                  Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)      3.480e-01  4.195e-02  2.301e+02   8.297    9e-15 ***\nFEMALE          -1.835e-02  2.094e-02  2.327e+02  -0.876   0.3819    \nMINORITY        -5.698e-02  2.789e-02  2.279e+02  -2.043   0.0422 *  \nlog(INCOME + 1)  2.102e-03  2.449e-02  2.272e+02   0.086   0.9317    \nEXPO             4.516e-01  2.492e-02  1.041e+03  18.122   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n(The correlation=FALSE shortens the printout.)\nApparently, MINORITY and EXPO are the covariates with significant effects; minority status is correlated with a lower tolerance for deviance, while “deviant” friends are of course correlated positively with tolerance of deviance. Let’s build a few hierarchical models including these in various specifications (can you identify what models are what? Some of these models are not necessarily good choices). We first center our age so we have meaningful intercepts.\n\nnys1$age13 = nys1$AGE - 13\n\nmodelKS1 = lme(ATTIT ~ MINORITY,\n              data=nys1,\n              random=~age13 + EXPO|ID )\n\nmodelKS2 = lme(ATTIT ~ MINORITY + age13, \n              data=nys1,\n              random=~age13 + EXPO|ID )\n\nmodelKS3 = lme(ATTIT ~ MINORITY + age13, \n              data=nys1,\n              random=~EXPO|ID )\n\nmodelKS4 = lme(ATTIT ~ MINORITY + age13 + EXPO, \n              data=nys1,\n              random=~1|ID )\n\nAnd now we examine them:\n\nlibrary( texreg )\nscreenreg( list( modelKS1, modelKS2, modelKS3, modelKS4 ))\n\n\n===================================================================\n                 Model 1      Model 2      Model 3      Model 4    \n-------------------------------------------------------------------\n(Intercept)         0.24 ***     0.31 ***     0.29 ***     0.34 ***\n                   (0.01)       (0.01)       (0.01)       (0.01)   \nMINORITY           -0.05        -0.05 *      -0.04        -0.06 *  \n                   (0.02)       (0.02)       (0.03)       (0.03)   \nage13                            0.06 ***     0.05 ***     0.05 ***\n                                (0.00)       (0.00)       (0.00)   \nEXPO                                                       0.37 ***\n                                                          (0.02)   \n-------------------------------------------------------------------\nAIC              -305.22      -374.18      -312.13      -394.06    \nBIC              -260.38      -324.37      -277.27      -364.18    \nLog Likelihood    161.61       197.09       163.07       203.03    \nNum. obs.        1079         1079         1079         1079       \nNum. groups: ID   239          239          239          239       \n===================================================================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n\n\nOK, Number 4 seems like a pretty good model. Let’s see how much it improves when we add AR[1]:\n\nmodelKS5 = lme(ATTIT ~ MINORITY + age13 + EXPO, \n              data=nys1,\n              random=~1|ID,\n              correlation=corAR1(form=~AGE|ID) )\nAIC( modelKS5 )\n\n[1] -433.5943\n\nfixef( modelKS4 )\n\n(Intercept)    MINORITY       age13        EXPO \n 0.34054593 -0.05606947  0.04830698  0.36778951 \n\nfixef( modelKS5 )\n\n(Intercept)    MINORITY       age13        EXPO \n 0.34083507 -0.05704474  0.04733879  0.35207989 \n\n\nNote that the estimates for all the effects are essentially unchanged. However, the AIC is almost 40 points better. Also, because the model has done a better job explaining residual variance, the p-value for the coefficient on MINORITY has dropped from 0.032 to 0.029, as we can see on the summary display below. This is not a large drop, but a noticeable one:\n\nsummary( modelKS5 )\n\nLinear mixed-effects model fit by REML\n  Data: nys1 \n        AIC       BIC   logLik\n  -433.5943 -398.7338 223.7972\n\nRandom effects:\n Formula: ~1 | ID\n        (Intercept)  Residual\nStdDev:   0.1186009 0.1864592\n\nCorrelation Structure: ARMA(1,0)\n Formula: ~AGE | ID \n Parameter estimate(s):\n     Phi1 \n0.3212696 \nFixed effects:  ATTIT ~ MINORITY + age13 + EXPO \n                 Value   Std.Error  DF   t-value p-value\n(Intercept)  0.3408351 0.011858053 838 28.742920   0.000\nMINORITY    -0.0570447 0.025968587 237 -2.196683   0.029\nage13        0.0473388 0.004523745 838 10.464512   0.000\nEXPO         0.3520799 0.024451130 838 14.399330   0.000\n Correlation: \n         (Intr) MINORI age13 \nMINORITY -0.457              \nage13    -0.013  0.010       \nEXPO      0.006 -0.001 -0.224\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-2.82097989 -0.65136103 -0.08846076  0.61819746  2.77303796 \n\nNumber of Observations: 1079\nNumber of Groups: 239 \n\n\nIs any of this drop in the \\(p\\)-value due to overfitting? Given the size of the change in AIC, it seems doubtful that that’s a significant factor.\nLet’s try including heteroskedasticity, without AR[1]:\n\nmodelKS6 = lme(ATTIT ~ MINORITY + age13 + EXPO, \n              data=nys1,\n              random=~1|ID,\n              weights=varIdent(form=~1|agefac) )\nAIC( modelKS6 )\n\n[1] -389.5696\n\n\nThis did not improve AIC in this case, so we can avoid looking at this model further.\nFor completeness, let’s look at a model with both AR(1) and heteroskedasticity:\n\nmodelKS7 = lme(ATTIT ~ MINORITY + age13 + EXPO, \n              data=nys1,\n              random=~1|ID,\n              correlation=corAR1(form=~AGE|ID),\n              weights=varIdent(form=~1|agefac) )\nAIC( modelKS7 )\n\n[1] -431.1943\n\n\nAgain, no improvement. So we settle with our AR[1] model with a random intercept to get overall level of a student."
  },
  {
    "objectID": "lmer_optimization.html#convergence-and-optimization-algorithms",
    "href": "lmer_optimization.html#convergence-and-optimization-algorithms",
    "title": "43  Optimization Algorithms for MLMs",
    "section": "43.1 Convergence and optimization algorithms",
    "text": "43.1 Convergence and optimization algorithms\nUnlike OLS, which has a simple closed-form solution for parameter estimates, multi-level models are complex and often do not have closed-form solutions.1 As a result, programming languages use optimization algorithms to fit models. These optimization algorithms are typically iterative processes that repeatedly test potential values and eventually converge to the model estimates.\nTypically, optimization algorithms involve approximating the log-likelihood function as a multivariate quadratic function. Sometimes this approximation is easy to find and closely matches the true log-likelihood; in these cases, convergence occurs quickly. However, we’ve seen that convergence is trickier when the log-likelihood function is flat near the maximum; it’s also trickier with more complex and fragile likelihoods, like those created by the link functions from Generalized Least Squares (GLS) models."
  },
  {
    "objectID": "lmer_optimization.html#what-to-do-when-your-model-wont-converge",
    "href": "lmer_optimization.html#what-to-do-when-your-model-wont-converge",
    "title": "43  Optimization Algorithms for MLMs",
    "section": "43.2 What to do when your model won’t converge",
    "text": "43.2 What to do when your model won’t converge\nIf your error won’t converge, you might get a warning message like this:\nWarning message: In checkConv(attr(opt, “derivs”), opt\\(par, ctrl = control\\)checkConv, : Model failed to converge with max|grad| = 0.0463355 (tol = 0.001, component 1)\nThis warning message tells us two things. First, remember that we are trying to find the maximum of the likelihood function, or the place where the slope = 0. In the warning, the tol = 0.001 tells us that R will be happy if it finds estimates where the slope \\(\\leq\\) 0.001. It’s also saying that our slope when R stopped converging was 0.0463355.\nSteps that you can take to resolve:\nTo address items #2 and #3, you add a Control option into your lme, lmer, or glmer function. Each of those functions has its own option, but they all take the same arguments:\nBelow are some other optimizer options that you can try. For simplicitly, we’re specifying them all as “glmer” options, but you could easily adjust them to match whichever model you are trying (but failing) to fit:\n\n## Use a Nelder-Mead optimizer\nlog_mod &lt;- glmer(pass ~ (gender + frl_new + f3) + \n                   (gender + frl_new + f3|sch), \n                 data = wide_dat, family = binomial(),\n                 control = glmerControl(optimizer = 'Nelder_Mead'))\n\n## Use a BFGS optimizer \nlog_mod &lt;- glmer(pass ~ (gender + frl_new + f3) + \n                   (gender + frl_new + f3|sch), \n                 data = wide_dat, family = binomial(),\n                 control = glmerControl(optimizer=\"optim\", optimMethod = \"BFGS\"))\n\n#If these aren't working, you can downlaod a special package to use the optimx optimizer\n#install.packages(‘optimx’)\nlibrary(optimx)\nlog_mod &lt;- glmer(pass ~ (gender + frl_new + f3) + \n                   (gender + frl_new + f3|sch), \n                 data = wide_dat, family = binomial(),\n                 glmerControl(optimizer = 'optimx', calc.derivs = FALSE,\n                              optCtrl = list(method = \"L-BFGS-B\", \n                                             starttests = FALSE, \n                                             kkt = FALSE)))\n\nAside from these examples, there are many other ways to adjust your optimization commands, which can be found here: https://rdrr.io/cran/lme4/man/lmerControl.html"
  },
  {
    "objectID": "lmer_optimization.html#technical-appendix-understanding-the-types-of-optimization-algorithms",
    "href": "lmer_optimization.html#technical-appendix-understanding-the-types-of-optimization-algorithms",
    "title": "43  Optimization Algorithms for MLMs",
    "section": "43.3 Technical Appendix: Understanding the Types of Optimization Algorithms",
    "text": "43.3 Technical Appendix: Understanding the Types of Optimization Algorithms\nThere are generally four “types” of algorithms employed to find MLE/REML solutions:"
  },
  {
    "objectID": "lmer_optimization.html#newton-methods",
    "href": "lmer_optimization.html#newton-methods",
    "title": "43  Optimization Algorithms for MLMs",
    "section": "43.4 Newton Methods",
    "text": "43.4 Newton Methods\nNewton’s method is the most “pure” of these approaches; essentially Newton’s method uses a Taylor series approximation to approximate a quadratic function and find its maxima. It involves finding the Hessian (a matrix containing all the second and partial derivatives from your likelihood). An advantage of this approach is that it is theoretically the best of the three named approaches because it will often require fewer iterations to converge. However, there are two drawbacks:"
  },
  {
    "objectID": "lmer_optimization.html#quasi-newton-methods",
    "href": "lmer_optimization.html#quasi-newton-methods",
    "title": "43  Optimization Algorithms for MLMs",
    "section": "43.5 Quasi-Newton Methods",
    "text": "43.5 Quasi-Newton Methods\nQuasi-Newton methods start with a “guess” for the Hessian, apply the quadratic formula to attain a new point, update the guess of the Hessian, and repeat until convergence is attained. Importantly, the approximated Hessian will converge to the Hessian so long as the Wolfe conditions (a set of conditions on the likelihood) are satisfied. The easiest guess for the initial Hessian is the identity matrix, making the first step simply a gradient descent. When the identity matrix is used as an initial guess, the quasi-Newton methods converge “super-linearly”–that is it displays linear convergece initially, but approach quadratic convergence as the approximated Hessian updates itself. There are many quasi-Newton methods, but the most common is the “BFGS” updating method.\nIn terms of time to convergence, quasi-Newton is typically much faster than pure Newton methods. This addresses the first drawback listed for Newton’s method, but it is still susceptible to the second issue. The other potential challenge with Quasi-Newton methods occurs when the Wolfe conditions are not satisfied - the method will typically not converge to the Hessian within a reasonable number of iterations, and can often exceed the maximum iterations set by a program."
  },
  {
    "objectID": "lmer_optimization.html#em-expectation-maximiation-algorithm",
    "href": "lmer_optimization.html#em-expectation-maximiation-algorithm",
    "title": "43  Optimization Algorithms for MLMs",
    "section": "43.6 EM (Expectation-Maximiation) Algorithm",
    "text": "43.6 EM (Expectation-Maximiation) Algorithm\nThe EM algorithm is another way of approximating the likelihood function and maximizing that approximation. It does this in a repeating series of stseps: the E (Expectation) step and the M (Maximization) step. In random effect models, where normality is assumed, the E-step results in an a quadratic function to be maximized in the M-step. Importantly, each iteration of the EM algorithm is guaranteed to increase the likelihood function, a feature that that may be too difficult to attain with the Newton methods when a quadratic function is not yet a good approximation. Thus, even if the likelihood function not well approximated by a quadratic function, we are assured to be getting closer to a maximum with the EM algorithm. Thus the EM algorithm fixes the second issue from Newton’s method. However, it only displays linear convergence (as opposed to “super linear” or “quadratic”) and can therefore take a very long time to converge."
  },
  {
    "objectID": "lmer_optimization.html#implementation-in-different-programs",
    "href": "lmer_optimization.html#implementation-in-different-programs",
    "title": "43  Optimization Algorithms for MLMs",
    "section": "43.7 Implementation in Different Programs",
    "text": "43.7 Implementation in Different Programs\n\n43.7.1 Stata/MPlus/HLM\nStata, Mplus, and HLM, each use a combination of the EM and the quasi-Newton methods when estimating models with random effects. The algorithms start with the EM algorithm and proceed until there is sufficient concavity to switch a quasi-Newton method. Using a combination of the EM and quasi-Newton methods minimizes computational time while maximizing the opportunity that the algorithm will converge to a maximum. Mplus and HLM will even switch back to the EM algorithm if the Wolfe conditions are not attained in a set amount of time; thus, my experience has been that Mplus and HLM tend to converge the fastest and tend to minimize convergence issues.\nDisclaimer: sometimes you may need to manually increase the number of EM iterations allowed to acheive convergence.\n\n\n43.7.2 R\nIf I am interpreting the lmerControls documentation correctly, this method starts with the EM algorithm and then applies “unconstrained and box-constrained optimization using PORT routines” from the nlminb function. I’ll classify this algorithm as “other”, as opposed to the three named approaches above.\nIn my opinion, lme’s optimization algorithm is less than ideal for two reasons. First, the number of initial EM steps is fixed and who’s to say that the default number of EM iterations will bring us to a region where the log-likelihood function is sufficiently concave?\nSecond, HLM and Mplus have been estimating random effect models for a long time, and developers from both have come to the conclusion that the quasi-Newton method as the second method in a combination is the best for these models. I’ll assume this is a very informed decision on the end of these developers. Yet, it does not appear that this is what is occuring in R. Instead, R uses “unconstrained and box-constrained optimization using PORT routines,” whatever that is.\nEven the according to the “See Also” section in the nlminb help file, the optim function is listed as preferred over the nlminb function. As it turns out, the optim function applies the “BFGS” quasi-Newton method as the default, which is consistent with Stata’s approach."
  },
  {
    "objectID": "lmer_optimization.html#footnotes",
    "href": "lmer_optimization.html#footnotes",
    "title": "43  Optimization Algorithms for MLMs",
    "section": "",
    "text": "“Closed form” means that there is a formula you can use to simply and directly calculate your estimates. For example, in OLS your matrix equation for \\(\\hat{\\beta} = (X'X)^{-1}X'Y\\)↩︎"
  },
  {
    "objectID": "cluster_demo.html#robust-errors-no-clustering",
    "href": "cluster_demo.html#robust-errors-no-clustering",
    "title": "44  Walk-through of calculating robust standard errors",
    "section": "44.1 Robust errors (no clustering)",
    "text": "44.1 Robust errors (no clustering)\nThe (no clustering, ordinary) linear regression model assumes that\n\\[y = X\\beta + \\varepsilon\\]\nwith the \\(\\varepsilon\\)’s independently and identically normally distributed with variance \\(\\sigma^2\\). Here \\(\\beta\\) is a column vector of regression coefficients, \\((\\beta_0, \\beta_1)\\) in our example. \\(y\\) is a vector of the outcomes and \\(\\varepsilon\\) is a vector of the residuals. \\(X\\) is a \\(n\\) by \\(p\\) matrix referred to as the model matrix (p is the number of predictors, including the intercept). In this example, the first column of the matrix is all 1’s, for the intercept, and the second column is each person’s value for ses. The third is each person’s value for sector (which will be the same for all students in a single school).\n\ndat = read.spss( \"data/hsb1.sav\", to.data.frame=TRUE )\nsdat = read.spss( \"data/hsb2.sav\", to.data.frame=TRUE )\ndat = merge( dat, sdat, by=\"id\", all.x=TRUE )\ndat = dat[ c( \"id\", \"mathach\", \"ses\", \"sector\" ) ]\ndat$id &lt;- factor( dat$id ) ### make the school variable a factor\nhead( dat )\n\n    id mathach    ses sector\n1 1224   5.876 -1.528      0\n2 1224  19.708 -0.588      0\n3 1224  20.349 -0.528      0\n4 1224   8.781 -0.668      0\n5 1224  17.898 -0.158      0\n6 1224   4.583  0.022      0\n\n\nMaking a model matrix from a regression\n\nX &lt;- model.matrix( mathach ~ ses + sector, data = dat )\nhead( X )\n\n  (Intercept)    ses sector\n1           1 -1.528      0\n2           1 -0.588      0\n3           1 -0.528      0\n4           1 -0.668      0\n5           1 -0.158      0\n6           1  0.022      0\n\ny &lt;- dat$mathach\nhead( y )\n\n[1]  5.876 19.708 20.349  8.781 17.898  4.583\n\n\nWith these assumptions, our estimate for \\(\\beta\\) using the OLS criterion is \\(\\hat{\\beta} = (X^TX)^{-1}X^Ty\\). We can calculate this directly with R.\n\nsolve(t(X) %*% X) %*% t(X) %*% y ##(X'X)^{-1}X'y\n\n                 [,1]\n(Intercept) 11.793254\nses          2.948558\nsector       1.935013\n\n\nCompare with lm: they are the same!\n\nmod = lm(mathach ~ ses + sector, data = dat)\nmod \n\n\nCall:\nlm(formula = mathach ~ ses + sector, data = dat)\n\nCoefficients:\n(Intercept)          ses       sector  \n     11.793        2.949        1.935  \n\n\nWe can also estimate standard errors for the coefficients by taking \\(\\sqrt{\\hat{\\sigma}^2diag((X^TX)^{-1})}\\).\n\nbeta_hat &lt;- solve(t(X) %*% X) %*% t(X) %*% y\npreds &lt;- X %*% beta_hat\nresids &lt;- y - preds\nsigma_2_hat &lt;- sum(resids^2)/(nrow(X)-3) ### estimate of the residual variance\nsqrt(sigma_2_hat * diag(solve(t(X) %*% X))) ### using the matrix algebra\n\n(Intercept)         ses      sector \n 0.10610213  0.09783058  0.15249341 \n\n\nAgain, compare:\n\nlibrary( arm )\ndisplay( mod ) ### same results\n\nlm(formula = mathach ~ ses + sector, data = dat)\n            coef.est coef.se\n(Intercept) 11.79     0.11  \nses          2.95     0.10  \nsector       1.94     0.15  \n---\nn = 7185, k = 3\nresidual sd = 6.35, R-Squared = 0.15\n\n\nBut notice that this assumes that the residuals have a single variance, \\(\\sigma^2\\). Frequently this assumption is implausible, in which case the standard errors we derive may not be correct. It would be useful to have a way to derive standard errors which does not require us to assume that the residuals are homoscedastic. This is where heteroscedasticity-robust standard errors, or Huber-White standard errors, come in. Huber-White standard errors are asymptotically correct, even if the residual variance is not constant at all values of the predictor.\nThe basic idea behind Huber-White standard errors is that we let each individual residual serve as an estimate of the variance of the residuals at that value of the predictors. If we let \\(V = (X^TX)^{-1},\\) \\(N\\) be the number of observations, and \\(K\\) be the number of predictors, including the intercept, then the formula for the standard errors is\n\\[ SE^2 = \\frac{N}{N-K} \\cdot diag\\left( V \\cdot \\left( \\sum X_i X_i^T \\varepsilon_i^2 \\right) \\cdot V\\right) \\]\nThis is called a sandwich estimator, where \\(V\\) is the bread and \\(\\sum X_i X_i^T \\varepsilon_i^2\\) (which is a \\(K\\) by \\(K\\) matrix) is the meat. Below, we implement this in R.\n\nN &lt;- nrow(dat) ### number of observations\nK &lt;- 3 ### number of regression coefficients, including the intercept\nV &lt;- solve(t(X) %*% X) ### the bread\nV\n\n              (Intercept)           ses        sector\n(Intercept)  2.796108e-04  3.460078e-05 -0.0002847979\nses          3.460078e-05  2.377141e-04 -0.0000702375\nsector      -2.847979e-04 -7.023750e-05  0.0005775742\n\nmeat &lt;- matrix(0, nrow = K, ncol = K) ### we'll build the meat as we go, iterating over the \n                                      ### individual rows\nfor(i in 1:nrow(dat)){\n  this_point &lt;- X[i, ] %*% t(X[i, ]) * resids[i]^2 ### the contribution of this particular \n                                                   ### point\n  meat &lt;- meat + this_point ### take the current meat, and add this point's contribution\n}\nmeat\n\n     (Intercept)        ses     sector\n[1,]  289161.019  -3048.176 133136.299\n[2,]   -3048.176 159558.729   9732.201\n[3,]  133136.299   9732.201 133136.299\n\nSEs = sqrt(diag(N/(N-K) * V %*% meat %*% V)) ### standard errors\nSEs\n\n(Intercept)         ses      sector \n 0.11021454  0.09487279  0.15476724 \n\n\nNotice that the estimated standard errors haven’t changed much, so whatever heteroscedasticity is present in this association doesn’t seem to be affecting them.\nCombining the above steps in a tidy bit of code gives:\n\nmod &lt;- lm(mathach ~ ses + sector, data = dat)\nresids = resid( mod )\n\nX &lt;- model.matrix(mathach ~ ses + sector, data = dat)\n\nV &lt;- solve(t(X) %*% X) ### the bread\nvcov_hw = V %*% t(X) %*% diag(resids^2) %*% X %*% V\n\nvcov_hw\n\n             (Intercept)          ses       sector\n(Intercept)  0.012142174  0.001957716 -0.012535538\nses          0.001957716  0.008997088 -0.003992666\nsector      -0.012535538 -0.003992666  0.023942897\n\nsqrt(diag(vcov_hw)) ### standard errors\n\n(Intercept)         ses      sector \n 0.11019153  0.09485298  0.15473493 \n\nsqrt( diag( vcov( mod ) ) )\n\n(Intercept)         ses      sector \n 0.10610213  0.09783058  0.15249341 \n\n\n\n44.1.1 R Packages to do all this for you\nThere is an R package to do all of this for us. The following gives us the “Variance Covariance” matrix:\n\nlibrary(sandwich)\nvc &lt;- vcovHC( mod, type = \"HC0\")\nprint( vc, digits=3 )\n\n            (Intercept)      ses   sector\n(Intercept)     0.01214  0.00196 -0.01254\nses             0.00196  0.00900 -0.00399\nsector         -0.01254 -0.00399  0.02394\n\n\nThe square root of the diagonal are our standard errors\n\nsqrt( diag( vc ) )\n\n(Intercept)         ses      sector \n 0.11019153  0.09485298  0.15473493 \n\n\nThey are what we hand-calculated above (up to some rounding error). Observe how the differences are all very close to zero:\n\nsqrt( diag( vc ) ) - SEs\n\n  (Intercept)           ses        sector \n-2.301170e-05 -1.980850e-05 -3.231386e-05 \n\n\nWe can use them for testing as follows\n\nlibrary( lmtest )\ncoeftest( mod, vcov. = vc )\n\n\nt test of coefficients:\n\n             Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) 11.793254   0.110192 107.025 &lt; 2.2e-16 ***\nses          2.948558   0.094853  31.086 &lt; 2.2e-16 ***\nsector       1.935013   0.154735  12.505 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n(Note the weird “.”. I don’t know why it is part of the name.)\nIn fact, these packages play well together, so you can tell lmtest to use the vcovHC function as follows:\n\ncoeftest( mod, vcov. = vcovHC )\n\n\nt test of coefficients:\n\n             Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) 11.793254   0.110237 106.981 &lt; 2.2e-16 ***\nses          2.948558   0.094913  31.066 &lt; 2.2e-16 ***\nsector       1.935013   0.154801  12.500 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAll this is well and good, but everything we have done so far is WRONG because we have failed to account for the clustering of students within schools. Huber-White (Sandwich) corrections only deal with heteroskedasticity, not clustering. We extend these ideas to do clustering next."
  },
  {
    "objectID": "cluster_demo.html#cluster-robust-standard-errors",
    "href": "cluster_demo.html#cluster-robust-standard-errors",
    "title": "44  Walk-through of calculating robust standard errors",
    "section": "44.2 Cluster Robust Standard Errors",
    "text": "44.2 Cluster Robust Standard Errors\nThe next step is to get standard errors which allow the residuals to be correlated within clusters and to have non-0 means within clusters (which violates the assumption of independence of residuals). The math here is harder to explain. We start by calculating \\(X*\\varepsilon\\), multiplying each row in \\(X\\) by the associated residual. Then we take the column sum of \\(X\\) within each cluster. This is easiest to understand for the intercept column, where the sum is simply equal to the sum of the residuals in that cluster. If all of the residuals in a cluster are large and positive (or large and negative), then this sum will be very large; if the residuals are close to mean 0 in a cluster, the sum will be small. We then bind the results into a \\(M\\) by \\(K\\) matrix, where \\(M\\) is the number of clusters, each row corresponds to a cluster, and each column corresponds to a coefficient, which we’ll call \\(U\\). This is the meat which we sandwich with \\(V\\). Finally, we take\n\\[\\sqrt{ diag( \\frac{M}{M-1}\\frac{N-1}{N-K} VU^TUV)}\\]\nwhich gives us estimated standard errors for the regression coefficients.\nThe intuition isn’t so clear here, but notice that the more highly correlated residuals are within clusters (especially clusters with extreme values of the predictors), the larger \\(U^TU\\) will be, and the less precise our estimates.\nHere’s a “by hand” implementation in R.\n\ncluster &lt;- dat$id\nM &lt;- length(unique(cluster))\nweight_mat &lt;- as.vector(resids) * X ### start by calculating for each X predictor values \n                                    ### weighted by the residuals\nhead( weight_mat )\n\n  (Intercept)        ses sector\n1   -1.411858  2.1573194      0\n2    9.648498 -5.6733165      0\n3   10.112584 -5.3394444      0\n4   -1.042618  0.6964687      0\n5    6.570618 -1.0381576      0\n6   -7.275123 -0.1600527      0\n\nu_icept &lt;- tapply(weight_mat[, '(Intercept)'], cluster, sum) ### sum up the weighted \n                                                             ### intercepts in each cluster\nu_ses &lt;- tapply(weight_mat[, 'ses'], cluster, sum) ### sum up the weighted slopes in \n                                                       ### each cluster\nu_sector &lt;- tapply(weight_mat[, 'sector'], cluster, sum)\n\nu &lt;- cbind(u_icept, u_ses, u_sector)\n\n### cluster-robust standard errors\nSE.adj.hand = sqrt((M/(M-1))*((N-1)/(N-K)) * diag(V %*% t(u) %*% u %*% V)) \nSE.adj.hand\n\n(Intercept)         ses      sector \n  0.2031455   0.1279373   0.3171766 \n\n\nThese are a lot higher than before; there’s a lot of within-cluster correlation, and our OLS-based estimated standard errors are unrealistically small.\nYou can use these standard errors in general if you’re not interested in modeling what’s happening at the cluster level and just want to get the right standard errors for your fixed effects.\n\n44.2.1 Using R Packages\nThere is a package that gives you the cluster-robust estimate of the variance-covariance matrix. You can then use this matrix to get your adjusted standard errors:\n\nlibrary( multiwayvcov )\n\nm1 &lt;- lm( mathach ~ ses + sector, data=dat )\nvcov_id &lt;- cluster.vcov(m1, dat$id)\ncoeftest(m1, vcov_id)\n\n\nt test of coefficients:\n\n            Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) 11.79325    0.20315 58.0532 &lt; 2.2e-16 ***\nses          2.94856    0.12794 23.0469 &lt; 2.2e-16 ***\nsector       1.93501    0.31718  6.1007 1.111e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCompare to if we ignored clustering:\n\ncoeftest( m1 )  ## BAD!!\n\n\nt test of coefficients:\n\n             Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) 11.793254   0.106102 111.150 &lt; 2.2e-16 ***\nses          2.948558   0.097831  30.139 &lt; 2.2e-16 ***\nsector       1.935013   0.152493  12.689 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe can look at how much bigger they are:\n\nSE.adj = sqrt( diag( vcov_id ) )\nSE.bad = sqrt( diag( vcov( m1 ) ) )\nSE.adj / SE.bad\n\n(Intercept)         ses      sector \n   1.914623    1.307743    2.079937 \n\n\nMore than 100% bigger for our sector variable and intercept. The ses variable is less so, since it varies within cluster.\nFinally, we check to see that our hand-calculation is the same as the package:\n\nSE.adj.hand - SE.adj\n\n  (Intercept)           ses        sector \n 1.296185e-14 -3.025358e-15 -2.997602e-15 \n\n\nUp to rounding errors, we are the same!\n\n\n44.2.2 Aside: Making your own function\nThe following is code to generate the var-cor matrix more efficiently. For reference (or to ignore):\n\n cl &lt;- function(dat, fm, cluster){\n   attach(dat, warn.conflicts = F)\n   require(sandwich)\n   require(lmtest)\n   M &lt;- length(unique(cluster))\n   N &lt;- length(cluster)\n   K &lt;- fm$rank\n   dfc &lt;- (M/(M-1))*((N-1)/(N-K))\n   uj  &lt;- apply(estfun(fm), 2, function(x) \n                       tapply(x, cluster, sum));\n   vcovCL &lt;- dfc*sandwich(fm, meat=crossprod(uj)/N)\n   coeftest(fm, vcovCL)\n }\n \ncl(dat, mod, dat$id)\n\n\nt test of coefficients:\n\n            Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) 11.79325    0.20315 58.0532 &lt; 2.2e-16 ***\nses          2.94856    0.12794 23.0469 &lt; 2.2e-16 ***\nsector       1.93501    0.31718  6.1007 1.111e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "peer_grading.html",
    "href": "peer_grading.html",
    "title": "45  Peer Grading",
    "section": "",
    "text": "Grading should be quick work. The idea is to develop some facility in responding to statistical arguments. We ask you to leave a sentence or two explaining your grade and commenting on any notable points raised, whether interesting or questionable. If there are any problems with Canvas, we’ll be watching on Piazza to respond to questions."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Antonakis, John, Nicolas Bastardoz, and Mikko Rönkkö. 2019. “On\nIgnoring the Random Effects Assumption in Multilevel Models: Review,\nCritique, and Recommendations.” Organizational Research\nMethods 24 (2): 443–83. https://doi.org/10.1177/1094428119877457."
  },
  {
    "objectID": "self_assess.html",
    "href": "self_assess.html",
    "title": "1  Do I take S-043? A self-assessment",
    "section": "",
    "text": "2 How to decide to take the course?\nI received numerous inquiries as to whether a given background is enough to take this course. Let me elaborate at length. The stated prerequisite to this course is S052, Stat 139, or equivalent, i.e., you have gone a bit beyond an applied course on linear regression (S040). Technically, if you have taken S040 or equivalent, you could take this course; the \"a bit beyond\" part is just wanting a bit more comfort with these foundational skills. More specifically, we greatly prefer you to have the following skills as a prerequisite for this course:\nYou will have an easier time if you have some of the following skills and experiences as well:\nIf you have a strong mathematical background, or strong comfort with math, or if you have a strong computer programming background, or strong comfort with that, then you can likely get a lot out of this course even if you do not have some of the above skills and knowledge.\nPeople from many, many different backgrounds take S043/Stat151. In general, the more background experience you have, the easier the course.  Even if you don't have as solid a background, you can still get a lot out of this course if you are willing to sign on for an intense experience. But please don’t be surprised if you sign up for an intense experience… and it is really intense!\nTo get a very rough sense of what it might feel like, take the quiz below.\nTrying to figure out how S43 will be for you? ​Add up the points across the following categories:"
  },
  {
    "objectID": "self_assess.html#total-the-above",
    "href": "self_assess.html#total-the-above",
    "title": "1  Do I take S-043? A self-assessment",
    "section": "3.1 Total the above",
    "text": "3.1 Total the above\nA VERY ROUGH recommendation is:\n&lt; 4:      Danger! Please email the instructor to talk about how this might go for you.\n4-6:      It will be really hard! You could take this course, but will likely find it will take much more time than a typical course. We would support you through this, but be warned that this could feel like a lot to take on.\n7-9:      It will be hard! There are lots of folks like you who are taking this course. The course will likely be a fair bit of work and could feel confusing/overwhelming at times. We would support you through this. At the end you will have learned a lot if you stick with it.\n10+:     It will probably feel like a normal course. No reservations. You can either work a reasonable amount and learn a lot about data science, or dig deeper to really go far with the skills we cover.\n14+:     It will probably be a cake-walk. You will learn some concepts but not have to work particularly hard in this course. We would still love to have you!\nStudents have historically said this course teaches you a lot; the question is just whether you have the time to allocate for the course. This quiz helps assess the time."
  }
]