[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Resources for S-043: Multilevel and Longitudinal Models",
    "section": "",
    "text": "Preface\nThis online book has a bunch of resources for S-043: Multilevel and Longitudinal Models. The book is written in Quarto, and is basically a bunch of handouts stapled together. It is very much a work in progress. If you notice errors, please notify us at:\nlmiratrix@gse.harvard.edu\njoshua_gilbert@g.harvard.edu"
  },
  {
    "objectID": "self_assess.html#how-to-decide-to-take-the-course",
    "href": "self_assess.html#how-to-decide-to-take-the-course",
    "title": "1  Do I take S-043? A self-assessment",
    "section": "1.1 How to decide to take the course?",
    "text": "1.1 How to decide to take the course?\nI received numerous inquiries as to whether a given background is enough to take this course. Let me elaborate at length. The stated prerequisite to this course is S052, Stat 139, or equivalent, i.e., you have gone a bit beyond an applied course on linear regression (S040). Technically, if you have taken S040 or equivalent, you could take this course; the “a bit beyond” part is just wanting a bit more comfort with these foundational skills. More specifically, we greatly prefer you to have the following skills as a prerequisite for this course:\n\nYou can calculate summary statistics and visualizations for data (the mean, standard deviation, scatterplots, histograms) using some sort of statistical software.\nYou can load data into some sort of statistical software, use that software to fit a regression model, and then interpret the results of the model.\nYou know how to include categorical covariates in a linear regression.\nYou know what an interaction term in a linear regression model is, and ideally can add one to a model you are fitting.\nYou can interpret confidence intervals and a p-values.\nYou can write down a regression equation and identify what the covariates and parameters are in a presented regression equation.\n\nYou will have an easier time if you have some of the following skills and experiences as well:\n\nYou have some experience with doing things in R (even a little helps here).\nYou know what logistic regression is, and know how to fit a logistic regression model using some sort of statistical software.\nYou have seen random intercept models before, as perhaps in S-052.\nYou have seen regression with fixed effects for groups, as perhaps in an econometrics course.\nYou have some experience doing quantitative research with real data in almost any capacity.\n\nIf you have a strong mathematical background, or strong comfort with math, or if you have a strong computer programming background, or strong comfort with that, then you can likely get a lot out of this course even if you do not have some of the above skills and knowledge.\nPeople from many, many different backgrounds take S043/Stat151. In general, the more background experience you have, the easier the course.  Even if you don’t have as solid a background, you can still get a lot out of this course if you are willing to sign on for an intense experience. But please don’t be surprised if you sign up for an intense experience… and it is really intense!\nTo get a very rough sense of what it might feel like, take the quiz below."
  },
  {
    "objectID": "self_assess.html#a-self-assessment-quiz",
    "href": "self_assess.html#a-self-assessment-quiz",
    "title": "1  Do I take S-043? A self-assessment",
    "section": "A self-assessment quiz",
    "text": "A self-assessment quiz\nTrying to figure out how S43 will be for you? ​Add up the points across the following categories:\n\nWork Experience\n+0        I have no work experience with quantitative data, or\n+1        I have some work experience with quantitative data, or\n+2        I have substantial work experience with quantitative data\n\n\nStat Courses\n-2        I have no prior stat experience under my belt, or\n+1        I have taken a very intro stat course (e.g., S12) or have at least a little stat knowledge, or\n+3        I have taken a linear regression course (e.g., S30, S40), or\n+5        I have taken an intermediate or advanced stats course (e.g., S52 or beyond)\n+1 bonus if concurrently enrolling in S052\n+1 bonus if classes were comfortable for you\n\n\nProgramming\n+0        I have basically no experience doing computer programming, or\n+1        I have some experience doing computer programming, or\n+2        I have a lot of experience doing computer programming\n\n\nR Skills\n+0        I have no real experience with R, or\n+1        I have a small bit of experience with R (e.g., ran scripts of it in S040), or\n+2        I have some experience with R (e.g., played around with scripts a bit in S040), or\n+3        I have substantial experience with R (e.g., write my own R code for my work)\n+1 bonus if learning R has been comfortable for you\n+1 bonus if you are comfortable with something like STATA\n\n\nMath\n+0        I don’t recall much math from my past education, or\n+1        I have taken (and somewhat remember) Calculus, or\n+2        I have taken classes beyond Calculus\n+1 bonus if classes were comfortable for you\n\n\nOther\n+2        I am content with getting a B or taking the class SAT/UNSAT"
  },
  {
    "objectID": "self_assess.html#total-the-above",
    "href": "self_assess.html#total-the-above",
    "title": "1  Do I take S-043? A self-assessment",
    "section": "1.2 Total the above",
    "text": "1.2 Total the above\nA VERY ROUGH recommendation is:\n&lt; 4:      Danger! Please email the instructor to talk about how this might go for you.\n4-6:      It will be really hard! You could take this course, but will likely find it will take much more time than a typical course. We would support you through this, but be warned that this could feel like a lot to take on.\n7-9:      It will be hard! There are lots of folks like you who are taking this course. The course will likely be a fair bit of work and could feel confusing/overwhelming at times. We would support you through this. At the end you will have learned a lot if you stick with it.\n10+:     It will probably feel like a normal course. No reservations. You can either work a reasonable amount and learn a lot about data science, or dig deeper to really go far with the skills we cover.\n14+:     It will probably be a cake-walk. You will learn some concepts but not have to work particularly hard in this course. We would still love to have you!\nStudents have historically said this course teaches you a lot; the question is just whether you have the time to allocate for the course. This quiz helps assess the time."
  },
  {
    "objectID": "r_basics.html",
    "href": "r_basics.html",
    "title": "R and R Markdown",
    "section": "",
    "text": "Some of you have never used R before. Some may have used R for years. This section contains some tutorials on basic R functionality that will be helpful for data wrangling and visualization and will lay a strong foundation for the MLM applications later on.\nAnother great resource is the free online book R for Data Science."
  },
  {
    "objectID": "intro_markdown.html",
    "href": "intro_markdown.html",
    "title": "2  Intro to R Markdown",
    "section": "",
    "text": "3 A big section\nwhile this\nproduces this:"
  },
  {
    "objectID": "intro_markdown.html#overview",
    "href": "intro_markdown.html#overview",
    "title": "2  Intro to R Markdown",
    "section": "2.1 Overview",
    "text": "2.1 Overview\nR Markdown (and its newer cousin Quarto) is a simple but powerful markdown language which you can use to create documents with inline R code and results. This makes it much easier for you to complete homework assignments and reports; makes it much less likely that your work will include errors; and makes your work much easier to reproduce. For example, if you find you have to drop cases from your dataset, you can simply add that line of code to your document, and recompile your document. Any text that’s drawn directly from your analyses will be automatically updated.\nOther R packages, such as Sweave and knitr, allow you to do the same things, but R Markdown has the added advantage of being relatively simple to use. This document will show you how to use R Markdown to create documents which draw directly on your data to produce reports."
  },
  {
    "objectID": "intro_markdown.html#getting-started",
    "href": "intro_markdown.html#getting-started",
    "title": "2  Intro to R Markdown",
    "section": "2.2 Getting started",
    "text": "2.2 Getting started\nEvery R Markdown document starts with a header. Headers look like this:\n---\ntitle: \"My perfect homework\"\nauthor: \"R master\"\noutput: pdf_document\n---\nA header can contain more or less information, as you see fit. Your computer needs to have a copy of LaTex installed in order to output .pdf documents. If you don’t, you should change output: pdf_document to output: html_document or output: word_document.\nYou identify sections of the document using hashtags; more hashtags indicate less important sections.\nFor example, this:\n# A big section\nproduces this:"
  },
  {
    "objectID": "intro_markdown.html#a-small-section",
    "href": "intro_markdown.html#a-small-section",
    "title": "2  Intro to R Markdown",
    "section": "3.1 A small section",
    "text": "3.1 A small section\nIf your document includes a table of contents, the sections get used to automatically generate the table of contents.\nYou can italicize words by writing *italicize* or _italicize_. You can bold words with **bold** or __bold__. N.B. Newer versions of Markdown and Quarto have a visual editor that allows you to format things in the usual way, e.g., control-B for bold. We generally recommend using the visual editor.\nYou can add superscripts (E=mc2) by writing E=mc^2^.\nYou can create unordered lists:\n- Item 1\n- Item 2\n- Item 3\nto get\n\nItem 1\nItem 2\nItem 3\n\nOr ordered lists:\n1. Item 1\n2. Item 2\n3. Item 3\nto get\n\nItem 1\nItem 2\nItem 3\n\nTo start a new page, just type \\newpage (not relevant for HTML output).\nAs you may have noticed, one of the driving ideas behind R Markdown is that the text should be interpretable even if it’s not compiled. A person should be able to read this text file and understand the basic organization and what all of the symbols denote.\nYou can also add links and images, and do many other things beyond what we’ll show you in this class. There are many resources out there, but here’s one place you can start.\nTo compile or knit the document, click on the button that says Knit or Render, or Shift + Ctrl/Cmd + K."
  },
  {
    "objectID": "intro_markdown.html#embedding-r-code",
    "href": "intro_markdown.html#embedding-r-code",
    "title": "2  Intro to R Markdown",
    "section": "3.2 Embedding R code",
    "text": "3.2 Embedding R code\nThere are two main ways to embed R code in R Markdown, code chunks or inline.\n\n3.2.1 Code chunks\nTo insert a code chunk click on Insert on the top right corner of your R Markdown file and select R. Or use keyboard shortcuts: Ctrl + Alt + I for PC and Cmd + Option + I for Mac:\nCode chunks have a number of different options. The most important ones for us are:\n\neval = TRUE, which means every time you knit the file, the code inside the R code chunk will get evaluated. This is the default.\necho = TRUE, which means every time you knit the file, the code inside the R code chunk witll be rendered, and you can see both the code itself and the results from evaluating the code.\n\nFor class, you should keep echo = TRUE, so that we can see your code and be able to tell what went wrong, if something did. You can set echo = FALSE for code chunks that load and manipulate data.\nOther code chunks options you may see in class are:\n\nwarning = FALSE, which means warning messages generated by the code will not be displayed.\nresults = 'asis', which means results will not be reformatted when the file is compiled (useful if results return raw HTLM).\nfig.height and fig.width, which specify the height and width (in inches) of plots created by the chunk.\n\nLet’s try loading some data:\n\nlibrary(haven)\ndat &lt;- read_dta(\"data/neighborhood.dta\")\n\nYou can see the code is displayed, and the command is carried out. The file dat is loaded in the R environment.\nInstead of specifying code chunks options every time, you can specify them globally in the setup chunk by using knitr::opts_chunk$set(echo = TRUE, eval = TRUE). You can then add additional options only to relevant chunks. If you want to exclude specific chunks, you can re-set echo = FALSE and eval = FALSE for those specific chunks.\nRunning code chunks: A good practice is to run individual code chunks to make sure they are doing what you want them to do. You can do this by executing individual lines of code, or whole chunks. Go to Run in the upper right corner and select what chunks to execute, e.g. Run Current Chunk, Run Next Chunk, etc.\n\n\n3.2.2 Inline code\nCode results can also be inserted directly in the text of your R Markdown file. This is particularly useful when you are extracting and interpreting model parameters. You can extract the coefficient from the model and use inline code to report it. If the data or model change, the text will change too when you knit the document.\nTo add inline code, enclose it in `r `. For example, to report the mean reading score, you can use\n\n`r mean(dat$p7read)`\n\nWhich will produce -0.0443549. That’s a few too many decimals, let’s round it too using\n\n`r round(mean(dat$p7read),2)`\n\nwhich produces -0.04”.\nHere we used two commands: round and mean. You can use more commands and write more complex inline code, depending on what you want to report."
  },
  {
    "objectID": "intro_markdown.html#embedding-plots",
    "href": "intro_markdown.html#embedding-plots",
    "title": "2  Intro to R Markdown",
    "section": "3.3 Embedding plots",
    "text": "3.3 Embedding plots\nPlots are easy to embed too. For example,\n\nlibrary(ggplot2)\n\ndat$male &lt;- factor(dat$male, levels = c(0, 1), labels = c(\"Female\", \"Male\"))\n  \nggplot(data=dat, aes(p7vrq, attain, colour=male)) + \n  geom_point() + \n  labs(title=\"Attainment as a function of verbal reasoning\",\n       x = \"Verbal reasoning quotient\", \n       y = \"Educational attainment\", colour=\"Gender\") +\n  geom_smooth(method=\"lm\", se=FALSE, colour=\"darkorchid3\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nGirls are rendered as coral, boys are rendered in turquoise, and the line of best fit is drawn in darkorchid3 (because why not). Just because you have a lot of colors and plotting characters to work with doesn’t mean you need to use them all. In the options, I specified fig.width = 7 and fig.height = 7. Notice that this command draws on dat, which we loaded in a previous chunk. When knitting the document, code chunks get executed in order and the results persist throughout the R Markdown document.\nFor the purposes of class, we want to see both your plot code and the plot itself. It’s not uncommon to use wrong code to create a plot that looks correct (at least visually)."
  },
  {
    "objectID": "intro_markdown.html#embedding-tables",
    "href": "intro_markdown.html#embedding-tables",
    "title": "2  Intro to R Markdown",
    "section": "3.4 Embedding tables",
    "text": "3.4 Embedding tables\nFinally, you can directly render tables in R Markdown. There are many different packages, but in class we’ll mostly use texreg and stargazer. (NB the newer function tab_model from sjPlot is excellent as well!)\nYou can use these packages to create a descriptive table. For example:\n# Here I specified results = asis. If I didn't stargazer will just render the table in html\n# I also specified message = FALSE, so that the package citation gets suppressed\nlibrary(stargazer)\nstargazer(dat, type = \"latex\")\n% Table created by stargazer v.5.2.3 by Marek Hlavac, Social Policy Institute. E-mail: marek.hlavac at gmail.com % Date and time: Thu, Aug 24, 2023 - 17:50:28\nWe can also use texreg and stargazer to create a taxonomy of regression models. We recommend texreg, which automatically outputs the variances of random effects (more on this soon).\nFor example:\n\nlibrary(texreg)\n\n# fit some models \nm1 &lt;- lm(attain ~ male, data=dat)\nm2 &lt;- lm(attain ~ male + momed, data=dat)\nm3 &lt;- lm(attain ~ male + momed + daded, data=dat)\n\nscreenreg(list(m1,m2,m3), \n          custom.coef.names=c(\"Intercept\", \"Male\", \"Maternal education\", \"Paternal education\"))\n\n\n=========================================================\n                    Model 1      Model 2      Model 3    \n---------------------------------------------------------\nIntercept              0.15 ***     0.03        -0.02    \n                      (0.03)       (0.03)       (0.03)   \nMale                  -0.12 **     -0.12 **     -0.12 ** \n                      (0.04)       (0.04)       (0.04)   \nMaternal education                  0.49 ***     0.24 ***\n                                   (0.05)       (0.05)   \nPaternal education                               0.54 ***\n                                                (0.06)   \n---------------------------------------------------------\nR^2                    0.00         0.05         0.09    \nAdj. R^2               0.00         0.05         0.08    \nNum. obs.           2310         2310         2310       \n=========================================================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n\n\nBoth packages include a lot of options and make it easy to produce publication-quality tables with little effort. We have provided more resources on Canvas and in other portions of this book."
  },
  {
    "objectID": "intro_markdown.html#embedding-mathematical-models",
    "href": "intro_markdown.html#embedding-mathematical-models",
    "title": "2  Intro to R Markdown",
    "section": "3.5 Embedding mathematical models",
    "text": "3.5 Embedding mathematical models\nWe’ll be writing a lot of mathematical models in class. R Markdown can use LaTeX style math-writing to display mathematical script. Another chapter in the book has more resources with LaTeXsyntax for the mostly commonly used models in the class. Similar to code chunks and inline code, you can use LaTeX for single or multiple equations, or for individual parameters embedded in the text.\nFor example, the following statement\n$$Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i$$\ncompiles to\n\\[Y_i = \\beta_0 + \\beta_1 Y_i + \\epsilon_i\\]\nAnd the following statement $\\mu$ compiles to \\(\\mu\\). This will be very helpful when we ask you to match R output to model parameters in homework."
  },
  {
    "objectID": "intro_linear_regression.html#simple-regression",
    "href": "intro_linear_regression.html#simple-regression",
    "title": "3  Intro to Regression",
    "section": "3.1 Simple Regression",
    "text": "3.1 Simple Regression\nWe are going to use an example dataset, RestaurantTips, that records tip amounts for a series of bills. Let’s first regress Tip on Bill. Before doing regression, we should plot the data to make sure using simple linear regression is reasonable. For kicks, we add in an automatic regression line as well by taking advantage of ggplot’s geom_smooth() method:\n\n# load the data into memory\ndata(RestaurantTips)\n\n# plot Tip on Bill\nggplot( RestaurantTips, aes(x = Bill, y = Tip) ) +\n    geom_point() +\n    geom_smooth( method=\"lm\", se=FALSE ) +\n    geom_smooth( method=\"loess\", se=FALSE, col=\"orange\" ) +\n    labs(title = \"Tip given Bill\")\n\n\n\n\n\n\n\n\nThat looks pretty darn linear! There are a few unusually large tips, but no extreme outliers, and variability appears to be constant at all levels of Bill , so we proceed:\n\n# fit the linear model\nmod &lt;- lm(Tip ~ Bill, data = RestaurantTips)\nsummary(mod)\n\n\nCall:\nlm(formula = Tip ~ Bill, data = RestaurantTips)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-2.391 -0.489 -0.111  0.284  5.974 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.29227    0.16616   -1.76    0.081 .  \nBill         0.18221    0.00645   28.25   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.98 on 155 degrees of freedom\nMultiple R-squared:  0.837, Adjusted R-squared:  0.836 \nF-statistic:  798 on 1 and 155 DF,  p-value: &lt;2e-16\n\n\nThe first line tells R to fit the regression. The thing on the left of the ~ is our outcome, the things on the right are our covariates or predictors. R then saves the results of all that work under the name mod (short for model - you can call it anything you want). Once we fit the model, we used summary() command to print the output to the screen.\nResults relevant to the intercept are in the (Intercept) row and results relevant to the slope are in the Bill row (Bill is the explanatory variable). The Estimate column gives the estimated coefficients, the Std. Error column gives the standard error for these estimates, the t value is simply estimate/SE, and the p-value is the result of a hypothesis test testing whether that coefficient is significantly different from 0.\nWe also see the RMSE as Residual standard error and \\(R^2\\) as Multiple R-squared. The last line of the regression output gives details relevant to an ANOVA table for testing our model against no model. It has the F-statistic, degrees of freedom, and p-value.\nYou can pull the coefficients of your model out with the coef() command:\n\ncoef(mod)\n\n(Intercept)        Bill \n     -0.292       0.182 \n\ncoef(mod)[1] # intercept\n\n(Intercept) \n     -0.292 \n\ncoef(mod)[2] # slope\n\n Bill \n0.182 \n\ncoef(mod)[\"Bill\"] # alternate way.\n\n Bill \n0.182 \n\n\nAlternatively, you can use the tidy() function from broom to turn the regression results into a tidy data frame, which makes it easier to work with:\n\ntidy(mod)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   -0.292   0.166       -1.76 8.06e- 2\n2 Bill           0.182   0.00645     28.2  5.24e-63\n\ntidy(mod)[[2,2]] # slope\n\n[1] 0.182\n\n\nWe can plot our regression line on top of the scatterplot manually using the geom_abline() layer in ggplot:\n\nggplot( RestaurantTips, aes( Bill, Tip ) ) +\n  geom_point() +\n  geom_abline( intercept = -0.292, slope =  0.182, col=\"red\" )"
  },
  {
    "objectID": "intro_linear_regression.html#multiple-regression",
    "href": "intro_linear_regression.html#multiple-regression",
    "title": "3  Intro to Regression",
    "section": "3.2 Multiple Regression",
    "text": "3.2 Multiple Regression\nWe now include the additional explanatory variables of number in party (Guests) and whether or not they pay with a credit card (Credit):\n\ntip.mod &lt;- lm(Tip ~ Bill + Guests + Credit, data=RestaurantTips )\nsummary(tip.mod)\n\n\nCall:\nlm(formula = Tip ~ Bill + Guests + Credit, data = RestaurantTips)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-2.384 -0.478 -0.108  0.272  5.984 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.25468    0.20273   -1.26     0.21    \nBill         0.18302    0.00846   21.64   &lt;2e-16 ***\nGuests      -0.03319    0.10282   -0.32     0.75    \nCredity      0.04217    0.18282    0.23     0.82    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.985 on 153 degrees of freedom\nMultiple R-squared:  0.838, Adjusted R-squared:  0.834 \nF-statistic:  263 on 3 and 153 DF,  p-value: &lt;2e-16\n\n\nThis output should look very similar to the output for one variable, except now there is a row corresponding to each explanatory variable. Our two-category (y, n) Credit variable was automatically converted to a 0-1 dummy variable (with “y” being 1 and “n” our baseline).\n\n3.2.1 Easy Tabulation and Graphing of Multiple Regression Models\nFor publication-ready tables and graphics, R has many wonderful packages to automate the process. I (JG) am partial to tab_model from sjPlot for regression tables, and ggeffects for regression graphs, as shown below. See more in the other chapters.\n\n# tabulate the results of our two tip models\ntab_model(mod, tip.mod)\n\n\n\n\n \nTip\nTip\n\n\nPredictors\nEstimates\nCI\np\nEstimates\nCI\np\n\n\n(Intercept)\n-0.29\n-0.62 – 0.04\n0.081\n-0.25\n-0.66 – 0.15\n0.211\n\n\nBill\n0.18\n0.17 – 0.19\n&lt;0.001\n0.18\n0.17 – 0.20\n&lt;0.001\n\n\nGuests\n\n\n\n-0.03\n-0.24 – 0.17\n0.747\n\n\nCredit [y]\n\n\n\n0.04\n-0.32 – 0.40\n0.818\n\n\nObservations\n157\n157\n\n\nR2 / R2 adjusted\n0.837 / 0.836\n0.838 / 0.834\n\n\n\n\n\n\n# graph model 2, with Bill on X, Credit as color, and Guests held constant at the mean\nggeffect(tip.mod, terms = c(\"Bill\", \"Credit\")) |&gt; \n  plot(add.data = TRUE, ci = FALSE)"
  },
  {
    "objectID": "intro_linear_regression.html#categorical-variables-and-factors",
    "href": "intro_linear_regression.html#categorical-variables-and-factors",
    "title": "3  Intro to Regression",
    "section": "3.3 Categorical Variables (and Factors)",
    "text": "3.3 Categorical Variables (and Factors)\nYou can include any explanatory categorical variable in a multiple regression model, and R will automatically create corresponding 0/1 variables. For example, if you were to include gender coded as male/female, R would create a variable GenderMale that is 1 for males and 0 for females.\n\n3.3.1 Numbers Coding Categories.\nIf you have multiple levels of a category, but your levels are coded with numbers you have to be a bit careful because R can treat this as a quantitative (continuous) variable by mistake in some cases. You will know it did this if you only see the single variable on one line of your output. For categorical variables with \\(k\\) categories, you should see \\(k-1\\) lines.\nTo make a variable categorical, even if the levels are numbers, convert the variable to a factor with as.factor or factor:\n\n# load the US states data\ndata( USStates )\n\n# convert Region to a factor\nUSStates &lt;- USStates |&gt; \n  mutate(Region = factor(Region))\n\n\n\n3.3.2 Setting new baselines.\nWe can reorder the levels if desired (the first is our baseline).\n\nlevels( USStates$Region )\n\n[1] \"MW\" \"NE\" \"S\"  \"W\" \n\nUSStates$Region = relevel(USStates$Region, \"S\" )\nlevels( USStates$Region )\n\n[1] \"S\"  \"MW\" \"NE\" \"W\" \n\n\nNow any regression will use the south as baseline.\n\n\n3.3.3 Testing for significance of a categorical variable.\nWhen deciding whether to keep a categorical variable, we need to test how important all the dummy variables for that category are to the model all at once. We do this with ANOVA. Here we examine whether region is useful for predicting the percent vote for Clinton in 2016:\n\nmlm = lm( ClintonVote ~ Region, data=USStates)\nanova( mlm )\n\nAnalysis of Variance Table\n\nResponse: ClintonVote\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)    \nRegion     3   1643     548    6.99 0.00057 ***\nResiduals 46   3603      78                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIt is quite important.\nWe can also compare for region beyond some other variable:\n\nmlm2 = lm( ClintonVote ~ HouseholdIncome + HouseholdIncome + HighSchool + \n               EighthGradeMath, data=USStates)\n\nmlm3 = lm( ClintonVote ~ HouseholdIncome + HouseholdIncome + HighSchool + \n               EighthGradeMath + Region, data=USStates)\nanova( mlm2, mlm3 )\n\nAnalysis of Variance Table\n\nModel 1: ClintonVote ~ HouseholdIncome + HouseholdIncome + HighSchool + \n    EighthGradeMath\nModel 2: ClintonVote ~ HouseholdIncome + HouseholdIncome + HighSchool + \n    EighthGradeMath + Region\n  Res.Df  RSS Df Sum of Sq    F Pr(&gt;F)  \n1     46 3287                           \n2     43 2649  3       638 3.45  0.025 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nRegion is still important, beyond including some further controls. Interpreting this mess of a regression is not part of this document; this document shows you how to run regressions but it doesn’t discuss whether you should or not.\n\n\n3.3.4 Missing levels in a factor\nR often treats categorical variables as factors. This is often useful, but sometimes annoying. A factor has different levels which are the different values it can be. For example:\n\ndata(FishGills3)\nlevels(FishGills3$Calcium)\n\n[1] \"\"       \"High\"   \"Low\"    \"Medium\"\n\ntable(FishGills3$Calcium)\n\n\n         High    Low Medium \n     0     30     30     30 \n\n\nNote the weird nameless level; it also has no actual observations in it. Nevertheless, if you make a boxplot, you will get an empty plot in addition to the other three. This error was likely due to some past data entry issue. You can drop the unused level:\n\nFishGills3$Calcium = droplevels(FishGills3$Calcium)\n\nYou can also turn a categorical variable into a numeric one like so:\n\nsummary( FishGills3$Calcium )\n\n  High    Low Medium \n    30     30     30 \n\nasnum = as.numeric( FishGills3$Calcium )\nasnum\n\n [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3\n[39] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[77] 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n\nRegression on only a categorical variable is fine:\n\nmylm = lm( GillRate ~ Calcium, data=FishGills3 )\nmylm\n\n\nCall:\nlm(formula = GillRate ~ Calcium, data = FishGills3)\n\nCoefficients:\n  (Intercept)     CalciumLow  CalciumMedium  \n         58.2           10.3            0.5  \n\n\nR has made you a bunch of dummy variables automatically. Here “high” is the baseline, selected automatically. We can also force it so there is no baseline by removing the intercept, in which case the coefficients are the means of each group.\n\nmymm = lm( GillRate ~ 0 + Calcium, data=FishGills3 )\nmymm\n\n\nCall:\nlm(formula = GillRate ~ 0 + Calcium, data = FishGills3)\n\nCoefficients:\n  CalciumHigh     CalciumLow  CalciumMedium  \n         58.2           68.5           58.7"
  },
  {
    "objectID": "intro_linear_regression.html#some-extra-stuff-optional",
    "href": "intro_linear_regression.html#some-extra-stuff-optional",
    "title": "3  Intro to Regression",
    "section": "3.4 Some extra stuff (optional)",
    "text": "3.4 Some extra stuff (optional)\n\n3.4.1 Confidence Intervals\nTo get confidence intervals around each parameter in your model, try this:\n\nconfint(tip.mod)\n\n             2.5 % 97.5 %\n(Intercept) -0.655  0.146\nBill         0.166  0.200\nGuests      -0.236  0.170\nCredity     -0.319  0.403\n\n\nYou can also create them easily using tidy and mutate:\n\ntip.mod |&gt; \n  tidy() |&gt; \n  mutate(upper = estimate + 1.96*std.error,\n         lower = estimate - 1.96*std.error)\n\n# A tibble: 4 × 7\n  term        estimate std.error statistic  p.value upper  lower\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 (Intercept)  -0.255    0.203      -1.26  2.11e- 1 0.143 -0.652\n2 Bill          0.183    0.00846    21.6   2.07e-48 0.200  0.166\n3 Guests       -0.0332   0.103      -0.323 7.47e- 1 0.168 -0.235\n4 Credity       0.0422   0.183       0.231 8.18e- 1 0.400 -0.316\n\n\n\n\n3.4.2 Prediction\nSuppose a server at this bistro is about to deliver a $20 bill, and wants to predict their tip. They can get a predicted value and 95% (this is the default level, change with level) prediction interval with\n\nnew.dat = data.frame( Bill = c(20) )\npredict(mod,new.dat,interval = \"prediction\")\n\n   fit  lwr  upr\n1 3.35 1.41 5.29\n\n\nThey should expect a tip somewhere between $1.41 and $5.30.\nIf we know a bit more we can use our more complex model called tip.mod from above:\n\nnew.dat = data.frame( Bill = c(20), Guests=c(1), Credit=c(\"n\") )\npredict(tip.mod,new.dat,interval = \"prediction\")\n\n   fit  lwr  upr\n1 3.37 1.41 5.34\n\n\nThis is the predicted tip for one guest paying with cash for a $20 tip. It is wider than our original interval because our model is a bit more unstable (it turns out guest number and credit card aren’t that relevant or helpful).\nCompare the prediction interval to the confidence interval\n\nnew.dat = data.frame( Bill = c(20), Guests=c(1), Credit=c(\"n\") )\npredict(tip.mod, new.dat, interval = \"confidence\")\n\n   fit  lwr  upr\n1 3.37 3.09 3.65\n\n\nThis predicts the mean tip for all single guests who pay a $20 bill with cash. Our interval is smaller because we are generating a confidence interval for where the mean is, and are ignoring that individuals will vary around that mean. Confidence intervals are different from prediction intervals.\n\n\n3.4.3 Removing Outliers\nIf you can identify which rows the outliers are on, you can do this by hand (say the rows are 5, 10, 12).\n\nnew.data = old.data[ -c(5,10,12), ]\nlm( Y ~ X, data=new.data )\n\nSome technical details: The c(5,10,12) is a list of 3 numbers. The c() is the concatenation function that takes things makes lists out of them. The “-list” notation means give me my old data, but without rows 5, 10, and 12. Note the comma after the list. This is because we identify elements in a dataframe with row, column notation. So old.data[1,3] would be row 1, column 3.\nIf you notice your points all have X bigger than some value, say 20.5, you could use filtering to keep everything less than some value:\n\nnew.data = filter( old.data, X &lt;= 20.5 )\n\n\n\n3.4.4 Missing data\nIf you have missing data, lm will automatically drop those cases because it doesn’t know what else to do. It will tell you this, however, with the summary command.\n\ndata(AllCountries)\ndev.lm = lm( BirthRate ~ Rural + Health + ElderlyPop, data=AllCountries )\nsummary( dev.lm  )\n\n\nCall:\nlm(formula = BirthRate ~ Rural + Health + ElderlyPop, data = AllCountries)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.592  -3.728  -0.791   3.909  16.218 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  26.5763     1.6795   15.82  &lt; 2e-16 ***\nRural         0.0985     0.0224    4.40  1.9e-05 ***\nHealth       -0.0995     0.0930   -1.07     0.29    \nElderlyPop   -1.0249     0.0881  -11.64  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.83 on 174 degrees of freedom\n  (39 observations deleted due to missingness)\nMultiple R-squared:  0.663, Adjusted R-squared:  0.657 \nF-statistic:  114 on 3 and 174 DF,  p-value: &lt;2e-16\n\n\n\n\n3.4.5 Residual plots and model fit\nIf we throw out model into the plot function, we get some nice regression diagnostics.\n\nplot(tip.mod)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo generate classic model fit diagnostics with more control, we need to calculate residuals, make a residual versus fitted values plot, and make a histogram of the residuals. We can make some quick and dirty plots with qplot (standing for “quick plot”) like so:\n\nqplot(tip.mod$fit, tip.mod$residuals )\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n\n\n\n\n\n\n\nand\n\nqplot(tip.mod$residuals, bins=30)\n\n\n\n\n\n\n\n\nWe see no real pattern other than some extreme outliers. The residual histogram suggests we are not really normally distributed, so we should treat our SEs and \\(p\\)-values with caution. These plots are the canonical “model-checking’’ plots you might use.\nAnother is the “fitted outcomes vs. actual outcomes’’ plot of:\n\npredicted = predict( dev.lm )\nactual = dev.lm$model$BirthRate\nqplot( actual, predicted, main=\"Fit vs. actual Birth Rate\" )\n\n\n\n\n\n\n\n\nNote the dev.lm variable has a model variable inside it. This is a data frame of the used data for the model (i.e., if cases were dropped due to missingness, they will not be in the model). We then grab the birth rates from this, and make a scatterplot. If we tried to skip this, and use the original data, we would get an error because our original data set has some observations that were dropped.\nNote we can’t just add our predictions to AllCountries since we would get an error due to this dropped data issue:\n\nAllCountries$predicted = predict( dev.lm )\n\nError in `$&lt;-.data.frame`(`*tmp*`, predicted, value = c(`1` = 31.630301617421,  : \n  replacement has 179 rows, data has 217\nWe can, however, predict like this:\n\nAllCountries$predicted = predict( dev.lm, newdata=AllCountries )\n\nThe newdata tells predict to generate a prediction for each row in AllCountries rather than each row in the left over data after lm dropped cases with missing values."
  },
  {
    "objectID": "r_tips.html#some-miscellaneous-advice",
    "href": "r_tips.html#some-miscellaneous-advice",
    "title": "4  Tips, Tricks, and Debugging in R",
    "section": "4.1 Some miscellaneous advice",
    "text": "4.1 Some miscellaneous advice\nSo you are starting to learn R. But there’s lots of good tricks you’ll never know about until somebody shows you. Clean code is one such good trick; consider the following: “Your most important collaborator is you from 6 months ago. Unfortunately, you can’t ask that-you any questions, because they don’t answer their email.”\nIn this section we give a few things that you may find useful, either now or later.\n\n4.1.1 A few random tips\nThe letter “l” looks like the number “1”—watch out for that. Things like “mylm” are usually all letters, with “lm” standing for linear model.\n\n\n4.1.2 Quick tips regarding R Markdown report generation\nDon’t put “View()” in your Markdown file when loading your csv file. Just put in the read_csv line. Otherwise you will not be able to knit.\nIf you can’t knit PDFs you need to install latex (tex). Once you do, reboot your computer. If things don’t work, then knit to Microsoft word (or, failing that, html as a last resort), print to pdf, and turn that in. But then ask a teaching fellow to help get things set up, since PDFs make for much more readable reports.\n\n\n4.1.3 Saving R objects\nIf you have the result of something that took awhile to run (e.g., a big multilevel model fit to a lot of data) you can try saving it like so:\n\nmyBigThing = lm(mpg ~ disp, data=mtcars) #something slow\nsaveRDS(myBigThing, savedPath)\n\n## Later on:\nmyBigThing &lt;- readRDS(savedPath)\n\n\n\n4.1.4 R style (based on Google style guide)\nTry to do the following\n\nComment your code!\nStructure of an R file:\n\nDescriptive comments (including date)\nLoad libraries\nConstants and script parameters (# iterations, etc.)\nFunctions (with descriptive comment after first line)\nEverything else\n\nvariableName / variable.name, FunctionVerb, kConstantName. not_like_this\nCurly Braces, line breaks: see previous slide\nConsistency: 2-space indents, y = (a * x) + b + rnorm(1, sd=sigma)\nAvoid attach()\n\n\n\n4.1.5 set.seed\nIf your code uses random numbers, then you should set your seed, which makes your script always generate the same sequence of random numbers.\nFor example, say your code had this:\n\ntryCatch({(1:(1:10)[rpois(1, 3)])}, error=function(e){(e)}) #works?\n\n[1] 1 2 3 4\n\nset.seed(97)\ntryCatch({(1:(1:10)[rpois(1, 3)])}, error=function(e){(e)}) #fails!\n\n&lt;simpleError in 1:(1:10)[rpois(1, 3)]: argument of length 0&gt;\n\n\n(Note the tryCatch() method is a way of generating errors and not crashing.)\nKey thing to know: Reproducible results help with debugging.\nIf you want to get fancy, try this (after installing the `TeachingDemos’ package):\n\nTeachingDemos::char2seed(\"quinn\") # Using your name as a seed says \"nothing up my sleeve\""
  },
  {
    "objectID": "r_tips.html#file-structure-how-not-to-do-it",
    "href": "r_tips.html#file-structure-how-not-to-do-it",
    "title": "4  Tips, Tricks, and Debugging in R",
    "section": "4.2 File structure: how not to do it",
    "text": "4.2 File structure: how not to do it\nEver seen this?\n\n/My Documents\n\nmy paper.tex\nmy paper draft 2.tex\nmy paper final.tex\nmy paper final revised.tex\nmy paper final revised 2.tex\nscript.r\nscript 2.r\ndata.csv\n\n\nTry instead something like:\n\n/stat 166-Small Data Analysis\n\nstat 166.rproj\n/Empty Project\n\n/code\n/data\n/text\n/figures\nreadme.txt\n\n/HW1\n\n…\n\n\n\nYour readme.txt might have informational notes such as “Got data from bit.ly/XYZ.” to remind you of what you were up to.\nYour figures folder should be full of figures you can easily regenerate with code in your code folder.\n\n4.2.1 Making Data Frames\nFor small datasets, you can type in data the hard way like so:\n\nexp.dat = data.frame( ID=c(\"a\",\"b\",\"c\",\"d\"), \n      cond = c(\"AI\",\"DI\",\"DI\",\"AI\"),\n            trial1 = c(\"E\",\"U\",\"U\",\"E\"),\n            dec1 = c(1,1,0,1),\n            trial2 = c(\"U\",\"E\",\"U\",\"E\"),\n            dec2 = c(0,0,0,1),\n                trial3 = c(\"U\",\"E\",\"E\",\"U\"),\n            dec3 = c(0,1,0,1),\n                trial4 = c(\"E\",\"U\",\"E\",\"U\"),\n            dec4 = c(0,1,0,0) )\nexp.dat  \n\n  ID cond trial1 dec1 trial2 dec2 trial3 dec3 trial4 dec4\n1  a   AI      E    1      U    0      U    0      E    0\n2  b   DI      U    1      E    0      E    1      U    1\n3  c   DI      U    0      U    0      E    0      E    0\n4  d   AI      E    1      E    1      U    1      U    0\n\n\nThis is for an experiment on 4 subjects. The first and forth subject got the AI treatment, the second two got the DI treatment. The subjects then had 4 trials each, and they received a “E” choice or a “U” choice, and the decision variable is whether they accepted the choice.\nAs you can see, data can get a bit complicated!\n\n\n4.2.2 Making sure your data are numeric\nSometimes when you load data in, R does weird things like decide all your numbers are actually words. This happens if some of your entries are not numbers. Then R makes them all not numbers. You can check this with the str() function:\n\nstr( exp.dat )\n\n'data.frame':   4 obs. of  10 variables:\n $ ID    : chr  \"a\" \"b\" \"c\" \"d\"\n $ cond  : chr  \"AI\" \"DI\" \"DI\" \"AI\"\n $ trial1: chr  \"E\" \"U\" \"U\" \"E\"\n $ dec1  : num  1 1 0 1\n $ trial2: chr  \"U\" \"E\" \"U\" \"E\"\n $ dec2  : num  0 0 0 1\n $ trial3: chr  \"U\" \"E\" \"E\" \"U\"\n $ dec3  : num  0 1 0 1\n $ trial4: chr  \"E\" \"U\" \"E\" \"U\"\n $ dec4  : num  0 1 0 0\n\n\nHere we see that we have factors (categorical variables) and numbers (num). All is well.\nIf something should be a number, then change it like so:\n\nlst &lt;-  c( 1, 2, 3, \"dog\", 5, 6 )\nstr( lst )\n\n chr [1:6] \"1\" \"2\" \"3\" \"dog\" \"5\" \"6\"\n\nlst &lt;- as.numeric( lst )\n\nWarning: NAs introduced by coercion\n\nlst\n\n[1]  1  2  3 NA  5  6\n\nstr( lst )\n\n num [1:6] 1 2 3 NA 5 6\n\n\nNote it warned you that you had non-numbers when you converted. The non-numbers are now missing (NA).\nFor a dataframe, you fix like this:\n\nexp.dat$trial1 = as.numeric( exp.dat$trial1 )\n\nWarning: NAs introduced by coercion\n\n\n\n\n4.2.3 Merging Data\nOften you have two datasets that you want to merge. For example, say you want to merge some data you have on a few states with some SAT information from the mosaic package.\n\nlibrary( mosaicData )\ndata( SAT )\nhead( SAT )\n\n       state expend ratio salary frac verbal math  sat\n1    Alabama   4.41  17.2   31.1    8    491  538 1029\n2     Alaska   8.96  17.6   48.0   47    445  489  934\n3    Arizona   4.78  19.3   32.2   27    448  496  944\n4   Arkansas   4.46  17.1   28.9    6    482  523 1005\n5 California   4.99  24.0   41.1   45    417  485  902\n6   Colorado   5.44  18.4   34.6   29    462  518  980\n\ndf = data.frame( state=c(\"Alabama\",\"California\",\"Fakus\"), \n                A=c(10,20,50), \n                frac=c(0.5, 0.3, 0.4) )\ndf\n\n       state  A frac\n1    Alabama 10  0.5\n2 California 20  0.3\n3      Fakus 50  0.4\n\nmerge( df, SAT, by=\"state\", all.x=TRUE )\n\n       state  A frac.x expend ratio salary frac.y verbal math  sat\n1    Alabama 10    0.5   4.41  17.2   31.1      8    491  538 1029\n2 California 20    0.3   4.99  24.0   41.1     45    417  485  902\n3      Fakus 50    0.4     NA    NA     NA     NA     NA   NA   NA\n\n\nThe records are combined by the “by” variable. I.e., each record in df is matched with each record in SAT with the same value of “state.”\nThings to note: If you have the same variable in each dataframe, it will keep both, and add a suffix of “.x” and “.y” to indicate where they came from.\nThe “all.x” means keep all records from your first dataframe (here df) even if there is no match. If you added “all.y=TRUE” then you would get all 50 states from the SAT dataframe even though df doesn’t have most of them. Try it!\nYou can merge on more than one variable. I.e., if you said \\verb|by=c(“A”,“B”)| then it would match records if they had the same value for both A and B. See below for an example on this.\n\n\n4.2.4 Lagged Data\nSometimes you have multiple times for your units (think country or state), and you want to regress, say, future X on current X. Then you want to have both future and current X for each case.\nHere think of a case as a Country at a point in time. E.g., we might have data like this:\n\ndtw = read.csv( \"data/fake_country_block.csv\", as.is=TRUE )\ndt = pivot_longer( dtw, cols=X1997:X2004,\n                   names_to = \"Year\", names_prefix = \"X\",\n                   values_to = \"X\" )\ndt$Year = as.numeric( dt$Year )\nslice_sample( dt, n=5 )\n\n# A tibble: 5 × 3\n  Country  Year     X\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 China    2000   3.4\n2 England  1999  53  \n3 China    2003   6  \n4 Morocco  1997  31.9\n5 England  2003  57.3\n\n\nWe then want to know what the X will be 2 years in the future. We can do this with the following trick:\n\ndt.fut = dt\ndt.fut$Year = dt.fut$Year - 2\nhead(dt.fut)\n\n# A tibble: 6 × 3\n  Country  Year     X\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 China    1995   0.5\n2 China    1996   1  \n3 China    1997   2  \n4 China    1998   3.4\n5 China    1999   4  \n6 China    2000   5.3\n\nnewdt = left_join( dt, dt.fut, \n                   by=c(\"Country\",\"Year\"), suffix=c(\"\",\".fut\") )\nhead( newdt, 10 )\n\n# A tibble: 10 × 4\n   Country  Year     X X.fut\n   &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 China    1997   0.5   2  \n 2 China    1998   1     3.4\n 3 China    1999   2     4  \n 4 China    2000   3.4   5.3\n 5 China    2001   4     6  \n 6 China    2002   5.3   7  \n 7 China    2003   6    NA  \n 8 China    2004   7    NA  \n 9 Morocco  1997  31.9  33  \n10 Morocco  1998  32    34  \n\n\nHere we are merging records that match Country and Year.\nNote that for the final two China entries, we don’t have a future X value. The merge will make it NA indicating it is missing.\nHow this works: we are tricking the program. We are making a new \\verb|dt.lag| data.frame and then putting all the entries into the past by two years. When we merge, and match on Country and Year, the current dataframe and the lagged dataframe get lined up by this shift. Clever, no?\nNow we could do regression:\n\nmy.lm = lm( X.fut ~ X + Country, data=newdt )\nsummary( my.lm )\n\n\nCall:\nlm(formula = X.fut ~ X + Country, data = newdt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.5869 -0.2610  0.0107  0.2753  0.5137 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      1.8684     0.2128    8.78  2.7e-06 ***\nX                1.0179     0.0582   17.48  2.3e-09 ***\nCountryEngland  -0.8259     2.9704   -0.28     0.79    \nCountryMorocco  -0.7514     1.7603   -0.43     0.68    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.351 on 11 degrees of freedom\n  (9 observations deleted due to missingness)\nMultiple R-squared:     1,  Adjusted R-squared:     1 \nF-statistic: 2.13e+04 on 3 and 11 DF,  p-value: &lt;2e-16\n\n\n\n\n\n4.2.5 Summarizing Data\nSometimes you want to collapse several cases into one. This is called aggregating. If you install a package called “dplyr” (Run install.packages( \"dplyr\" ) once to install, or better yet simply install tidyverse) then you will have great power.\nUsing newdt from above, we can summarize countries across all their time points:\n\nnewdt %&gt;% group_by( Country ) %&gt;% \n    summarise( mean.X = mean(X, na.rm=TRUE ),\n        sd.X = sd( X, na.rm=TRUE ) )\n\n# A tibble: 3 × 3\n  Country mean.X  sd.X\n  &lt;chr&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 China     3.65  2.37\n2 England  54.6   2.43\n3 Morocco  34.0   2.12\n\n\n\nYou can also augment data. Here we subtract the mean from each group:\n\ndshift = newdt %&gt;% group_by( Country ) %&gt;%\n    mutate( Xm = mean(X, na.rm=TRUE),\n            Xc = X - mean(X, na.rm=TRUE ) )\nhead(dshift)\n\n# A tibble: 6 × 6\n# Groups:   Country [1]\n  Country  Year     X X.fut    Xm    Xc\n  &lt;chr&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 China    1997   0.5   2    3.65 -3.15\n2 China    1998   1     3.4  3.65 -2.65\n3 China    1999   2     4    3.65 -1.65\n4 China    2000   3.4   5.3  3.65 -0.25\n5 China    2001   4     6    3.65  0.35\n6 China    2002   5.3   7    3.65  1.65"
  },
  {
    "objectID": "r_tips.html#troubleshooting-in-r",
    "href": "r_tips.html#troubleshooting-in-r",
    "title": "4  Tips, Tricks, and Debugging in R",
    "section": "4.3 Troubleshooting in R",
    "text": "4.3 Troubleshooting in R\nBy now you have gotten to the point where you can get some really weird errors in R and they can be quite, quite frustrating. This section talks about how to think about fixing them on your own. It also covers some common mistakes that can happen. Say you have some code that does a bootstrap and prints out a histogram. Nothing seems to work and the hist command is giving a strange error.\nFirst step\nPut all the commands, start to finish, in your script. The reason for this step is then you know what you are looking at. When scrolling to old commands and trying different things, you can get very tangled up. Anyway, say you do, and you are still getting a strange error:\nYou might think hist is the culprit, but that might not be true.\n\nFirst step is to check if you have any strange arguments to hist. Try running hist without any arguments other than the data.\nIf that doesn’t work (and here it won’t), then the next step is to see what is going on is to look at what you are making a histogram out of!\n\n\n[1] NaN NaN NaN NaN NaN NaN\n\n\nYou can also look at loveboot by clicking on it in your ‘Workspace’ to see if it is weird. If it has a bunch of NA or NaN then you need to fix your bootstrap code. You are trying to make a histogram out of bad data. Another rule:\nThose bad data came from somewhere! Let’s examine what is happening inside your bootstrap.\nThe easiest way is to run the stuff inside your replicate to get one replicate and see what is going on. This illustrates a very important debugging rule:\nThe code inside your replicate should run by itself. So try it, looking at the value each time\n\n  lovesampmale = sample(lovemale, 1000, replace=TRUE)\n  head(lovesampmale)\n\n[1] 1 1 1 0 1 0\n\n  propsampmale = table(lovesampmale)[0]/length(lovesampmale)\n  propsampmale\n\nnamed numeric(0)\n\n  mean(propsampmale)\n\n[1] NaN\n\n\nWe see that the propsampmale line is going wonky. We unpack the pieces\n\ntable(lovesampmale)\n\nlovesampmale\n  0   1   2 \n294 680  26 \n\nlength(lovesampmale)\n\n[1] 1000\n\ntable(lovesampmale)[0]\n\nnamed integer(0)\n\n\nWe finally find the error. We need quotation marks around the 0. Without the quotes, R interprets “[0]” as taking the 0th entry of the table, which doesn’t exist, rather than the entry named “0,” which does1\n\ntable(lovesampmale)[\"0\"]\n\n  0 \n294 \n\n\n\n4.3.1 Aside: the table technique\nThe “table technique” to calculate the proportion of some list of data that has a given value is dangerous. In particular if that value isn’t present, then the table could drop it, causing some real trouble. Instead use\n\nmean(propsampmale == 0)\n\n[1] NaN\n\n\n\n\n4.3.2 Code redundancies\nSometimes you don’t need parts of your code at all! The propsampmale has the answer. No need for the final mean in the above code!\n\n\n4.3.3 Categories should be words\nFor categories, don’t use numbers. Instead use\n\nlovemale = rep(c(\"Little\", \"Some\", \"Lots\"), c(372, 807,34))\n\nand then your mean line will be\n\nlovemale = rep(c(\"Little\", \"Some\", \"Lots\"), c(372, 807,34))\n\ngiving your final fixed code (plot not shown):\n\nlovemale = rep(c(\"Little\", \"Some\", \"Lots\"), c(372, 807,34))\nloveboot = replicate(1000, {\n  lovesampmale = sample( lovemale, replace=TRUE )\n  mean(lovesampmale == \"Little\")\n})\nhist(loveboot)"
  },
  {
    "objectID": "r_tips.html#footnotes",
    "href": "r_tips.html#footnotes",
    "title": "4  Tips, Tricks, and Debugging in R",
    "section": "",
    "text": "Why? Because for a table those things at the top are names and all names are considered words. We denote words in R with quotation marks↩︎"
  },
  {
    "objectID": "making_tables.html#making-a-table-one",
    "href": "making_tables.html#making-a-table-one",
    "title": "5  Making tables in Markdown",
    "section": "5.1 Making a “table one”",
    "text": "5.1 Making a “table one”\nThe “table one” is the first table in a lot of papers that show general means of different variables for different groups. The tableone package is useful:\n\nlibrary(tableone)\n\n# sample mean  \nCreateTableOne(data = dat,\n               vars = c(\"G\", \"Z\", \"X\"))\n\n               \n                Overall     \n  n              100        \n  G (%)                     \n     A            17 (17.0) \n     B            21 (21.0) \n     C            27 (27.0) \n     D            15 (15.0) \n     E            20 (20.0) \n  Z = tx (%)      45 (45.0) \n  X (mean (SD)) 0.04 (0.98) \n\n# you can also stratify by a variables of interest\ntb &lt;- CreateTableOne(data = dat,\n                     vars = c(\"X\", \"G\", \"Y\"), \n                     strata = c(\"Z\"))\ntb\n\n               Stratified by Z\n                co            tx           p      test\n  n                55           45                    \n  X (mean (SD)) -0.06 (1.03)  0.16 (0.91)   0.271     \n  G (%)                                     0.589     \n     A              8 (14.5)     9 (20.0)             \n     B             12 (21.8)     9 (20.0)             \n     C             14 (25.5)    13 (28.9)             \n     D             11 (20.0)     4 ( 8.9)             \n     E             10 (18.2)    10 (22.2)             \n  Y (mean (SD))  0.19 (1.14)  0.17 (0.76)   0.900     \n\n\nYou can then use kable as so:\n\nprint(tb$ContTable, printToggle = FALSE) %&gt;%\n    knitr::kable()\n\n\n\n\n\nco\ntx\np\ntest\n\n\n\n\nn\n55\n45\n\n\n\n\nX (mean (SD))\n-0.06 (1.03)\n0.16 (0.91)\n0.271\n\n\n\nY (mean (SD))\n0.19 (1.14)\n0.17 (0.76)\n0.900"
  },
  {
    "objectID": "making_tables.html#table-of-summary-stats",
    "href": "making_tables.html#table-of-summary-stats",
    "title": "5  Making tables in Markdown",
    "section": "5.2 Table of summary stats",
    "text": "5.2 Table of summary stats\nYou can also easily make pretty tables using the stargazer package. You need to ensure the data is a data.frame, not tibble, because stargazer is old school. It appears to only do continuous variables.\nFinally, you need to modify the R code chunk so it looks like this:\nso the output of stargazer gets formatted properly in your R Markdown.\n\nlibrary(stargazer)\n\nstargazer(as.data.frame(dat))\n% Table created by stargazer v.5.2.3 by Marek Hlavac, Social Policy Institute. E-mail: marek.hlavac at gmail.com % Date and time: Thu, Aug 24, 2023 - 17:50:44\n\nYou can include only some of the variables and omit stats that are not of interest:\n\n# to include only variables of interest\nstargazer(as.data.frame(dat), header=FALSE, \n          omit.summary.stat = c(\"p25\", \"p75\", \"min\", \"max\"), # to omit percentiles\n          title = \"Table 1: Descriptive statistics\")\n\nSee the stargazer help file for how to set/change more of the options: https://cran.r-project.org/web/packages/stargazer/stargazer.pdf"
  },
  {
    "objectID": "reg_table_demo.html#the-basics-of-regression-tables",
    "href": "reg_table_demo.html#the-basics-of-regression-tables",
    "title": "6  Making Regression Tables and More Complete Summary Output",
    "section": "6.1 The basics of regression tables",
    "text": "6.1 The basics of regression tables\nFor the basics we quickly illustrate regression tables using a subset of the Making Caring Common dataset, which we will eventually discuss in class. This dataset has a measure of emotional safety (our outcome) and we want to see, in a specific school, if this is predicted by gender and/or grade.\nOur data look like this:\n\nsample_n( sch1, 6 )\n\n  ID    esafe grade gender     disc race_white\n1  1 4.000000     6   Male 1.000000          1\n2  1 3.428571     6 Female 1.222222          1\n3  1 3.285714     5 Female 1.333333          0\n4  1 3.857143     7 Female 1.888889          1\n5  1 4.000000     6   Male 2.000000          1\n6  1 3.000000     5   Male 1.777778          1\n\n\nWe fit some models:\n\nM_A = lm( esafe ~ grade, data = sch1 )\nM_B = lm( esafe ~ grade + gender, data = sch1 )\nM_C = lm( esafe ~ grade * gender, data = sch1 )\n\nOk, we have fit our regression models. We can look at big complex printout of a single model like so:\n\nsummary( M_C )\n\n\nCall:\nlm(formula = esafe ~ grade * gender, data = sch1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.7894 -0.1570  0.1550  0.2662  0.4938 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       4.12733    0.37946  10.877   &lt;2e-16 ***\ngrade            -0.07764    0.05859  -1.325   0.1879    \ngenderMale       -0.72735    0.49762  -1.462   0.1467    \ngrade:genderMale  0.13327    0.07627   1.747   0.0834 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4333 on 108 degrees of freedom\nMultiple R-squared:  0.04914,   Adjusted R-squared:  0.02273 \nF-statistic: 1.861 on 3 and 108 DF,  p-value: 0.1406\n\n\nOr we can make regression tables. There are two packages, one is texreg\n\nlibrary( texreg )\nscreenreg(list(M_A, M_B, M_C))\n\n\n====================================================\n                  Model 1     Model 2     Model 3   \n----------------------------------------------------\n(Intercept)         3.68 ***    3.62 ***    4.13 ***\n                   (0.25)      (0.25)      (0.38)   \ngrade               0.00        0.00       -0.08    \n                   (0.04)      (0.04)      (0.06)   \ngenderMale                      0.13       -0.73    \n                               (0.08)      (0.50)   \ngrade:genderMale                            0.13    \n                                           (0.08)   \n----------------------------------------------------\nR^2                 0.00        0.02        0.05    \nAdj. R^2           -0.01        0.00        0.02    \nNum. obs.         112         112         112       \n====================================================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n\n\nAnother is stargazer.\n\nlibrary( stargazer )\nstargazer( M_A, M_B, M_C, header=FALSE, type='text')\n\n\n===============================================================================\n                                        Dependent variable:                    \n                    -----------------------------------------------------------\n                                               esafe                           \n                            (1)                 (2)                 (3)        \n-------------------------------------------------------------------------------\ngrade                      0.004               0.001              -0.078       \n                          (0.038)             (0.038)             (0.059)      \n                                                                               \ngenderMale                                     0.130              -0.727       \n                                              (0.083)             (0.498)      \n                                                                               \ngrade:genderMale                                                  0.133*       \n                                                                  (0.076)      \n                                                                               \nConstant                 3.676***            3.624***            4.127***      \n                          (0.249)             (0.250)             (0.379)      \n                                                                               \n-------------------------------------------------------------------------------\nObservations                112                 112                 112        \nR2                        0.0001               0.022               0.049       \nAdjusted R2               -0.009               0.004               0.023       \nResidual Std. Error  0.440 (df = 110)    0.437 (df = 109)    0.433 (df = 108)  \nF Statistic         0.009 (df = 1; 110) 1.241 (df = 2; 109) 1.861 (df = 3; 108)\n===============================================================================\nNote:                                               *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01"
  },
  {
    "objectID": "reg_table_demo.html#extending-to-the-multilevel-model",
    "href": "reg_table_demo.html#extending-to-the-multilevel-model",
    "title": "6  Making Regression Tables and More Complete Summary Output",
    "section": "6.2 Extending to the multilevel model",
    "text": "6.2 Extending to the multilevel model\nFor our multilevel examples, we use the Making Caring Common data from Project A, and fit data to the 8th grade students only, but do it for all schools. We have made a High School dummy variable.\nOur two models we use for demo purposes have a HS term and no HS term:\n\nmodA &lt;- lmer( esafe ~ 1 + (1 | ID), data=dat.g8)\nmodB &lt;- lmer( esafe ~ 1 + HS + (1 | ID), data=dat.g8)\n\nIn the next sections we first show how to get better summary output (according to some folks) and then we walk through making regression tables in a bit more detail than above."
  },
  {
    "objectID": "reg_table_demo.html#better-summary-output-for-lmer-getting-p-values",
    "href": "reg_table_demo.html#better-summary-output-for-lmer-getting-p-values",
    "title": "6  Making Regression Tables and More Complete Summary Output",
    "section": "6.3 Better summary output for lmer: Getting p-values",
    "text": "6.3 Better summary output for lmer: Getting p-values\nThe lmerTest package is a way of making R give you more complete output. We are going to load it, and then put the new lmer models into new variables so we can see how the different model fitting packages work with the regression table packages below.\n\nlibrary( lmerTest )\nmodB.T &lt;- lmer( esafe ~ 1 + HS + (1 | ID), data=dat.g8)\nmodA.T &lt;- lmer( esafe ~ 1 + (1 | ID), data=dat.g8)\n\nsummary( modB.T )\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: esafe ~ 1 + HS + (1 | ID)\n   Data: dat.g8\n\nREML criterion at convergence: 2746.8\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.3883 -0.6156  0.2021  0.7628  1.7331 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept) 0.04809  0.2193  \n Residual             0.46459  0.6816  \nNumber of obs: 1305, groups:  ID, 26\n\nFixed effects:\n            Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)  3.52798    0.08637 29.91033  40.846   &lt;2e-16 ***\nHSTRUE      -0.29480    0.10787 25.77814  -2.733   0.0112 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n       (Intr)\nHSTRUE -0.801"
  },
  {
    "objectID": "reg_table_demo.html#the-texreg-package",
    "href": "reg_table_demo.html#the-texreg-package",
    "title": "6  Making Regression Tables and More Complete Summary Output",
    "section": "6.4 The texreg package",
    "text": "6.4 The texreg package\nThere are two options, one is screenreg and the other is texreg().\n\n6.4.1 screenreg\nScreenreg is fine for MLMs. It looks a bit like raw output, but it is clear and clean. It will take models fit using lmer or lmerTest no problem.\n\nscreenreg(list(modA,modB))\n\n\n===============================================\n                     Model 1       Model 2     \n-----------------------------------------------\n(Intercept)              3.35 ***      3.53 ***\n                        (0.06)        (0.09)   \nHSTRUE                                -0.29 ** \n                                      (0.11)   \n-----------------------------------------------\nAIC                   2756.78       2754.79    \nBIC                   2772.30       2775.49    \nLog Likelihood       -1375.39      -1373.40    \nNum. obs.             1305          1305       \nNum. groups: ID         26            26       \nVar: ID (Intercept)      0.07          0.05    \nVar: Residual            0.46          0.46    \n===============================================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n\n\nComment: Note that the number of stars are different for the display vs the summary output! (Look at the HS coefficient for example.) Not good, it would seem.\nThis is because the \\(p\\)-values are calculated using the normal approximation by the screenreg command, and using the \\(t\\)-test with approximate degrees of freedom by lmerTest. This makes a difference. Consider the following, using the \\(t\\) statistics for the HS variable:\n\n2 * pt( -2.733, df=25.77814 )\n\n[1] 0.0111831\n\n2 * pnorm( -2.733 )\n\n[1] 0.006276033\n\n\nOne is below 0.01, and one is not. An extra star!\n\n\n6.4.2 Using texreg and latex\nThe textreg command is part of the texreg package and can be integrated with latex (which you would need to install). Once you do this, when you compile to a pdf, all is well. In the R code chunk you need to include results=\"asis\" to get the latex to compile right. E.g., “r, results=\"asis\"” when you declare a code chunk.\ntexreg(list(modA,modB), table=FALSE)\nNote that the table=FALSE puts the table right where you want it, not at some random spot latex things is nice. Latex likes to have “floating tables” which it puts where there is space."
  },
  {
    "objectID": "reg_table_demo.html#stargazer",
    "href": "reg_table_demo.html#stargazer",
    "title": "6  Making Regression Tables and More Complete Summary Output",
    "section": "6.5 Stargazer",
    "text": "6.5 Stargazer\nlibrary( stargazer )\nstargazer(modA, modB, header=FALSE, type='latex')\nOne issue is stargazer does not include the random effect variances, so the output is quite limited for multilevel modeling. It also has less stringent conditions for when to put down stars. One star is below 0.10, two is below 0.05, and three is below 0.01. This is quite generous. Also it is using the normal approximation.\n\n6.5.1 Stargazer with lmerTest\nStargazer with lmerTest is a bit fussy. This shows how to make it work if you have loaded the lmerTest package. Recall the lmerTest package makes your lmer commands have p-values and whatnot. But this means your new lmer() command is not quite the same as the old—and stargazer is expecting the old. You gix this by lying to R, telling it the new thing is the old thing. This basically works.\nNow for stargazer, we need to tell it that our models are the right type. First note:\n\nclass( modB )\n\n[1] \"lmerMod\"\nattr(,\"package\")\n[1] \"lme4\"\n\nclass( modB.T)\n\n[1] \"lmerModLmerTest\"\nattr(,\"package\")\n[1] \"lmerTest\"\n\n\nSo we fix as follows:\nlibrary( stargazer )\nclass( modB.T ) = \"lmerMod\" \nclass( modA.T ) = \"lmerMod\" \nstargazer(modA.T, modB.T, header=FALSE, type='latex' )"
  },
  {
    "objectID": "math_reference.html#introduction",
    "href": "math_reference.html#introduction",
    "title": "7  A Math Reference: Sample Modeling Equations to Borrow",
    "section": "7.1 Introduction",
    "text": "7.1 Introduction\nThis document has a bunch of mathematical equations we use in the class. It is a good reference for how to write your own math equations in your life moving forward. Generally, people write math equations using something called Latex. Latex (or Tex) is a way of writing documents where mixed in with the writing of what you want to say are commands describing how you want your document to look. This is a very powerful thing: there are Tex editors that allow you to write entire articles, books, reports, poetry, or whatever with extreme control over the typesetting used. It creates beautifully typeset documents that are easy to distinguish from those written in, say, MS Word due to how they adjust whitespace on the page. That being said, it can be a lot to jump in to.\nEnter R Markdown. R Markdown is a useful and easy way to take advantage of this syntax without the overhead of writing entire documents in Latex, even if you don’t have any R code in your document. Inside R Markdown you can write math equations, and then when you render the report, it not only runs all the R code, but it formats all the math for you as well! You can even have R Markdown render to MS Word to give you a word doc with all your math equations ready to go.\n\n7.1.1 Using this document\nYou are probably reading the PDF version of this document. But really you should open the .Rmd file and cut and paste the relevant equations into your own work, and then modify as necessary."
  },
  {
    "objectID": "math_reference.html#overview-of-using-latex",
    "href": "math_reference.html#overview-of-using-latex",
    "title": "7  A Math Reference: Sample Modeling Equations to Borrow",
    "section": "7.2 Overview of Using Latex",
    "text": "7.2 Overview of Using Latex\nFor math in your writing, you denote the beginning and the end of a math equation in your text using “$”s—one at the start and one at the stop. E.g., “$ math stuff $”. Most greek letters are written as their names with a backslash “\\” just before it. E.g., “\\alpha”.\nSo if I want to write an alpha, I write “$\\alpha$” and get \\(\\alpha\\).\nI can do subscripts by using an underscore. E.g., “$\\alpha_j$” gives \\(\\alpha_j\\). I can also do superscripts by using a hat. E.g., “$\\alpha^2$” gives \\(\\alpha^2\\).\n\n\n7.2.1 Some useful greek letters\nHere are some useful greek letters and symbols\n\n\n\nLetter\nName\n\n\n\n\n\\(\\alpha\\)\n\\alpha\n\n\n\\(\\beta\\)\n\\beta\n\n\n\\(\\delta, \\Delta\\)\n\\delta, \\Delta\n\n\n\\(\\epsilon\\)\n\\epsilon\n\n\n\\(\\sigma, \\Sigma\\)\n\\sigma, \\Sigma\n\n\n\\(\\rho\\)\n\\rho\n\n\n\\(\\mu\\)\n\\mu (Meew!)\n\n\n\\(\\tau\\)\n\\tau\n\n\n\\(\\times\\)\n\\times\n\n\n\\(\\sim\\)\n\\sim\n\n\n\nSee many more symbols at, e.g., https://www.caam.rice.edu/~heinken/latex/symbols.pdf. This was found by searching “tex symbols” on Google.\n\n\n7.2.2 Equations on lines by themselves\nTo write an equation on a line by itself, put the math stuff in between a pair of double “$”. E.g., if we write:\n$$ Y = a X + b $$\nWe get \\[ Y = a X + b \\]\nIf we want multiple lines, we have to put our equation between a \\begin{aligned} and \\end{aligned} command and use a double backslash (“\\\\”) to denote each line break (even if we have a line break we have to do this—we have to explicitly tell the program converting our raw text to nice formatted text where the line breaks are). Finally, inside the begin-end block of math, line things up with & symbols on each row of our equation. The & symbols will be lined up vertically.\nSo if we write\n$$\n\\begin{aligned}\nY &= 10 X + 2 \\\\\nY - 5 &= 3 X^2 + 5\n\\end{aligned}\n$$\nwe get \\[\n\\begin{aligned}\nY &= 10 X + 2 \\\\\nY - 5 &= 3 X^2 + 5\n\\end{aligned}\n\\] Also consider:\n$$\n\\begin{aligned}\na + b + c + d &= c \\\\\n d &= e + f + g + h \n\\end{aligned}\n$$\ngiving \\[\n\\begin{aligned}\na + b + c + d &= c \\\\\nd &= e + f + g + h\n\\end{aligned}\n\\]\n\n\n7.2.3 Normal text in equations\nIf you put words in your equations, they get all italliced and weird, without their spaces:\n$$\n5 + my dog = 10\n$$\n\\[\n5 + my dog = 10\n\\]\nYou can fix using the “\\mbox{}” command as so:\n$$\n5 + \\mbox{my dog} = 10\n$$\n\\[\n5 + \\mbox{my dog} = 10\n\\]\nWe next walk through some latex code for the models you will most see."
  },
  {
    "objectID": "math_reference.html#random-intercept-model",
    "href": "math_reference.html#random-intercept-model",
    "title": "7  A Math Reference: Sample Modeling Equations to Borrow",
    "section": "7.3 Random Intercept Model",
    "text": "7.3 Random Intercept Model\nOur canonical Random Intercept model is as follows. First, our Level 1 model:\n$$\n\\begin{aligned}\ny_{ij} &= \\alpha_{j} + \\beta_{1} ses_{ij} +  \\epsilon_{ij} \\\\\n\\epsilon_{ij} &\\sim N( 0, \\sigma^2_y ) \\\\\n\\end{aligned}\n$$\n\\[\n\\begin{aligned}\ny_{ij} &= \\alpha_{j} + \\beta_{1} ses_{ij} +  \\epsilon_{ij} \\\\\n\\epsilon_{ij} &\\sim N( 0, \\sigma^2_y ) \\\\\n\\end{aligned}\n\\]\nOur Level 2 model:\n$$\n\\begin{aligned}\n\\alpha_{j} &= \\gamma_{0} + \\gamma_{1} sector_j + u_{j} \\\\\nu_{j} &\\sim N( 0, \\sigma^2_\\alpha ) \\\\\n\\end{aligned}\n$$\n\\[\n\\begin{aligned}\n\\alpha_{j} &= \\gamma_{0} + \\gamma_{1} sector_j + u_{j} \\\\\nu_{j} &\\sim N( 0, \\sigma^2_\\alpha ) \\\\\n\\end{aligned}\n\\]\nThe Gelman and Hill bracket notation looks like this:\n$$\n\\begin{aligned}\ny_{i} &= \\alpha_{j[i]} + \\beta_{1} ses_{i} +  \\epsilon_{i} \\\\\n\\epsilon_i &\\sim N( 0, \\sigma^2_y ) \\\\\n\\alpha_{j} &= \\gamma_{0} + \\gamma_{1} sector_j + u_{j} \\\\\nu_{j} &\\sim N( 0, \\sigma^2_\\alpha ) \\\\\n\\end{aligned}\n$$\n\\[\n\\begin{aligned}\ny_{i} &= \\alpha_{j[i]} + \\beta_{1} ses_{i} +  \\epsilon_{i} \\\\\n\\epsilon_i &\\sim N( 0, \\sigma^2_y ) \\\\\n\\alpha_{j} &= \\gamma_{0} + \\gamma_{1} sector_j + u_{j} \\\\\nu_{j} &\\sim N( 0, \\sigma^2_\\alpha ) \\\\\n\\end{aligned}\n\\]\nThe reduced form would look like this:\n$$\ny_{i} = \\gamma_{0} + \\gamma_{1} sector_{j[i]} + \\beta_{1} ses_{i} + u_{j[i]} + \\epsilon_{i}\n$$\n\\[\ny_{i} = \\gamma_{0} + \\gamma_{1} sector_{j[i]} + \\beta_{1} ses_{i} + u_{j[i]} + \\epsilon_{i}\n\\]\nwith\n$$\n\\epsilon_i \\sim N( 0, \\sigma^2_y ), \\mbox{ and } u_{j} \\sim N( 0, \\sigma^2_\\alpha )\n$$\n\\[\n\\epsilon_i \\sim N( 0, \\sigma^2_y ), \\mbox{ and } u_{j} \\sim N( 0, \\sigma^2_\\alpha )\n\\]\nIf we want to be really prissy, we can write down the i.i.d. aspect of our random effects like this\n$$\n\\epsilon_i \\stackrel{i.i.d}{\\sim} N( 0, \\sigma^2_y ), \n\\mbox{ and } u_{j} \\stackrel{i.i.d}{\\sim} N( 0, \\sigma^2_\\alpha )\n$$\n\\[\n\\epsilon_i \\stackrel{i.i.d}{\\sim} N( 0, \\sigma^2_y ), \\\\\n\\mbox{ and } u_{j} \\stackrel{i.i.d}{\\sim} N( 0, \\sigma^2_\\alpha )\n\\] The \\stackrel{}{} command takes two bits of latex, each in the curly braces, and stacks them on top of each other."
  },
  {
    "objectID": "math_reference.html#random-slope-model",
    "href": "math_reference.html#random-slope-model",
    "title": "7  A Math Reference: Sample Modeling Equations to Borrow",
    "section": "7.4 Random Slope Model",
    "text": "7.4 Random Slope Model\nThe canonical random slope model for HS&B with ses at level 1 and sector at level 2\nLevel 1 models:\n$$\n\\begin{aligned}\ny_{ij} &= \\beta_{0j} + \\beta_{1j} ses_{ij} +  \\epsilon_{ij} \\\\\n\\epsilon_{ij} &\\sim N( 0, \\sigma^2_y ) \\\\\n\\end{aligned}\n$$\n\\[\n\\begin{aligned}\ny_{ij} &= \\beta_{0j} + \\beta_{1j} ses_{ij} +  \\epsilon_{ij} \\\\\n\\epsilon_{ij} &\\sim N( 0, \\sigma^2_y ) \\\\\n\\end{aligned}\n\\]\nLevel 2 models:\n$$\n\\begin{aligned}\n\\beta_{0j} &= \\gamma_{00} + \\gamma_{01} sector_j + u_{0j} \\\\\n\\beta_{1j} &= \\gamma_{10} + \\gamma_{11} sector_j + u_{1j} \n\\end{aligned}\n$$\n\\[\n\\begin{aligned}\n\\beta_{0j} &= \\gamma_{00} + \\gamma_{01} sector_j + u_{0j} \\\\\n\\beta_{1j} &= \\gamma_{10} + \\gamma_{11} sector_j + u_{1j}\n\\end{aligned}\n\\]\nwith\n$$\n\\begin{pmatrix} u_{0j} \\\\\nu_{1j}\n\\end{pmatrix} \\sim  N\n\\begin{bmatrix}\n\\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix}\\!\\!,&\n\\begin{pmatrix}\n\\tau_{00} & \\tau_{01}\\\\\n & \\tau_{11} \\\\\n\\end{pmatrix}\n\\end{bmatrix}\n$$\n\\[\n\\begin{pmatrix} u_{0j} \\\\\nu_{1j}\n\\end{pmatrix} \\sim  N\n\\begin{bmatrix}\n\\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix}\\!\\!,&\n\\begin{pmatrix}\n\\tau_{00} & \\tau_{01}\\\\\n& \\tau_{11} \\\\\n\\end{pmatrix}\n\\end{bmatrix}\n\\]\nThe derivation of the reduced form is:\n$$\n\\begin{aligned}\ny_{ij} &= \\beta_{0j} + \\beta_{1j} ses_{ij} + \\epsilon_{ij}\\\\\n&= \\left( \\gamma_{00} + \\gamma_{01} sector_j + u_{0j} \\right)+ \\\\\\\n  (\\gamma_{10} + \\gamma_{11} sector_j + u_{1j}) ses_{ij} +  \\epsilon_{ij} \\\\\n&= \\gamma_{00} + \\gamma_{01} sector_j + u_{0j}  + \\gamma_{10}ses_{ij} + \\\\\n  \\gamma_{11} sector_j ses_{ij} + u_{1j} ses_{ij} +  \\epsilon_{ij}  \\\\\n&= \\gamma_{00} + \\gamma_{01} sector_j +  \\gamma_{10}ses_{ij} + \\\\\n  \\gamma_{11} sector_j ses_{ij} + \\left(u_{0j} + u_{1j} ses_{ij} + \\epsilon_{ij} \\right) \n\\end{aligned}\n$$\n\\[\n\\begin{aligned}\ny_{ij} &= \\beta_{0j} + \\beta_{1j} ses_{ij} + \\epsilon_{ij}\\\\\n&= \\left( \\gamma_{00} + \\gamma_{01} sector_j + u_{0j} \\right)+ (\\gamma_{10} + \\gamma_{11} sector_j + u_{1j}) ses_{ij} +  \\epsilon_{ij} \\\\\n&= \\gamma_{00} + \\gamma_{01} sector_j + u_{0j}  + \\gamma_{10}ses_{ij} + \\gamma_{11} sector_j ses_{ij} + u_{1j} ses_{ij} +  \\epsilon_{ij}  \\\\\n&= \\gamma_{00} + \\gamma_{01} sector_j +  \\gamma_{10}ses_{ij} + \\gamma_{11} sector_j ses_{ij} + \\left(u_{0j} + u_{1j} ses_{ij} + \\epsilon_{ij} \\right)\n\\end{aligned}\n\\]\nCommentary: There are various and competing ways of writing the covariance matrix for the random effects. The \\(\\tau_{**}\\) notation is easy and expands to any sized matrix (if we, for example, have more than one random slope). But all the \\(\\tau_{**}\\) are variances, not standard deviations, and we often like to talk about random effect variation in terms of standard deviations. We can thus use something like this instead:\n$$\n\\begin{pmatrix} u_{0j} \\\\\nu_{1j}\n\\end{pmatrix} \\sim  N\n\\begin{bmatrix}\n\\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix}\\!\\!,&\n\\begin{pmatrix}\n\\sigma^2_{0} & \\rho \\sigma_0 \\sigma_1 \\\\\n & \\sigma^2_{1} \\\\\n\\end{pmatrix}\n\\end{bmatrix}\n$$\n\\[\n\\begin{pmatrix} u_{0j} \\\\\nu_{1j}\n\\end{pmatrix} \\sim  N\n\\begin{bmatrix}\n\\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix}\\!\\!,&\n\\begin{pmatrix}\n\\sigma^2_{0} & \\rho \\sigma_0 \\sigma_1 \\\\\n& \\sigma^2_{1} \\\\\n\\end{pmatrix}\n\\end{bmatrix}\n\\]\nHere we have a correlation of random effects, \\(\\rho\\), instead of a covariance, \\(\\tau_{01}\\). And we can talk about the standard deviation of, e.g., the random intercepts, as \\(\\sigma_0\\) rather than \\(\\sqrt{ \\tau_{00} }\\). Different ways of writing the same mathematical thing are called different parameterizations; they are equivalent, but are more or less clear for different contexts.\nUnfortunately, this means there is no one right answer for how to write down a mathematical equation!"
  },
  {
    "objectID": "math_reference.html#summations-and-fancy-stuff",
    "href": "math_reference.html#summations-and-fancy-stuff",
    "title": "7  A Math Reference: Sample Modeling Equations to Borrow",
    "section": "7.5 Summations and fancy stuff",
    "text": "7.5 Summations and fancy stuff\nFractions are as follows:\n$$\ncor( A, B ) = \\frac{ cov( A, B ) }{ \\sigma_A \\sigma_B }\n$$\n\\[\ncor( A, B ) = \\frac{ cov( A, B ) }{ \\sigma_A \\sigma_B }\n\\]\nFor reference, you can do summations and whatnot as follows:\n$$\nVar( Y_{i} ) = \\frac{1}{n-1} \\sum_{i=1}^n \\left( Y_{i} - \\bar{Y} \\right)^2 \n$$\n\\[\nVar( Y_{i} ) = \\frac{1}{n-1} \\sum_{i=1}^n \\left( Y_{i} - \\bar{Y} \\right)^2\n\\]\nAnd if you have fractions you can have big brackets with “\\left(” and “\\right)” as follows:\n$$\nX = \\left( \\frac{1}{2} + y \\right)\n$$\n\\[\nX = \\left( \\frac{1}{2} + y \\right)\n\\]\nAnnoyingly, you always need a pair of these big brackets. If you really don’t want one, you use a backslash and a dot, like so:\n$$\nX = \\left( \\frac{1}{2} + y \\right.\n$$\n\\[\nX = \\left( \\frac{1}{2} + y \\right.\n\\]\nThe rest you can find on StackOverflow or similar. Or perhaps have ChatGPT help you write your code!"
  },
  {
    "objectID": "pivot_wider.html#optional-using-old-school-reshape",
    "href": "pivot_wider.html#optional-using-old-school-reshape",
    "title": "8  pivot_longer and pivot_wider",
    "section": "8.1 Optional: Using old School reshape",
    "text": "8.1 Optional: Using old School reshape\nSay you have data in a form where a row has a value for a variable for several different points in time. The following code turns it into a data.frame where each row (case) is a value for the variable at that point in time. You also have an ID variable for which Country the GDP came from.\n\ndtw = read.csv( \"data/fake_country_block.csv\", as.is=TRUE )\ndtw\n\n  Country X1997 X1998 X1999 X2000 X2001 X2002 X2003 X2004\n1   China   0.5     1     2   3.4     4   5.3   6.0     7\n2 Morocco  31.9    32    33  34.0    NA  36.0  37.0    NA\n3 England  51.3    52    53  54.3    55  56.0  57.3    58\n\n\nHere we have three rows, but actually a lot of cases if we consider each time point a case. For trying it on your own, get the sample csv file ()[here]\nSee the website to get the sample csv file \\verb|fake_country_block.csv|.\nThe following our original data by making a case for each time point:\n\ndt = reshape( dtw, idvar=\"Country\", timevar=\"Year\", varying=2:9, sep=\"\", direction=\"long\" )\nhead(dt)\n\n             Country Year    X\nChina.1997     China 1997  0.5\nMorocco.1997 Morocco 1997 31.9\nEngland.1997 England 1997 51.3\nChina.1998     China 1998  1.0\nMorocco.1998 Morocco 1998 32.0\nEngland.1998 England 1998 52.0\n\n\nThings to notice: each case has a “row name” made out of the country and the Year. The “2:9” indicates a range of columns for the variable that is actually the same variable.\nR picked up that, for each of these columns, “X” is the name of the variable and the number is the time, and seperated them. You can set the name of your time variable, \\verb|timevar|, to whatever you want.\nThe above output is called “long format” and the prior is called “wide format.”\nYou can go in either direction. Here:\n\ndtn = reshape( dt, idvar=\"Country\", timevar=\"Year\" )\ndtn\n\n             Country X.1997 X.1998 X.1999 X.2000 X.2001 X.2002 X.2003 X.2004\nChina.1997     China    0.5      1      2    3.4      4    5.3    6.0      7\nMorocco.1997 Morocco   31.9     32     33   34.0     NA   36.0   37.0     NA\nEngland.1997 England   51.3     52     53   54.3     55   56.0   57.3     58\n\n\nYou can reshape on multiple variables. For example:\n\nexp.dat = data.frame( ID=c(\"a\",\"b\",\"c\",\"d\"), \n      cond = c(\"AI\",\"DI\",\"DI\",\"AI\"),\n            trial1 = c(\"E\",\"U\",\"U\",\"E\"),\n            dec1 = c(1,1,0,1),\n            trial2 = c(\"U\",\"E\",\"U\",\"E\"),\n            dec2 = c(0,0,0,1),\n                trial3 = c(\"U\",\"E\",\"E\",\"U\"),\n            dec3 = c(0,1,0,1),\n                trial4 = c(\"E\",\"U\",\"E\",\"U\"),\n            dec4 = c(0,1,0,0) )\nexp.dat\n\n  ID cond trial1 dec1 trial2 dec2 trial3 dec3 trial4 dec4\n1  a   AI      E    1      U    0      U    0      E    0\n2  b   DI      U    1      E    0      E    1      U    1\n3  c   DI      U    0      U    0      E    0      E    0\n4  d   AI      E    1      E    1      U    1      U    0\n\nrs = reshape( exp.dat,  idvar=\"ID\", \n        varying=c( 3:10 ), sep=\"\", direction=\"long\")            \nhead(rs)\n\n    ID cond time trial dec\na.1  a   AI    1     E   1\nb.1  b   DI    1     U   1\nc.1  c   DI    1     U   0\nd.1  d   AI    1     E   1\na.2  a   AI    2     U   0\nb.2  b   DI    2     E   0\n\n\nIt sorts out which variables are which. Note the names have to be exactly the same for any group of variables.\nOnce you have reshaped, you can look at things more easily (I use mosaic’s tally instead of the base table):\n\nmosaic::tally( trial ~ dec, data=rs )\n\n     dec\ntrial 0 1\n    E 4 4\n    U 5 3\n\n\nor\n\nmosaic::tally( trial~dec+cond, data=rs )\n\n, , cond = AI\n\n     dec\ntrial 0 1\n    E 1 3\n    U 3 1\n\n, , cond = DI\n\n     dec\ntrial 0 1\n    E 3 1\n    U 2 2\n\n\n\n8.1.1"
  },
  {
    "objectID": "intro_missing_data.html#introduction",
    "href": "intro_missing_data.html#introduction",
    "title": "9  An Introduction to Missing Data",
    "section": "9.1 Introduction",
    "text": "9.1 Introduction\nHandling missing data is the icky, unglamorous part of any statistical analysis. It is where the skeletons lie. There’s a range of options available, which are, broadly speaking:\n\nDelete the observations with missing covariates (this is a “complete case analysis”)\nPlug in some kind of reasonable value for the missing covariate. This is called “imputation.” We discuss three ways of doing this that are increasingly sophisticated and layered on each other:\n\n\n\nMean imputation. Simply take the mean of all the observations where you know the value, and then use that for anything that is missing.\nRegression imputation. You generate regression equations describing how all the variables are connected, and use those to predict any missing value.\nStochastic regression imputation. Here we use regression imputation, but we also add some residual noise to all our imputed values so that our imputed values have as much variation as our actual values (otherwise our imputed values will tend to be all clumped together).\n\n\n\nMultiply impute the missing data, by fully modeling the covariate and the missingness, and generating a range of complete datasets under this model. Here you end up with a bunch of complete datasets that are all “reasonable guesses” as to what the full dataset might have been. You then analyze each one, and aggregate your findings across them to get a final answer.\n\nThe first two general approaches are imperfect, while the third is often more work than the original analysis that we were hoping to perform. For this course, doing a 2a, 2b, or 2c are all reasonable choices. If you have very little missing data you can often get away with 1. We have no expectations that people will take the plunge into #3 (multiple imputation). In real life, people will often analyze their data with a complete case analysis and some other strategy, and then compare the results. In Education, if missingness is below 10% people usually just do mean imputation, but regression imputation would probably be superior.\nThis handout provides an introduction to missing data, and includes a few commands to explore and deal with missing data. In this document we first talk about exploring missing data (in particular getting plots that show you if you have any notable patterns in how things are missing) and then we give a brief walk-through of the 3 methods listed above.\nWe will the mice and VIM packages, which you can install using install.packages() if you have not yet done so. These are simple and powerful packages for visualizing and imputing missing data. At the end of this document we also describe the Amelia package.\n\nlibrary(tidyverse)\nlibrary(mice)\nlibrary(VIM)\n\nThroughout we use a small built-in R dataset on air quality as a working example.\n\n  data(airquality)\n  nrow(airquality)\n\n[1] 153\n\n  head(airquality)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\n  summary(airquality[1:4])\n\n     Ozone          Solar.R         Wind            Temp     \n Min.   :  1.0   Min.   :  7   Min.   : 1.70   Min.   :56.0  \n 1st Qu.: 18.0   1st Qu.:116   1st Qu.: 7.40   1st Qu.:72.0  \n Median : 31.5   Median :205   Median : 9.70   Median :79.0  \n Mean   : 42.1   Mean   :186   Mean   : 9.96   Mean   :77.9  \n 3rd Qu.: 63.2   3rd Qu.:259   3rd Qu.:11.50   3rd Qu.:85.0  \n Max.   :168.0   Max.   :334   Max.   :20.70   Max.   :97.0  \n NA's   :37      NA's   :7"
  },
  {
    "objectID": "intro_missing_data.html#visualizing-missing-data",
    "href": "intro_missing_data.html#visualizing-missing-data",
    "title": "9  An Introduction to Missing Data",
    "section": "9.2 Visualizing missing data",
    "text": "9.2 Visualizing missing data\nJust like with anything in statistics, the first thing to do is to look at our data. We want to know which variables are often missing, and if some variables are often missing together. We also want to know how much data is missing. The mice package has a variety of plots to show us patterns of missingness:\n\n  md.pattern(airquality)\n\n\n\n\n\n\n\n\n    Wind Temp Month Day Solar.R Ozone   \n111    1    1     1   1       1     1  0\n35     1    1     1   1       1     0  1\n5      1    1     1   1       0     1  1\n2      1    1     1   1       0     0  2\n       0    0     0   0       7    37 44\n\n\nThis plot gives us the different missing data patterns and the number of observations that have each missing data pattern. For example, the second row in the plot says there are 35 observations that have a missing data pattern where only Ozone is missing.\nEasier to understand patterns!\nWe can also just look at 10 observations to see everything that is going on. Here we take the first 10 rows of our dataset, but could also take a random 10 row with the tidyverse’s sample_n method.\n\n  airqualitysub = airquality[1:10, ]\n  airqualitysub\n\n   Ozone Solar.R Wind Temp Month Day\n1     41     190  7.4   67     5   1\n2     36     118  8.0   72     5   2\n3     12     149 12.6   74     5   3\n4     18     313 11.5   62     5   4\n5     NA      NA 14.3   56     5   5\n6     28      NA 14.9   66     5   6\n7     23     299  8.6   65     5   7\n8     19      99 13.8   59     5   8\n9      8      19 20.1   61     5   9\n10    NA     194  8.6   69     5  10\n\n\nWe see that we have one observation missing two covariates and one each of missing Ozone only and Solar.R only.\n\n9.2.1 The VIM Package\nThe VIM package gives some alternate plots to explore missing data patterns. For example, aggr():\n\n aggr(airquality, col=c('navyblue','red'),\n      numbers=TRUE, sortVars=TRUE, labels=names(data),\n      cex.axis=.7, gap=3, prop=c(TRUE, FALSE), \n      ylab=c(\"Histogram of missing data\",\"Pattern\"))\n\n\n\n\n\n\n\n\n\n Variables sorted by number of missings: \n Variable  Count\n    Ozone 0.2418\n  Solar.R 0.0458\n     Wind 0.0000\n     Temp 0.0000\n    Month 0.0000\n      Day 0.0000\n\n\nOn the left, we have the proportion of missing data for each variable in our dataset. We can see that Ozone and Solar.R have missing values. On the right, we have the joint distribution of missingness. We can see that 111 observations have no missing values. From those with missing values, the majority have missing values for Ozone, some have missing values for Solar.R and only 2 observations have missing values for both Ozone and Solar.R.\n\n  marginplot(airquality[1:2])\n\n\n\n\n\n\n\n\nHere we have a scatterplot for the first two variables in our dataset: Ozone and Solar.R. These are the variables that have missing data. In addition to the standard scatterplot we are familiar with, information about missingness is shown in the margins. The red dots indicate observations with one or both values missing (so there can be a bunch of dots stacked up in the bottom-left corner). The numbers (37, 7, and 2 tells us how many observations are missing either or both of these variables)."
  },
  {
    "objectID": "intro_missing_data.html#complete-case-analysis",
    "href": "intro_missing_data.html#complete-case-analysis",
    "title": "9  An Introduction to Missing Data",
    "section": "9.3 Complete case analysis",
    "text": "9.3 Complete case analysis\nWorking with complete cases (dropping observations with any missing data on our outcome and predictors) is always an option. We have been doing this in class and section. However, this can lead to substantial data loss, if we have a lot of missingness and it can heavily bias our results depending on why observations are missing.\nComplete case analysis is the R default.\n\n  fit &lt;- lm(Ozone ~ Wind, data = airquality )\n  summary(fit)\n\n\nCall:\nlm(formula = Ozone ~ Wind, data = airquality)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-51.57 -18.85  -4.87  15.23  90.00 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    96.87       7.24   13.38  &lt; 2e-16 ***\nWind           -5.55       0.69   -8.04  9.3e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 26.5 on 114 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.362, Adjusted R-squared:  0.356 \nF-statistic: 64.6 on 1 and 114 DF,  p-value: 9.27e-13\n\n\nNote the listing in the summary of number of items deleted. You can find out which rows were deleted:\n\n## which rows/observations were deleted\n  deleted &lt;- na.action(fit)\n  deleted\n\n  5  10  25  26  27  32  33  34  35  36  37  39  42  43  45  46  52  53  54  55 \n  5  10  25  26  27  32  33  34  35  36  37  39  42  43  45  46  52  53  54  55 \n 56  57  58  59  60  61  65  72  75  83  84 102 103 107 115 119 150 \n 56  57  58  59  60  61  65  72  75  83  84 102 103 107 115 119 150 \nattr(,\"class\")\n[1] \"omit\"\n\n  naprint(deleted)\n\n[1] \"37 observations deleted due to missingness\"\n\n\nWe have more incomplete rows if we add Solar.R as predictor.\n\n  fit2 &lt;- lm(Ozone ~ Wind+Solar.R, data=airquality)\n  naprint(na.action(fit2))\n\n[1] \"42 observations deleted due to missingness\"\n\n\nWe can also drop observations with missing data ourselves instead of letting R do it for us. Dropping data preemptively is generally a good idea, especially if you plan on using predict().\n\n## complete cases on all variables in the data set \ncomplete.v1 = filter( airquality, complete.cases(airquality) )\n  \n## drop observations with missing values, but ignoring a specific variable  \ncomplete.v2 = filter(airquality, complete.cases(select( airquality, -Wind) ) )\n\n## drop observations with missing values on a specific variable  \ncomplete.v3 = filter(airquality, !is.na(Ozone))\n\nOnce you have subset your data, you just analyze what is left as normal. Easy as pie!"
  },
  {
    "objectID": "intro_missing_data.html#mean-imputation",
    "href": "intro_missing_data.html#mean-imputation",
    "title": "9  An Introduction to Missing Data",
    "section": "9.4 Mean imputation",
    "text": "9.4 Mean imputation\nInstead of dropping observations with missing values, we can plug in some kind of reasonable value for the missing value, e.g. the grand/global mean. While this can be statistically questionable, it does allow us to use the information provided by that unit’s outcome and other covariates, without, we hope, unduly affecting the analysis of the missing covariate.\nGenerally, people will first plug in the mean value for anything missing, but then also make a dummy variable of whether that observation had a missing value there (or sometimes any missing value). You would then include both the original vector of covariates (with the means plugged in) along with the dummy variable in subsequent regressions and analyses.\n\n9.4.1 Doing Mean Imputation manually\nManually, we can just replace missing values for a variable with the grand/global mean.\n\n## make a new copy of the data\n  data.mean.impute = airquality\n  \n## select the observations with missing Ozone\n  miss.ozone = is.na(data.mean.impute$Ozone)\n  \n## replace those NAs with mean(Ozone)\n  data.mean.impute[miss.ozone,\"Ozone\"] = mean(airquality$Ozone, na.rm=TRUE)\n\nIn a multi-level context, it might make more sense to impute using the group mean rather than the grand mean. Here’s a generic function to do it. Here we group by month:\n\n## a function that replaces missing values in a vector \n## by the mean of the other values\n  mean.impute = function(y) { \n      y[is.na(y)] = mean(y, na.rm=TRUE)\n      return(y)\n  }\n\n  data.mean.impute = airquality %&gt;% group_by(Month) %&gt;%\n    mutate(Ozone = mean.impute(Ozone),\n           Solar.R = mean.impute(Solar.R) ) \n\nWe have mean imputed the Ozone column and the Solar.R column\n\n\n9.4.2 Mean imputation with the Mice package\nWe can use the mice package to do mean imputation. The mice package is a package that can do some quite complex imputation, and so when you call mice() (which says “impute missing values please”) you get back a rather complex object telling you what mice imputed, for whom, etc. This object, which is a mids object (see help(mids)), contains the multiply imputed dataset (or in our case, so far, singly imputed). The mice package then provides a lot of nice functions allowing you to get your imputed information out of this object.\nWe first demonstrate this for the 10 observations sampled above. Mice is generally going to be a two-step process: impute data, get completed dataset.\nFor step 1:\n\n  imp &lt;- mice(airqualitysub, method=\"mean\", m=1, maxit=1)\n\n\n iter imp variable\n  1   1  Ozone  Solar.R\n\n\nWarning: Number of logged events: 1\n\n  imp\n\nClass: mids\nNumber of multiple imputations:  1 \nImputation methods:\n  Ozone Solar.R    Wind    Temp   Month     Day \n \"mean\"  \"mean\"      \"\"      \"\"      \"\"      \"\" \nPredictorMatrix:\n        Ozone Solar.R Wind Temp Month Day\nOzone       0       1    1    1     0   1\nSolar.R     1       0    1    1     0   1\nWind        1       1    0    1     0   1\nTemp        1       1    1    0     0   1\nMonth       1       1    1    1     0   1\nDay         1       1    1    1     0   0\nNumber of logged events:  1 \n  it im dep     meth   out\n1  0  0     constant Month\n\n\nFor step 2:\n\n  cmp = complete(imp)\n  cmp\n\n   Ozone Solar.R Wind Temp Month Day\n1   41.0     190  7.4   67     5   1\n2   36.0     118  8.0   72     5   2\n3   12.0     149 12.6   74     5   3\n4   18.0     313 11.5   62     5   4\n5   23.1     173 14.3   56     5   5\n6   28.0     173 14.9   66     5   6\n7   23.0     299  8.6   65     5   7\n8   19.0      99 13.8   59     5   8\n9    8.0      19 20.1   61     5   9\n10  23.1     194  8.6   69     5  10\n\n\nWe see there are no missing values in cmp. They were all imputed with the mean of the other non-missing values. This is mean imputation.\nNow let’s impute the full dataset.\n\n  imp &lt;- mice(airquality, method=\"mean\", m=1, maxit=1)\n\n\n iter imp variable\n  1   1  Ozone  Solar.R\n\n  cmp = complete( imp )\n\nWe next make a dummy variable for each row of our data noting whether anything was imputed or not. We use the ici (Incomplete Case Indication) function to list all rows with any missing values.\n\n  head( ici(airquality) )\n\n[1] FALSE FALSE FALSE FALSE  TRUE  TRUE\n\n\nNote how we have a TRUE or FALSE for each row of our data.\nWe then store this as a covariate in our completed dataset:\n\n  cmp$imputed = ici(airquality)\n  head( cmp )\n\n  Ozone Solar.R Wind Temp Month Day imputed\n1  41.0     190  7.4   67     5   1   FALSE\n2  36.0     118  8.0   72     5   2   FALSE\n3  12.0     149 12.6   74     5   3   FALSE\n4  18.0     313 11.5   62     5   4   FALSE\n5  42.1     186 14.3   56     5   5    TRUE\n6  28.0     186 14.9   66     5   6    TRUE\n\n\n\n9.4.2.1 How well did mean imputation work?\nMean imputation has problems. The imputed values will all be the same, and thus when we look at how much variation is in our variables after imputation, it will go down. Compare the SD of our completed dataset Ozone values to the SD of the Ozone values for our non-missing values.\n\n  sd( airquality$Ozone, na.rm=TRUE )\n\n[1] 33\n\n  sd( cmp$Ozone )\n\n[1] 28.7\n\n\nNext, let’s look at some plots of our completed data, coloring the points by whether they were imputed.\n\nlibrary(ggplot2)\nggplot( cmp, aes(x=Ozone, col=imputed) ) +\n    stat_bin( geom=\"step\", position=\"identity\",\n              breaks=seq(-20, 200, 10) )\n\n\n\n\n\n\n\nggplot( cmp, aes(y=Ozone, x=Solar.R, col=imputed) ) +\n    geom_point() +\n    labs( y=\"Ozone (ppb)\", x=\"Solar Radiation (lang)\" )\n\n\n\n\n\n\n\n\nWhat we see in the above plots is that our imputed observations do not look like the rest of our data because one (or both) of their values always is in the exact center. This creates the “+” shape. It also gives the big spike at the mean for the histogram.\n\n\n9.4.2.2 Important Aside: Namespaces and function collisions\nWe now need to discuss a sad aspect of R. The short story is, different packages have functions with the same names and so if you have both packages loaded you will need to specify which package to use when calling such a function. You can do this by giving the “surname” of the function at the beginning of the function call (like, I believe, the Chinese). This comes up because for us the method complete() exists both in the tidyverse and in mice. In tidyverse, complete() fills in rows of missing combinations of values. In mice, complete() gives us a completed dataset after we have made an imputation call.\nIt turns out that since we loaded tidyverse first and mice second, the mice’s complete() method is the default. But if we loaded the packages in the other order, we would get strange errors. To be clear, we thus tell R to use mice by writing:\n\n  cmp = mice::complete( imp )\n\nIn general, you can detect such “namespace collisions” by noticing weird error messages all of a sudden when you don’t expect them. You can then type, for example, help( complete ) and it will list all the different completes around.\n\n  help( complete )\n\nAlso when you load a package it will write down what functions are getting mixed up for you. If you were looking at your R code you would get something like this:\ntidyr::complete() masks mice::complete()"
  },
  {
    "objectID": "intro_missing_data.html#regression-imputation",
    "href": "intro_missing_data.html#regression-imputation",
    "title": "9  An Introduction to Missing Data",
    "section": "9.5 Regression imputation",
    "text": "9.5 Regression imputation\nRegression imputation is half way between mean imputation and multiple imputation. In regression imputation we predict what values we expect for anything missing based on the other values of the observation. For example, if we know that urban/rural is correlated with race, we might impute a different value for race if we know an observation came from an urban environment vs. rural. We do this with regression: we fit a model predicting each variable using the others and then use that regression model to predict any missing values.\nWe can do this manually, but then it gets very hard when multiple variables are missing for a given observation. The mice package is more clever: it does variables one at a time, and the cycles around so everything can get imputed.\n\n9.5.1 Manually\nHere is how to use other variables to predict missing values.\n\n  ic( airqualitysub )\n\n   Ozone Solar.R Wind Temp Month Day\n5     NA      NA 14.3   56     5   5\n6     28      NA 14.9   66     5   6\n10    NA     194  8.6   69     5  10\n\n  fit &lt;- lm(Ozone ~ Solar.R, data=airqualitysub)\n\n## predict for missing ozone  \n  need.pred = subset( airqualitysub, is.na( Ozone ) )\n  need.pred\n\n   Ozone Solar.R Wind Temp Month Day\n5     NA      NA 14.3   56     5   5\n10    NA     194  8.6   69     5  10\n\n  pred &lt;- predict(fit, newdata=need.pred)\n  pred\n\n   5   10 \n  NA 23.1 \n\n\nBut now we have to merge back in, and we didn’t solve for case 5 because we are missing the variable we would use to predict the other missing variable. Ick. This is where missing data gets really hard (when we have multiple missing values on multiple variables). So let’s quit now and turn to a package that will handle all of this for us.\n\n\n9.5.2 Mice\nTo do regression imputation using mice, we simply call the mice() method:\n\n  imp &lt;- mice(airquality[,1:2], method=\"norm.predict\", m=1, maxit=3,seed=1)\n\n\n iter imp variable\n  1   1  Ozone  Solar.R\n  2   1  Ozone  Solar.R\n  3   1  Ozone  Solar.R\n\n\nWe have everything! How did it do it? By chaining equations. First we start with mean imputation. Then we use our fit model to predict for one covariate, and then we use those predicted scores to predict for the next covariate, and so forth. We cycle back and then everything is jointly predicting everything else.\nThe complete() method gives us a complete dataset with everything imputed. Like so:\n\n  cdat = mice::complete( imp )\n  head( cdat )\n\n  Ozone Solar.R\n1  41.0     190\n2  36.0     118\n3  12.0     149\n4  18.0     313\n5  42.7     186\n6  28.0     169\n\n  nrow( cdat )\n\n[1] 153\n\n  nrow( airquality )\n\n[1] 153\n\n\nNext we make a variable of which cases have imputed values and not (any row with missing data must have been partially imputed.)\n\n  cdat$imputed = ici( airquality )\n\nAnd see our results! Compare to mean imputation, above.\n\nggplot( cdat, aes(x=Ozone, col=imputed) ) +\n    stat_bin( geom=\"step\", position=\"identity\",\n              breaks=seq(-20, 200, 10) )\n\n\n\n\n\n\n\nggplot( cdat, aes(y=Ozone, x=Solar.R, col=imputed) ) +\n    geom_point() +\n    labs( y=\"Ozone (ppb)\", x=\"Solar Radiation (lang)\" )\n\n\n\n\n\n\n\n\nThis is better than mean imputation. See how we impute different Ozone for different Solar Radiation values, taking advantage of the information of knowing that they are correlated? But it still is obvious what is mean imputed and what is not. Also, the variance of our imputed values still does not contain the residual variation around the predicted values that we would get in real data. We can do one more enhancement to fix this.\n\n\n9.5.3 Stochastic regression imputation\nWe extend regression imputation by randomly drawing observations that look like real ones. See in the two imputations below we get slightly different values for our imputed data.\nHere we do it on our mini-dataset and look at the imputed values for our observations with missing values only:\n\n  imp &lt;- mice(airqualitysub[,1:2],method=\"norm.nob\",m=1,maxit=1,seed=1)\n\n\n iter imp variable\n  1   1  Ozone  Solar.R\n\n  imp$imp\n\n$Ozone\n       1\n5   8.09\n10 44.58\n\n$Solar.R\n      1\n5 181.2\n6  83.7\n\n  imp &lt;- mice(airqualitysub[,1:2],method=\"norm.nob\",m=1,maxit=1,seed=4)\n\n\n iter imp variable\n  1   1  Ozone  Solar.R\n\n  imp$imp\n\n$Ozone\n      1\n5  34.4\n10 31.6\n\n$Solar.R\n    1\n5 381\n6 260\n\n\nNow let’s do it on the full data and look at the imputed values and compare to our plots above.\n\n  imp &lt;- mice(airquality[,1:2],method=\"norm.nob\",m=1,maxit=1,seed=1)\n\n\n iter imp variable\n  1   1  Ozone  Solar.R\n\n  cdat = mice::complete( imp )\n  cdat$imputed = ici( airquality )\n\n  ggplot( cdat, aes(x=Ozone, col=imputed) ) +\n    stat_bin( geom=\"step\", position=\"identity\",\n              breaks=seq(-20, 200, 10) )\n\n\n\n\n\n\n\n  ggplot( cdat, aes(y=Ozone, x=Solar.R, col=imputed) ) +\n    geom_point() +\n    labs( y=\"Ozone (ppb)\", x=\"Solar Radiation (lang)\" )\n\n\n\n\n\n\n\n\nBetter, but not perfect. What is better? What is still not perfect?"
  },
  {
    "objectID": "intro_missing_data.html#multiple-imputation",
    "href": "intro_missing_data.html#multiple-imputation",
    "title": "9  An Introduction to Missing Data",
    "section": "9.6 Multiple imputation",
    "text": "9.6 Multiple imputation\nIf missing data is a significant issue in your dataset, then mean or regression imputation can be a bit too hacky and approximate. In these contexts, multiple imputation is the way to go.\nWe do this as follows:\n\n  imp &lt;- mice(airqualitysub, seed=2, print=FALSE)\n\nWarning: Number of logged events: 1\n\n  imp\n\nClass: mids\nNumber of multiple imputations:  5 \nImputation methods:\n  Ozone Solar.R    Wind    Temp   Month     Day \n  \"pmm\"   \"pmm\"      \"\"      \"\"      \"\"      \"\" \nPredictorMatrix:\n        Ozone Solar.R Wind Temp Month Day\nOzone       0       1    1    1     0   1\nSolar.R     1       0    1    1     0   1\nWind        1       1    0    1     0   1\nTemp        1       1    1    0     0   1\nMonth       1       1    1    1     0   1\nDay         1       1    1    1     0   0\nNumber of logged events:  1 \n  it im dep     meth   out\n1  0  0     constant Month\n\n  imp$imp\n\n$Ozone\n    1  2  3  4  5\n5  18 41 28 23 23\n10 36 18 36 19 28\n\n$Solar.R\n    1  2   3  4  5\n5 149 99 194 99 19\n6 194 19 194 19 19\n\n$Wind\n[1] 1 2 3 4 5\n&lt;0 rows&gt; (or 0-length row.names)\n\n$Temp\n[1] 1 2 3 4 5\n&lt;0 rows&gt; (or 0-length row.names)\n\n$Month\n[1] 1 2 3 4 5\n&lt;0 rows&gt; (or 0-length row.names)\n\n$Day\n[1] 1 2 3 4 5\n&lt;0 rows&gt; (or 0-length row.names)\n\n\nSee multiple columns of imputed data? (We have 5 here.)\nFirst aside: All variables you’ll be using for your model should be included in the imputation model. Notice we included the full dataset in mice, not just the variables with missing values. This way we can account for associations between all the outcome and the predictors in the model we’ll be fitting. Your imputation model can be more complicated than your model of interest. That is, you can include additional variables that predict missing values but will not be part of your final model of interest.\nSecond aside: All variables in your imputation model should be in the correct functional form! Quadratic, higher order polynomials and interaction terms are just another variable that we need to impute. Although it may seem logical to impute your variables first and then calculate the interaction or non-linear term, this can lead to bias.\nThird aside: The ordering of the variables in the dataset you are feeding into mice can make a difference in results and model convergence. Generally, you want to order your variables from least to most missing. Here, we reorder the variables from least to most missing, and obtain different results.\n\n  test = airqualitysub[,c(6,5,4,3,2,1)]\n  head(test)\n\n  Day Month Temp Wind Solar.R Ozone\n1   1     5   67  7.4     190    41\n2   2     5   72  8.0     118    36\n3   3     5   74 12.6     149    12\n4   4     5   62 11.5     313    18\n5   5     5   56 14.3      NA    NA\n6   6     5   66 14.9      NA    28\n\n  test.imp &lt;- mice(test, seed=2, print=FALSE)\n\nWarning: Number of logged events: 1\n\n  test.imp$imp\n\n$Day\n[1] 1 2 3 4 5\n&lt;0 rows&gt; (or 0-length row.names)\n\n$Month\n[1] 1 2 3 4 5\n&lt;0 rows&gt; (or 0-length row.names)\n\n$Temp\n[1] 1 2 3 4 5\n&lt;0 rows&gt; (or 0-length row.names)\n\n$Wind\n[1] 1 2 3 4 5\n&lt;0 rows&gt; (or 0-length row.names)\n\n$Solar.R\n    1   2   3   4   5\n5 194 118 194 313 190\n6 118 194 118 118 190\n\n$Ozone\n    1  2  3  4  5\n5  18 23 23 23 41\n10 12  8 18 19  8\n\n\nHow to get each complete dataset?\n\n## first complete dataset \n  mice::complete(imp, 1)\n\n   Ozone Solar.R Wind Temp Month Day\n1     41     190  7.4   67     5   1\n2     36     118  8.0   72     5   2\n3     12     149 12.6   74     5   3\n4     18     313 11.5   62     5   4\n5     18     149 14.3   56     5   5\n6     28     194 14.9   66     5   6\n7     23     299  8.6   65     5   7\n8     19      99 13.8   59     5   8\n9      8      19 20.1   61     5   9\n10    36     194  8.6   69     5  10\n\n## and our second complete dataset\n  mice::complete(imp, 2)\n\n   Ozone Solar.R Wind Temp Month Day\n1     41     190  7.4   67     5   1\n2     36     118  8.0   72     5   2\n3     12     149 12.6   74     5   3\n4     18     313 11.5   62     5   4\n5     41      99 14.3   56     5   5\n6     28      19 14.9   66     5   6\n7     23     299  8.6   65     5   7\n8     19      99 13.8   59     5   8\n9      8      19 20.1   61     5   9\n10    18     194  8.6   69     5  10\n\n\nSee how they are different? They were randomly imputed. We basically used the stochastic regression thing, above, multiple times.\n\n  mice::complete(imp, 1)[ ici(airqualitysub), ]\n\n   Ozone Solar.R Wind Temp Month Day\n5     18     149 14.3   56     5   5\n6     28     194 14.9   66     5   6\n10    36     194  8.6   69     5  10\n\n  mice::complete(imp, 2)[ ici(airqualitysub), ]\n\n   Ozone Solar.R Wind Temp Month Day\n5     41      99 14.3   56     5   5\n6     28      19 14.9   66     5   6\n10    18     194  8.6   69     5  10\n\n\nOn full data:\n\n  imp &lt;- mice(airquality, seed=1, print=FALSE)\n\nNow we estimate for each imputed dataset using the with() method that, in mice, will do the regression for each completed dataset. See help with.mids.\n\n  fit &lt;- with(imp, lm(Ozone ~ Wind + Temp + Solar.R))\n  fit\n\ncall :\nwith.mids(data = imp, expr = lm(Ozone ~ Wind + Temp + Solar.R))\n\ncall1 :\nmice(data = airquality, printFlag = FALSE, seed = 1)\n\nnmis :\n  Ozone Solar.R    Wind    Temp   Month     Day \n     37       7       0       0       0       0 \n\nanalyses :\n[[1]]\n\nCall:\nlm(formula = Ozone ~ Wind + Temp + Solar.R)\n\nCoefficients:\n(Intercept)         Wind         Temp      Solar.R  \n   -66.2402      -2.8219       1.6134       0.0563  \n\n\n[[2]]\n\nCall:\nlm(formula = Ozone ~ Wind + Temp + Solar.R)\n\nCoefficients:\n(Intercept)         Wind         Temp      Solar.R  \n   -71.2842      -2.9055       1.6749       0.0633  \n\n\n[[3]]\n\nCall:\nlm(formula = Ozone ~ Wind + Temp + Solar.R)\n\nCoefficients:\n(Intercept)         Wind         Temp      Solar.R  \n   -66.9511      -2.9322       1.6479       0.0543  \n\n\n[[4]]\n\nCall:\nlm(formula = Ozone ~ Wind + Temp + Solar.R)\n\nCoefficients:\n(Intercept)         Wind         Temp      Solar.R  \n   -33.8480      -3.6628       1.3244       0.0427  \n\n\n[[5]]\n\nCall:\nlm(formula = Ozone ~ Wind + Temp + Solar.R)\n\nCoefficients:\n(Intercept)         Wind         Temp      Solar.R  \n   -77.4163      -2.7438       1.7264       0.0663  \n\n\nThis can take any function call that takes a formula. So glm, lm, whatever… We can then pool the estimates using the standard theory of combining multiply imputed datasets. The basic idea is to combine the variation/uncertainty of the multiple sets with the average uncertainty we would have for each set if it was truly complete and not imputed.\n\n  tab &lt;- summary(pool(fit))\n  colnames( tab )\n\n[1] \"term\"      \"estimate\"  \"std.error\" \"statistic\" \"df\"        \"p.value\"  \n\n  tab[,c(1:3,5)]\n\n         term estimate std.error   df\n1 (Intercept) -63.1480   26.6769 13.8\n2        Wind  -3.0132    0.6831 24.0\n3        Temp   1.5974    0.2742 19.6\n4     Solar.R   0.0566    0.0222 52.4\n\n\nAside: You will notice that once we fit our model on the imputed data, with() returned an object of class mira. Mira objects can be pooled to get the pooled estimates, whereas objects of class glm, lm, lmer, etc. cannot be pooled. You will also notice that you cannot use predict with a mira object. To use predict, you can stack the imputed datasets and fit your model on this complete dataset. Parameter estimates generated by pool are the average of the parameter estimates from the model fit on each imputed dataset separately. So your coefficients are fine. However, your SEs will be underestimated. How underestimated your SEs will be depends, to an extent, on how much data is missing and whether it is missing at random.\nOur old, sad method:\n\n  fit &lt;- lm(Ozone~Wind+Temp+Solar.R,data=airquality,na.action=na.omit)\n  summary( fit )\n\n\nCall:\nlm(formula = Ozone ~ Wind + Temp + Solar.R, data = airquality, \n    na.action = na.omit)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-40.48 -14.22  -3.55  10.10  95.62 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -64.3421    23.0547   -2.79   0.0062 ** \nWind         -3.3336     0.6544   -5.09  1.5e-06 ***\nTemp          1.6521     0.2535    6.52  2.4e-09 ***\nSolar.R       0.0598     0.0232    2.58   0.0112 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 21.2 on 107 degrees of freedom\n  (42 observations deleted due to missingness)\nMultiple R-squared:  0.606, Adjusted R-squared:  0.595 \nF-statistic: 54.8 on 3 and 107 DF,  p-value: &lt;2e-16\n\n  round(coef(summary(fit)),3)\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   -64.34     23.055   -2.79    0.006\nWind           -3.33      0.654   -5.09    0.000\nTemp            1.65      0.254    6.52    0.000\nSolar.R         0.06      0.023    2.58    0.011\n\n\nIn this case, the missing data estimates are basically the same as the complete case analysis, it appears. We only had 5% missing data though."
  },
  {
    "objectID": "intro_missing_data.html#extensions",
    "href": "intro_missing_data.html#extensions",
    "title": "9  An Introduction to Missing Data",
    "section": "9.7 Extensions",
    "text": "9.7 Extensions\n\n9.7.1 Non-continuous variables\nEverything shown above can easily be extended to non-continuous variables. The easiest way to do this is using the mice package. It allows you to specify the type of variable you are imputing, e.g. dichotomous or categorical. Mice will automatically detect and handle non-continuous variables. You can also specify these variables yourself. Here is an example using nhanes data (another built-in R dataset).\n\n## load data \n  data(nhanes2)\n  head(nhanes2)\n\n    age  bmi  hyp chl\n1 20-39   NA &lt;NA&gt;  NA\n2 40-59 22.7   no 187\n3 20-39   NA   no 187\n4 60-99   NA &lt;NA&gt;  NA\n5 20-39 20.4   no 113\n6 60-99   NA &lt;NA&gt; 184\n\n## create some missing values for an ordered categorical variable\n  nhanes2$age[1:5] = NA\n  head(nhanes2) \n\n    age  bmi  hyp chl\n1  &lt;NA&gt;   NA &lt;NA&gt;  NA\n2  &lt;NA&gt; 22.7   no 187\n3  &lt;NA&gt;   NA   no 187\n4  &lt;NA&gt;   NA &lt;NA&gt;  NA\n5  &lt;NA&gt; 20.4   no 113\n6 60-99   NA &lt;NA&gt; 184\n\n## impute 5 datasets \n  imp.cat &lt;- mice(nhanes2, m = 5, print=FALSE)     \n  full.cat = mice::complete(imp.cat)           ## print the first imputed data set\n  head(full.cat)\n\n    age  bmi hyp chl\n1 40-59 21.7 yes 187\n2 40-59 22.7  no 187\n3 20-39 27.5  no 187\n4 60-99 24.9 yes 218\n5 40-59 20.4  no 113\n6 60-99 24.9 yes 184\n\n\nWe can check what imputation method mice used for each variable:\n\n  imp.cat$method\n\n      age       bmi       hyp       chl \n\"polyreg\"     \"pmm\"  \"logreg\"     \"pmm\" \n\n\nWe can see that mice used the polyreg imputation method for the variable age, which means it treated it as an unordered categorical variable. But this is an ordered variable: higher values categories signified older age. We can manually force mice to treat age as an ordered categorical variable. We will keep the imputation methods for the remaining variables the same.\n\n  imp.cat2 &lt;- mice(nhanes2, meth=c(\"polr\",\"pmm\",\"logreg\",\"pmm\"), m=5, print=FALSE)\n  head(mice::complete(imp.cat2, 1))\n\n    age  bmi hyp chl\n1 40-59 27.5 yes 184\n2 60-99 22.7  no 187\n3 60-99 20.4  no 187\n4 20-39 35.3  no 184\n5 40-59 20.4  no 113\n6 60-99 22.7  no 184\n\n  imp.cat2$method\n\n     age      bmi      hyp      chl \n  \"polr\"    \"pmm\" \"logreg\"    \"pmm\" \n\n\n\n\n9.7.2 Multi-level data\nMultilevel data gets more tricky: should we impute taking into account cluster? How do we do that?\nFor an initial pass, I would recommend simply doing regression imputation ignoring cluster/grouping, and then adding in that dummy variable of whether a value is imputed.\n\n\n9.7.3 Longitudinal data\nWith longitudinal data we can often use all our data even for individuals with missing data on the outcome, if we assume data are MAR (“Missing at Random”). MAR means that conditional on the observed data, missingness may depend on any observed data, but not on unobserved data. we explore our missing data on individuals over time and on outcomes as above to get a sense of whether MAR is a reasonable assumption or not. Then lmer basically handles the rest for us, as far as we have enough observations per individual, on average, to estimate the number of random effects we are trying to estimate. With respect to missing data on covariates or predictors, you can handle those with one of the methods described above.\nHere we show how to explore missing data in longitudinal analysis using data on toenail detachment, which you will see in the unit on generalized MLMs. The data is from a RCT where patients were getting a different type of drug to prevent toenail detachment (the outcome).\n\n## load data\n  toes = foreign::read.dta( \"data/toenail.dta\" )\n\nFirst, let’s look at how many times patients were observed.\n\n## how many time points per patient?\n  table( table( toes$patient ) )\n\n\n  1   2   3   4   5   6   7 \n  5   3   7   6  10  39 224 \n\n\nWe have 224 patients observed at all 7 time points, and the rest of the patients are observed at fewer time points, between 1 and 6.\n\n## define function \nsummarise.patient = function( patient ) {\n    pat = rep( \".\", 7 )\n    pat[ patient$visit ] = 'X'\n    paste( pat, collapse=\"\" )\n}\n  ## For each patient, this code makes a string of \".\" \n  ## then it replaces all dots with an \"X\" if we have data for that visit\n\n## summarize missingness  \nmiss = toes %&gt;% group_by( patient ) %&gt;%\n    do( pattern = summarise.patient(.) ) %&gt;%\n    unnest(cols = c(pattern))\n  ## Group the data by patient \n  ## Then use the do() command on each chunk of our dataframe\n  ## The \".\" means \"the chunk\" (it is a pronoun, essentially).  \n  ## This code creates a list of character vectors\n  ## The unnest() takes our character vector out of this list made by \"do\"\n\nhead( miss )\n\n# A tibble: 6 × 2\n  patient pattern\n    &lt;dbl&gt; &lt;chr&gt;  \n1       1 XXXXXXX\n2       2 XXXXXX.\n3       3 XXXXXXX\n4       4 XXXXXXX\n5       6 XXXXXXX\n6       7 XXXXXXX\n\n\nHere we see the different patterns of missing outcomes, i.e., when patients leave and if they come back. When patients leave and never come back, regardless of the time point (see lines 4 and 5), we have monotone missingness.\n\n## sort missing patterns in decreasing order\n## starting with no missingness \nsort( table( miss$pattern ), decreasing=TRUE )\n\n\nXXXXXXX XXXXX.X XXXX.XX XXX.... X...... XXXXX.. XXXX... XX..... XXX.XXX XXXXXX. \n    224      21      10       6       5       5       4       3       3       3 \nXXX.X.. XXXX..X X.XXXXX XX..X.. XX.XXX. XX.XXXX XXX..XX XXX.X.X \n      2       2       1       1       1       1       1       1 \n\n## summarize number of data patterns \nmiss = miss %&gt;% group_by( pattern ) %&gt;%\n    summarise( n=n() )\nmiss = arrange( miss, -n )\nmiss\n\n# A tibble: 18 × 2\n   pattern     n\n   &lt;chr&gt;   &lt;int&gt;\n 1 XXXXXXX   224\n 2 XXXXX.X    21\n 3 XXXX.XX    10\n 4 XXX....     6\n 5 X......     5\n 6 XXXXX..     5\n 7 XXXX...     4\n 8 XX.....     3\n 9 XXX.XXX     3\n10 XXXXXX.     3\n11 XXX.X..     2\n12 XXXX..X     2\n13 X.XXXXX     1\n14 XX..X..     1\n15 XX.XXX.     1\n16 XX.XXXX     1\n17 XXX..XX     1\n18 XXX.X.X     1\n\n## percent missing data (224 complete cases)\n224 / sum( miss$n )\n\n[1] 0.762\n\n  ## 76% of patients with complete data\n\nSecond, we look at patterns of missing outcomes. The outcome here is toenail detachment.\n\n## reshape data to wide \n  dat.wide = reshape( toes2, direction=\"wide\", v.names=\"outcome\",\n                    idvar=\"patient\", timevar = \"visit\" )\n  head( dat.wide )\n\n   patient treatment           Tx outcome.1 outcome.2 outcome.3 outcome.4\n1        1         1 Itraconazole         1         1         1         0\n8        2         0  Terbinafine         0         0         1         1\n14       3         0  Terbinafine         0         0         0         0\n21       4         0  Terbinafine         1         0         0         0\n28       6         1 Itraconazole         1         1         1         0\n35       7         1 Itraconazole         0         0         0         0\n   outcome.5 outcome.6 outcome.7\n1          0         0         0\n8          0         0        NA\n14         0         0         1\n21         0         0         0\n28         0         0         0\n35         1         1         1\n\n## looking at missing data with mice package\n  md.pattern( dat.wide )\n\n\n\n\n\n\n\n\n    patient treatment Tx outcome.1 outcome.2 outcome.3 outcome.4 outcome.7\n224       1         1  1         1         1         1         1         1\n21        1         1  1         1         1         1         1         1\n10        1         1  1         1         1         1         1         1\n2         1         1  1         1         1         1         1         1\n3         1         1  1         1         1         1         1         0\n5         1         1  1         1         1         1         1         0\n4         1         1  1         1         1         1         1         0\n3         1         1  1         1         1         1         0         1\n1         1         1  1         1         1         1         0         1\n1         1         1  1         1         1         1         0         1\n2         1         1  1         1         1         1         0         0\n6         1         1  1         1         1         1         0         0\n1         1         1  1         1         1         0         1         1\n1         1         1  1         1         1         0         1         0\n1         1         1  1         1         1         0         0         0\n3         1         1  1         1         1         0         0         0\n1         1         1  1         1         0         1         1         1\n5         1         1  1         1         0         0         0         0\n          0         0  0         0         6        11        22        30\n    outcome.5 outcome.6    \n224         1         1   0\n21          1         0   1\n10          0         1   1\n2           0         0   2\n3           1         1   1\n5           1         0   2\n4           0         0   3\n3           1         1   1\n1           1         0   2\n1           0         1   2\n2           1         0   3\n6           0         0   4\n1           1         1   1\n1           1         1   2\n1           1         0   4\n3           0         0   5\n1           1         1   1\n5           0         0   6\n           31        50 150\n\n## Another way to generating missingness patterns is to create a function\n## This function takes the visits and outcomes and puts a 1 or 0 if there is an\n## outcome and a dot if missing.\nmake.pat = function( visit, outcome ) {\n    pat = rep( \".\", 7 )\n    pat[ visit ] = outcome\n    paste( pat, collapse=\"\" )\n}\n\n## call our function on all our patients.\noutcomes = toes %&gt;% group_by( patient ) %&gt;%\n    summarise( tx = Tx[[1]],\n               num.obs = n(),\n               num.detach = sum( outcome ),\n               out = make.pat( visit, outcome ) )\n\nhead( outcomes, 20 )\n\n# A tibble: 20 × 5\n   patient tx           num.obs num.detach out    \n     &lt;dbl&gt; &lt;fct&gt;          &lt;int&gt;      &lt;dbl&gt; &lt;chr&gt;  \n 1       1 Itraconazole       7          3 1110000\n 2       2 Terbinafine        6          2 001100.\n 3       3 Terbinafine        7          1 0000001\n 4       4 Terbinafine        7          1 1000000\n 5       6 Itraconazole       7          3 1110000\n 6       7 Itraconazole       7          3 0000111\n 7       9 Itraconazole       7          0 0000000\n 8      10 Terbinafine        7          0 0000000\n 9      11 Itraconazole       7          4 1111000\n10      12 Terbinafine        7          3 0100110\n11      13 Terbinafine        7          4 1111000\n12      15 Itraconazole       6          2 11000.0\n13      16 Itraconazole       6          1 0000.10\n14      17 Terbinafine        6          4 11110.0\n15      18 Terbinafine        6          1 001.000\n16      19 Itraconazole       7          0 0000000\n17      20 Terbinafine        6          0 000.000\n18      21 Itraconazole       3          2 110....\n19      22 Terbinafine        7          3 1110000\n20      23 Itraconazole       7          0 0000000\n\n## how many folks have no detachments?\ntable( outcomes$num.detach )\n\n\n  0   1   2   3   4   5   6   7 \n163  25  25  31  30   8   4   8 \n\n163 / nrow(outcomes)\n\n[1] 0.554\n\n## how many always detached?\nsum( outcomes$num.detach == outcomes$num.obs )\n\n[1] 16\n\n16 / nrow(outcomes)\n\n[1] 0.0544"
  },
  {
    "objectID": "intro_missing_data.html#further-reading",
    "href": "intro_missing_data.html#further-reading",
    "title": "9  An Introduction to Missing Data",
    "section": "9.8 Further reading",
    "text": "9.8 Further reading\nSome further reading on handling missing data. But this is really a course into itself.\n\nGelman & Hill Chapter 25 has a more detailed discussion of missing data imputation.\nWhite IR, Royston P, Wood AM. Multiple imputation using chained equations: issues and guidance for practice. Statistics in Medicine 2011;30: 377-399.\nGraham, JW, Olchowski, AE, Gilreath, TD, 2007. How Many Imputations are Really Needed? Some Practical Clarifications of Multiple Imputation Theory 206–213. https://doi.org/10.1007/s11121-007-0070-9\nvan Buurin S, Groothuis-Oudshoorn K, MICE: Multivariate Imputation by Chained Equations. Journal of Statistical Software. 2011;45(3):1-68.\nGrund S, Lüdtke O, Robitzsch A. Multiple Imputation of Missing Data for Multilevel Models: Simulations and Recommendations. DOI: 10.1177/1094428117703686"
  },
  {
    "objectID": "intro_missing_data.html#appendix-more-about-the-mice-package",
    "href": "intro_missing_data.html#appendix-more-about-the-mice-package",
    "title": "9  An Introduction to Missing Data",
    "section": "9.9 Appendix: More about the mice package",
    "text": "9.9 Appendix: More about the mice package\nThe mice package gives back a very complex object that has a lot of information about how values were imputed, which values were imputed, and so forth. In the following we unpack the imp variable from above a bit more.\nLooking at the imputation object\nIn the following code, we look at the object we get back from mice(). It has lots of parts that we can peek into.\nFirst, the imp list inside of imp stores all of our newly imputed data. It is itself a list of each variable with their imputed values:\n\n  imp$imp\n\n$Ozone\n      1   2   3   4   5\n5     6  19  14   8  14\n10   12  12   7  23  23\n25   14  19  14  19  14\n26   37  18  32  32  18\n27   11   1  18  13  18\n32   65  45  13  28  29\n33   22  36  12  18  16\n34   13  18   1  13  13\n35   63  35  45  52  71\n36   23  39  20  59  96\n37   24  16  12  34  18\n39   64 135  85  80  91\n42  115  76 115  37  91\n43   66 122  78  64 122\n45   44  28  45  23  16\n46   23  45  46  45  35\n52   20  52  63  47  47\n53   59  59  48 115  37\n54   40  16  35  37  63\n55   40  35  48  39  49\n56   23  39  59  59  16\n57   44  52  40  52  20\n58   30  30  27  14  23\n59   45  32  16  16  46\n60   44  27  34  28  30\n61   89  64  80  37  64\n65   16  16  14  23  29\n72   46  52  65  45  35\n75   35  64  71  18  78\n83   20  40  71  46  59\n84   28  63  37  29  63\n102 115  78  78  37  66\n103  46  29  31  23  40\n107  16  30  13  14  22\n115  41  12  44   7  22\n119  50  78 122  85  50\n150  24  12  27  21  12\n\n$Solar.R\n     1   2   3   4   5\n5    7 313  82  13 314\n6  322 187 222  24 238\n11  66 274 139 135 112\n27  20  24   7 238 193\n96 175 223 284 197 220\n97  51 139 274 237  98\n98  98 203 220 188 276\n\n$Wind\n[1] 1 2 3 4 5\n&lt;0 rows&gt; (or 0-length row.names)\n\n$Temp\n[1] 1 2 3 4 5\n&lt;0 rows&gt; (or 0-length row.names)\n\n$Month\n[1] 1 2 3 4 5\n&lt;0 rows&gt; (or 0-length row.names)\n\n$Day\n[1] 1 2 3 4 5\n&lt;0 rows&gt; (or 0-length row.names)\n\n  str( imp$imp )\n\nList of 6\n $ Ozone  :'data.frame':    37 obs. of  5 variables:\n  ..$ 1: int [1:37] 6 12 14 37 11 65 22 13 63 23 ...\n  ..$ 2: int [1:37] 19 12 19 18 1 45 36 18 35 39 ...\n  ..$ 3: int [1:37] 14 7 14 32 18 13 12 1 45 20 ...\n  ..$ 4: int [1:37] 8 23 19 32 13 28 18 13 52 59 ...\n  ..$ 5: int [1:37] 14 23 14 18 18 29 16 13 71 96 ...\n $ Solar.R:'data.frame':    7 obs. of  5 variables:\n  ..$ 1: int [1:7] 7 322 66 20 175 51 98\n  ..$ 2: int [1:7] 313 187 274 24 223 139 203\n  ..$ 3: int [1:7] 82 222 139 7 284 274 220\n  ..$ 4: int [1:7] 13 24 135 238 197 237 188\n  ..$ 5: int [1:7] 314 238 112 193 220 98 276\n $ Wind   :'data.frame':    0 obs. of  5 variables:\n  ..$ 1: logi(0) \n  ..$ 2: logi(0) \n  ..$ 3: logi(0) \n  ..$ 4: logi(0) \n  ..$ 5: logi(0) \n $ Temp   :'data.frame':    0 obs. of  5 variables:\n  ..$ 1: logi(0) \n  ..$ 2: logi(0) \n  ..$ 3: logi(0) \n  ..$ 4: logi(0) \n  ..$ 5: logi(0) \n $ Month  :'data.frame':    0 obs. of  5 variables:\n  ..$ 1: logi(0) \n  ..$ 2: logi(0) \n  ..$ 3: logi(0) \n  ..$ 4: logi(0) \n  ..$ 5: logi(0) \n $ Day    :'data.frame':    0 obs. of  5 variables:\n  ..$ 1: logi(0) \n  ..$ 2: logi(0) \n  ..$ 3: logi(0) \n  ..$ 4: logi(0) \n  ..$ 5: logi(0) \n\n  str( imp$imp$Ozone )\n\n'data.frame':   37 obs. of  5 variables:\n $ 1: int  6 12 14 37 11 65 22 13 63 23 ...\n $ 2: int  19 12 19 18 1 45 36 18 35 39 ...\n $ 3: int  14 7 14 32 18 13 12 1 45 20 ...\n $ 4: int  8 23 19 32 13 28 18 13 52 59 ...\n $ 5: int  14 23 14 18 18 29 16 13 71 96 ...\n\n\nWe see that Ozone and Solar.R have imputed values, and the other variables do not.\nNext, we see two missing observations in our original data and then see the two imputed values for these two missing observations.\n\n  airqualitysub$Ozone\n\n [1] 41 36 12 18 NA 28 23 19  8 NA\n\n  imp$imp$Ozone[,1]\n\n [1]   6  12  14  37  11  65  22  13  63  23  24  64 115  66  44  23  20  59  40\n[20]  40  23  44  30  45  44  89  16  46  35  20  28 115  46  16  41  50  24\n\n\nWe can make (the hard way) a vector of Ozone by plugging our missing values into the original data. But the complete() method, above, is preferred.\n\n  oz = airqualitysub$Ozone\n  oz[ is.na( oz ) ] = imp$imp$Ozone[,1]\n\nWarning in oz[is.na(oz)] = imp$imp$Ozone[, 1]: number of items to replace is\nnot a multiple of replacement length\n\n  oz\n\n [1] 41 36 12 18  6 28 23 19  8 12\n\n\nWhat else is there in imp?\n\n  names(imp)\n\n [1] \"data\"            \"imp\"             \"m\"               \"where\"          \n [5] \"blocks\"          \"call\"            \"nmis\"            \"method\"         \n [9] \"predictorMatrix\" \"visitSequence\"   \"formulas\"        \"post\"           \n[13] \"blots\"           \"ignore\"          \"seed\"            \"iteration\"      \n[17] \"lastSeedValue\"   \"chainMean\"       \"chainVar\"        \"loggedEvents\"   \n[21] \"version\"         \"date\"           \n\n\nWhat was our imputation method?\n\n  imp$method\n\n  Ozone Solar.R    Wind    Temp   Month     Day \n  \"pmm\"   \"pmm\"      \"\"      \"\"      \"\"      \"\" \n\n\nMean imputation for each variable with missing values. Later this will say other thing.\nWhat was used to impute what?\n\n  imp$predictorMatrix\n\n        Ozone Solar.R Wind Temp Month Day\nOzone       0       1    1    1     1   1\nSolar.R     1       0    1    1     1   1\nWind        1       1    0    1     1   1\nTemp        1       1    1    0     1   1\nMonth       1       1    1    1     0   1\nDay         1       1    1    1     1   0"
  },
  {
    "objectID": "intro_missing_data.html#appendix-the-amelia-package",
    "href": "intro_missing_data.html#appendix-the-amelia-package",
    "title": "9  An Introduction to Missing Data",
    "section": "9.10 Appendix: The amelia package",
    "text": "9.10 Appendix: The amelia package\nAmelia is another multiple imputation and missing data package. We do not prefer it, but have some demonstration code in the following, for reference.\n\n  library(Amelia)\n\nFor missingness we can make the following:\n\n  missmap(airquality)\n\n\n\n\n\n\n\n\nEach row of the plot is a row of the data, and missing values are shown in brown. But ugly! And hard to see any trends in the missingness.\nYou can use the Amelia package to do mean imputation.\n\n  library(dplyr)\n\n## exclude variables that do not vary\n  a.airquality = airquality %&gt;% dplyr::select(-Month)\n\n## impute data\n  a.imp &lt;- amelia(a.airquality, m=5)\n\n-- Imputation 1 --\n\n  1  2  3  4  5  6\n\n-- Imputation 2 --\n\n  1  2  3  4  5  6\n\n-- Imputation 3 --\n\n  1  2  3  4  5\n\n-- Imputation 4 --\n\n  1  2  3  4  5  6  7\n\n-- Imputation 5 --\n\n  1  2  3  4  5  6\n\n  a.imp\n\n\nAmelia output with 5 imputed datasets.\nReturn code:  1 \nMessage:  Normal EM convergence. \n\nChain Lengths:\n--------------\nImputation 1:  6\nImputation 2:  6\nImputation 3:  5\nImputation 4:  7\nImputation 5:  6\n\n\nWe can plot our imputed values against our observed values to check that they make sense. We will do this for just one of five datasets we just imputed using Amelia.\n\n## put imputed values from the third dataset in an object\n  one_imp &lt;- a.imp$imputations[[3]]$Ozone\n\n## make object with observed values \n## from observations without missing Ozone values\n  obs_data &lt;- a.airquality$Ozone \n  \n## make a plot overlaying observed and imputed values\n  hist(one_imp[is.na(obs_data)], prob=TRUE, xlab=\"Ozone\",\n       main=\"Histogram of Imputed Values in 3rd Imputation \\nCompared to Density in Observed Data\",\n       col=\"cyan\", ylim=c(0,0.02))\n  lines(density(obs_data[!is.na(obs_data)]), col=\"darkblue\", lwd=2)\n\n\n\n\n\n\n\n\nYou can also do multiple imputation in Amelia. However, Amelia does not have an easy way to combine the estimates from the imputed datasets (no analogue of with() in mice). You can write a function that fits your model of interest in each imputed dataset and then use a package like mitools to pool the estimates and variances.\nMuch easier to use mice!\nAside: A more important limitation of Amelia is that the algorithm it uses to impute missing values assumes multivariate normality, which is often questionable, especially when you have binary variables."
  },
  {
    "objectID": "using_ggplot.html",
    "href": "using_ggplot.html",
    "title": "Using ggPlot",
    "section": "",
    "text": "This section’s handouts are on using ggPlot, with an emphasis on using small multiples and other tricks to plot clustered or longitudinal data and results from multilevel data analysis.\nIt includes how to use prediction to visualize a model’s fit."
  },
  {
    "objectID": "intro_ggplot.html#summarizing",
    "href": "intro_ggplot.html#summarizing",
    "title": "10  Intro to ggplot",
    "section": "10.1 Summarizing",
    "text": "10.1 Summarizing\nYou can also automatically add various statistical summaries, such as simple regression lines:\n\nggplot( dat, aes(y=mathach, x=ses, col=sector ) ) + \n    geom_point() + \n    stat_smooth( method=\"lm\", se = FALSE )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nNotice how it automatically realized you have two subgroups of data defined by sector. It gives you a regression line for each group.\nThe elements of the plot are stacked, and if you remove one of the elements, it will not appear:\n\nggplot( dat, aes(y=mathach, x=ses, col=sector ) ) + \n  stat_smooth( method=\"lm\" )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nHere we also added some uncertainty bars around the regression lines by not saying se = FALSE. (Including uncertainty is the default.)"
  },
  {
    "objectID": "intro_ggplot.html#grouping",
    "href": "intro_ggplot.html#grouping",
    "title": "10  Intro to ggplot",
    "section": "10.2 Grouping",
    "text": "10.2 Grouping\nCombining these ideas we can make a trend line for each school:\n\nmy.plot = ggplot( dat, aes(y=mathach, x=ses, col=sector, group=schoolid ) ) + \n    stat_smooth( method=\"lm\", se = FALSE )\n\nmy.plot\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe trendlines automatically extend to the limits of the data they are run on, hence the different lengths.\nAlso, notice we “saved” the plot in the variable my.plot. Only when we “print” the plot will the plot appear on your display. When we type the name of a variable, it prints. Once you have a plot stored in a variable you can augment it very easily.\nAs you may now realize, ggplot2 is very, very powerful."
  },
  {
    "objectID": "intro_ggplot.html#customization",
    "href": "intro_ggplot.html#customization",
    "title": "10  Intro to ggplot",
    "section": "10.3 Customization",
    "text": "10.3 Customization\nWe next show some other things you can do. For example, you can make lots of little plots:\n\nmy.plot + \n  facet_grid( ~ female ) + \n    ggtitle(\"School-level trend lines for their male and female students\") +\n    labs(x=\"SES\",y=\"Math Achievement\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nOr,\n\n# random subset of schoolid\nsch &lt;- sample( unique( dat$schoolid ), 6 )\n\n# pipe into ggplot \nmy.six.plot &lt;- dat |&gt; \n  filter(schoolid %in% sch) |&gt; \n  ggplot(aes(y=mathach, x=ses, col=sector ) ) + \n    facet_wrap( ~ schoolid, ncol=3 ) + \n    geom_point() + stat_smooth( method=\"lm\" )\n\nmy.six.plot\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nAlso shown are adding titles."
  },
  {
    "objectID": "intro_ggplot.html#themes",
    "href": "intro_ggplot.html#themes",
    "title": "10  Intro to ggplot",
    "section": "10.4 Themes",
    "text": "10.4 Themes\nYou can very quickly change the entire presentation of your plot using themes. There are pre-packaged ones, and you can make your own that you use over and over. Here we set up a theme to be used moving forward\n\nlibrary( ggthemes )\nmy_t = theme_calc() + theme( legend.position=\"bottom\", \n                             legend.direction=\"horizontal\", \n                             legend.key.width=unit(1,\"cm\")  )\ntheme_set( my_t )\n\nCompare the same plot from above, now with a new theme.\n\nmy.six.plot\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nCool, no?"
  },
  {
    "objectID": "intro_ggplot.html#next-steps",
    "href": "intro_ggplot.html#next-steps",
    "title": "10  Intro to ggplot",
    "section": "10.5 Next steps",
    "text": "10.5 Next steps\nThere is a lot of information out there on ggplot and my best advice is to find code examples, and then modify them as needed. There are tutorials and blogs that walk through building plots (search for “ggplot tutorial” for example), but seeing examples seems to be the best way to learn the stuff. For example, you could use the above code for your project one quite readily. And don’t be afraid to ask how to modify plots on Piazza!\nIn particular, check out the excellent “R for Data Science’’ textbook. It extensively uses ggplot, starting here."
  },
  {
    "objectID": "summary_plots.html#national-youth-survey-example",
    "href": "summary_plots.html#national-youth-survey-example",
    "title": "11  Simple tables, plots, and model diagnostics",
    "section": "11.1 National Youth Survey Example",
    "text": "11.1 National Youth Survey Example\nOur running example is the National Youth Survey (NYS) data as described in Raudenbush and Bryk, page 190. This data comes from a survey in which the same students were asked yearly about their acceptance of 9 “deviant” behaviors (such as smoking marijuana, stealing, etc.). The study began in 1976, and followed two cohorts of children, starting at ages 11 and 14 respectively. We will analyze the first 5 years of data.\nAt each time point, we have measures of:\n\nATTIT, the attitude towards deviance, with higher numbers implying higher tolerance for deviant behaviors.\nEXPO, the “exposure”, based on asking the children how many friends they had who had engaged in each of the “deviant” behaviors.\n\nBoth of these variables have been transformed to a logarithmic scale to reduce skew.\nFor each student, we have:\n\nGender (binary)\nMinority status (binary)\nFamily income, in units of $10K (this can be either categorical or continuous).\n\n\n11.1.1 Getting the data ready\nWe’ll focus on the first cohort, from ages 11-15. First, let’s read the data. Note that this data frame is in “wide format”. That is, there is only one row for each student, with all the different observations for that student in different columns of that one row.\n\nnyswide &lt;- read_csv(\"data/nyswide.csv\")\nhead(nyswide)\n\n# A tibble: 6 × 14\n     ID ATTIT.11 EXPO.11 ATTIT.12 EXPO.12 ATTIT.13 EXPO.13 ATTIT.14 EXPO.14\n  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1     3     0.11   -0.37     0.2    -0.27     0      -0.37     0      -0.27\n2     8     0.29    0.42     0.29    0.2      0.11    0.42     0.51    0.2 \n3     9     0.8     0.47     0.58    0.52     0.64    0.2      0.75    0.47\n4    15     0.44    0.07     0.44    0.32     0.89    0.47     0.75    0.26\n5    33     0.2    -0.27     0.64   -0.27     0.69   -0.27    NA      NA   \n6    45     0.11    0.26     0.37   -0.17     0.37    0.14     0.37    0.14\n# ℹ 5 more variables: ATTIT.15 &lt;dbl&gt;, EXPO.15 &lt;dbl&gt;, FEMALE &lt;dbl&gt;,\n#   MINORITY &lt;dbl&gt;, INCOME &lt;dbl&gt;\n\n\nFor our purposes, we want it in “long format”, i.e. each student has multiple rows for the different observations. The pivot_longer() command does this for us.\n\nnys1 &lt;- nyswide |&gt; \n  pivot_longer(ATTIT.11:EXPO.15, names_to = \"score\") |&gt; \n  mutate(outcome = word(score, 1, 1, sep = \"\\\\.\"),\n         age = as.numeric(word(score, 2, 2, sep = \"\\\\.\")),\n         age_fac = factor(age)) |&gt; \n  select(-score) |&gt; \n  pivot_wider(names_from = outcome) |&gt; \n  # drop missing ATTIT values\n  drop_na(ATTIT)\n\nhead( nys1 )\n\n# A tibble: 6 × 8\n     ID FEMALE MINORITY INCOME   age age_fac ATTIT  EXPO\n  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1     3      1        0      3    11 11       0.11 -0.37\n2     3      1        0      3    12 12       0.2  -0.27\n3     3      1        0      3    13 13       0    -0.37\n4     3      1        0      3    14 14       0    -0.27\n5     3      1        0      3    15 15       0.11 -0.17\n6     8      0        0      4    11 11       0.29  0.42\n\n\nJust to get a sense of the data, let’s plot each age as a boxplot\n\n  ggplot(nys1, aes(age_fac, ATTIT)) +\n    geom_boxplot() + \n    labs(title = \"Boxplot of attitude towards deviance by age\", \n         x = \"Age\", y = \"Attitude towards deviance\")\n\n\n\n\nNote: The boxplot’s “x” variable is the group. You get one box per group. The “y” variable is the data we are making boxplots of.\nNote some features of the data:\n\nFirst, we see that ATTIT goes up over time.\nSecond, we see the variation of points also goes up over time. This is evidence of heteroskedasticity.\n\nIf we plot individual lines we have:\n\nnys1 |&gt; \n  drop_na() |&gt; \n  ggplot(aes(age, ATTIT, group=ID)) +\n    geom_line(alpha=0.2, position = \"jitter\") + \n    labs(title = \"Individual trajectories of attitude towards deviance over time\",\n         x = \"Age\",\n         y =\"Attitude towards deviance\")\n\n\n\n\nNote how we have correlation of residuals: some students have systematically lower trajectories and some students have systematically higher trajectories (although there is a lot of bouncing around)."
  },
  {
    "objectID": "summary_plots.html#tabulating-data-categorical-variables",
    "href": "summary_plots.html#tabulating-data-categorical-variables",
    "title": "11  Simple tables, plots, and model diagnostics",
    "section": "11.2 Tabulating data (Categorical variables)",
    "text": "11.2 Tabulating data (Categorical variables)\nWe can tabulate data as so:\n\ntable(nys1$age)\n\n\n 11  12  13  14  15 \n202 209 230 220 218 \n\n\nor\n\ntable(nys1$MINORITY, nys1$age)\n\n   \n     11  12  13  14  15\n  0 159 165 182 175 175\n  1  43  44  48  45  43\n\n\nInterestingly, we have more observations for later ages.\nWe can make “proportion tables” as well:\n\nprop.table( table( nys1$MINORITY, nys1$INCOME  ), margin=1 )\n\n   \n          1       2       3       4       5       6       7       8       9\n  0 0.06075 0.13551 0.18341 0.18107 0.14369 0.10981 0.06893 0.05257 0.00935\n  1 0.28251 0.41704 0.12556 0.05830 0.05830 0.02242 0.01345 0.00000 0.00000\n   \n         10\n  0 0.05491\n  1 0.02242\n\n\nThe margin determines what adds up to 100%."
  },
  {
    "objectID": "summary_plots.html#summary-stats-continuous-variables",
    "href": "summary_plots.html#summary-stats-continuous-variables",
    "title": "11  Simple tables, plots, and model diagnostics",
    "section": "11.3 Summary stats (continuous variables)",
    "text": "11.3 Summary stats (continuous variables)\nThe tableone package is useful:\n\n  library(tableone)\n  \n# sample mean  \n  CreateTableOne(data = nys1,\n                 vars = c(\"ATTIT\"))\n\n                   \n                    Overall    \n  n                 1079       \n  ATTIT (mean (SD)) 0.33 (0.27)\n\n# you can also stratify by a variables of interest\n  CreateTableOne(data = nys1,\n                 vars = c(\"ATTIT\"), \n                 strata = c(\"FEMALE\"))\n\n                   Stratified by FEMALE\n                    0           1           p      test\n  n                  559         520                   \n  ATTIT (mean (SD)) 0.37 (0.27) 0.29 (0.27) &lt;0.001     \n\n# you can also include both binary variables\n  CreateTableOne(data = nys1, \n                 vars = c(\"ATTIT\", \"agefac\"),  # include both binary and continuous variables here\n                 factorVars = c(\"agefac\"), # include only binary variables here\n                 strata = c(\"FEMALE\"))\n\nWarning in ModuleReturnVarsExist(vars, data): The data frame does not have:\nagefac Dropped\n\n\nWarning in ModuleReturnVarsExist(factorVars, data): The data frame does not\nhave: agefac Dropped\n\n\n                   Stratified by FEMALE\n                    0           1           p      test\n  n                  559         520                   \n  ATTIT (mean (SD)) 0.37 (0.27) 0.29 (0.27) &lt;0.001"
  },
  {
    "objectID": "summary_plots.html#table-of-summary-stats",
    "href": "summary_plots.html#table-of-summary-stats",
    "title": "11  Simple tables, plots, and model diagnostics",
    "section": "11.4 Table of summary stats",
    "text": "11.4 Table of summary stats\nYou can easily make pretty tables using the stargazer package:\n\n  library(stargazer)\n\nPlease cite as: \n Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n R package version 5.2.3. https://CRAN.R-project.org/package=stargazer \n# to include all variables\n  stargazer(nys1, header = FALSE)\n\nYou can include only some of the variables and omit stats that are not of interest:\n\n# to include only variables of interest\n  stargazer(nys1[2:7], header=FALSE, \n            omit.summary.stat = c(\"p25\", \"p75\", \"min\", \"max\"), # to omit percentiles\n            title = \"Table 1: Descriptive statistics\")\n\nSee the stargazer help file for how to set/change more of the options: https://cran.r-project.org/web/packages/stargazer/stargazer.pdf"
  },
  {
    "objectID": "summary_plots.html#high-school-and-beyond-example",
    "href": "summary_plots.html#high-school-and-beyond-example",
    "title": "11  Simple tables, plots, and model diagnostics",
    "section": "11.5 High School and Beyond Example",
    "text": "11.5 High School and Beyond Example\nFor this part, we’ll use the HSB data to summarize variables by group/school.\n\n# load data \ndat &lt;- read_dta(\"data/hsb.dta\")"
  },
  {
    "objectID": "summary_plots.html#summarizing-by-group",
    "href": "summary_plots.html#summarizing-by-group",
    "title": "11  Simple tables, plots, and model diagnostics",
    "section": "11.6 Summarizing by group",
    "text": "11.6 Summarizing by group\nTo plot summaries by group, first aggregate your data, and plot the results. Like so:\n\naggdat = dat %&gt;% \n  group_by(schoolid, sector) %&gt;%\n  summarize( per.fem = mean( female ) )\n\n`summarise()` has grouped output by 'schoolid'. You can override using the\n`.groups` argument.\n\nhead( aggdat )\n\n# A tibble: 6 × 3\n# Groups:   schoolid [6]\n  schoolid sector per.fem\n     &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1     1224      0   0.596\n2     1288      0   0.44 \n3     1296      0   0.646\n4     1308      1   0    \n5     1317      1   1    \n6     1358      0   0.367\n\n\nThe including sector (a level 2 variable) is a way to ensure it gets carried through to the aggregated results. Neat trick.\nAnyway, we then plot:\n\nqplot( aggdat$per.fem,\n       main = \"Percent female students\", \n       xlab = \"\")\n\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nNote the single sex (catholic) schools. We can facet to see both groups:\n\nqplot( per.fem, data=aggdat,\n       main = \"Percent female students\", \n       xlab = \"\") +\n  facet_wrap( ~ sector )\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "summary_plots.html#diagnostic-plots",
    "href": "summary_plots.html#diagnostic-plots",
    "title": "11  Simple tables, plots, and model diagnostics",
    "section": "11.7 Diagnostic plots",
    "text": "11.7 Diagnostic plots\nWe can also make some disagnostic plots for our model. first, let’s fit a random intercept model.\n\nm1 &lt;- lmer(mathach ~ 1 + ses + (1|schoolid), data=dat)\narm::display(m1)\n\nlmer(formula = mathach ~ 1 + ses + (1 | schoolid), data = dat)\n            coef.est coef.se\n(Intercept) 12.66     0.19  \nses          2.39     0.11  \n\nError terms:\n Groups   Name        Std.Dev.\n schoolid (Intercept) 2.18    \n Residual             6.09    \n---\nnumber of obs: 7185, groups: schoolid, 160\nAIC = 46653.2, DIC = 46637\ndeviance = 46641.0 \n\n\nWe can check if some of our assumptions are being grossly violated, i.e. residuals at all levels are normally distributed.\n\n  qplot(ranef(m1)$schoolid[,1],\n       main = \"Histogram of random intercepts\", xlab=\"\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n  qplot(resid(m1), \n       main = \"Hisogram of residuals\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nWe can check for heteroskedasticity by plotting residuals against predicted values\n\n  dat$yhat  = predict(m1)\n  dat$resid = resid(m1)\n  \n  ggplot(dat, aes(yhat, resid)) + \n      geom_point(alpha=0.3) + \n      geom_smooth() + \n      labs(title = \"Residuals against predicted values\",\n           x = \"Predicted values\", y =\"Residuals\")\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\nIt looks reasonable (up to the discrete and bounded nature of our data). No major weird curves in the loess line through the residuals means linearity is a reasonable assumption. That being said, our nominal SEs around our loess line are tight, so the mild curve is probably evidence of some model misfit.\nWe can also look at the distribution of random effects using the lattice package\n\n  library(lattice)\n  qqmath(ranef(m1, condVar=TRUE), strip=FALSE)\n\n$schoolid"
  },
  {
    "objectID": "plot_expand_grid.html#introduction",
    "href": "plot_expand_grid.html#introduction",
    "title": "12  Example of making plots with expand.grid",
    "section": "12.1 Introduction",
    "text": "12.1 Introduction\nThis script demonstrates using the predict() function to make plots with separate lines for different groups. A core element is the expand.grid() method. The central idea is this: for each of our groups we manually create a series of points at different levels of our covariate (e.g. ses or time) and then predict the outcome for each of these values. We then plot these predicted points, and it makes a smooth curve for that group.\nIn this document we start with clustered data (the HS&B dataset) and then illustrate how to this with longitudinal data as well."
  },
  {
    "objectID": "plot_expand_grid.html#making-plots-for-the-hsb-dataset",
    "href": "plot_expand_grid.html#making-plots-for-the-hsb-dataset",
    "title": "12  Example of making plots with expand.grid",
    "section": "12.2 Making plots for the HS&B Dataset",
    "text": "12.2 Making plots for the HS&B Dataset\nIn this section we first look at how to plot the model results by making a tiny dataset from the fixed effects, and then we extend to more powerful plotting of individual schools.\n\n12.2.1 Setting up the HS&B data\nThe “many small worlds” view says each school has its own regression line. We are going to plot them all. See the lecture code files for how to load the HS&B dataset. For clarity it is omitted from the printout. We end up with this for the schools:\n\nhead( sdat )\n\n    id size   sector meanses\n1 1224  842   public  -0.428\n2 1288 1855   public   0.128\n3 1296 1719   public  -0.420\n4 1308  716 catholic   0.534\n5 1317  455 catholic   0.351\n6 1358 1430   public  -0.014\n\n\nand this for students (we merged in the school info already):\n\nhead( dat )\n\n    id minority female    ses mathach size sector meanses\n1 1224        0      1 -1.528   5.876  842 public  -0.428\n2 1224        0      1 -0.588  19.708  842 public  -0.428\n3 1224        0      0 -0.528  20.349  842 public  -0.428\n4 1224        0      0 -0.668   8.781  842 public  -0.428\n5 1224        0      0 -0.158  17.898  842 public  -0.428\n6 1224        0      0  0.022   4.583  842 public  -0.428\n\n\nWe fit a fancy random slopes model with 2nd level covariates that impact both the overall school means and the ses by math achievment slopes. Our model is \\[\n\\begin{aligned}\ny_{ij} &= \\beta_{0j} + \\beta_{1j} ses_{ij} +  \\epsilon_{ij} \\\\\n\\beta_{0j} &= \\gamma_{00} + \\gamma_{01} sector_j + u_{0j} \\\\\n\\beta_{1j} &= \\gamma_{10} + \\gamma_{11} sector_j + u_{1j}\n\\end{aligned}\n\\] We omit the equations for the random effect distributions. The \\(\\epsilon_{ij}\\) are normal, and the \\((u_{0j},u_{1j})\\) are bivariate normal, as usual. We fit the model as so:\n\nM1 = lmer( mathach ~ 1 + ses*sector + (1 + ses|id), data=dat )\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge with max|grad| = 0.00578929 (tol = 0.002, component 1)\n\ndisplay( M1 )\n\nlmer(formula = mathach ~ 1 + ses * sector + (1 + ses | id), data = dat)\n                   coef.est coef.se\n(Intercept)        11.75     0.23  \nses                 2.96     0.14  \nsectorcatholic      2.13     0.35  \nses:sectorcatholic -1.31     0.22  \n\nError terms:\n Groups   Name        Std.Dev. Corr \n id       (Intercept) 1.95          \n          ses         0.28     1.00 \n Residual             6.07          \n---\nnumber of obs: 7185, groups: id, 160\nAIC = 46585.1, DIC = 46557.2\ndeviance = 46563.2 \n\n\n\n\n12.2.2 Plotting the model results\nWe can plot the model results by making a little dataset by hand. This section of the handout illustrates how you can hand-construct plots by directly calculating predicted values from your model. This is a very useful skill, and we recommend studying this area of the handout as a way of learning how to control plotting at a very direct level.\nSo, to continue, we proceed in three steps.\nStep 1: Decide on the plot. Let’s make a plot of outcome vs. ses with two lines (one for catholic and one for public). Sometimes it is worth actually sketching the desired plot on scratch paper, identifying the x and y axes and general lines desired.\nStep 2: calculate some outcomes using our model. We do this by deciding what values we want to plot, and then making the outcome.\n\nquantile( dat$ses, c( 0.05, 0.95 ) )\n\n    5%    95% \n-1.318  1.212 \n\nplt = data.frame( ses = c(-1.5, 1.25, -1.5, 1.25 ),\n                  catholic = c( 0, 0, 1, 1 ) )\ncf = fixef( M1 )\ncf\n\n       (Intercept)                ses     sectorcatholic ses:sectorcatholic \n         11.751789           2.957538           2.129531          -1.313363 \n\nplt = mutate( plt,\n              Y = cf[[1]] + cf[[2]]*ses + cf[[3]]*catholic + cf[[4]]*ses*catholic )\nplt\n\n    ses catholic         Y\n1 -1.50        0  7.315482\n2  1.25        0 15.448711\n3 -1.50        1 11.415057\n4  1.25        1 15.936538\n\n\nNote that we have made a little mini-dataset with just the points we want to put on our plot. We calculated these points “by hand”. There is no shame in this.\nStep 3: plot. We plot using ggplot:\n\nplt$catholic = factor( plt$catholic, \n                       labels=c(\"public\",\"catholic\"),\n                       levels=c(0,1) )\nggplot( plt, aes( ses, Y, col=catholic ) ) +\n    geom_line()\n\n\n\n\n\n12.2.2.1 A fancy diversion: categorical variables on the \\(x\\)-axis\nSay we decided to fit a model where we have ses categories:\n\ndat$ses.cat = cut( dat$ses, \n                   breaks=quantile( dat$ses, c( 0, 0.33, 0.67, 1 ) ),\n                   labels = c( \"low\",\"mid\",\"high\"),\n                   include.lowest = TRUE )\ntable( dat$ses.cat )\n\n\n low  mid high \n2371 2462 2352 \n\nM1b = lmer( mathach ~ 1 + ses.cat*sector + (1 + ses|id), data=dat )\ndisplay( M1b )\n\nlmer(formula = mathach ~ 1 + ses.cat * sector + (1 + ses | id), \n    data = dat)\n                           coef.est coef.se\n(Intercept)                 9.19     0.27  \nses.catmid                  2.28     0.25  \nses.cathigh                 5.07     0.29  \nsectorcatholic              3.44     0.42  \nses.catmid:sectorcatholic  -0.98     0.38  \nses.cathigh:sectorcatholic -2.47     0.42  \n\nError terms:\n Groups   Name        Std.Dev. Corr \n id       (Intercept) 2.05          \n          ses         0.47     0.23 \n Residual             6.10          \n---\nnumber of obs: 7185, groups: id, 160\nAIC = 46691.5, DIC = 46660.7\ndeviance = 46666.1 \n\n\nMake our outcomes:\n\nplt = data.frame( ses.mid = c( 0, 1, 0, 0, 1, 0 ),\n                  ses.high = c( 0, 0, 1, 0, 0, 1 ),\n                  catholic = c( 0, 0, 0, 1, 1, 1 ) )\ncf = fixef( M1b )\ncf\n\n               (Intercept)                 ses.catmid \n                 9.1915044                  2.2808807 \n               ses.cathigh             sectorcatholic \n                 5.0721921                  3.4398984 \n ses.catmid:sectorcatholic ses.cathigh:sectorcatholic \n                -0.9759927                 -2.4707460 \n\nplt = mutate( plt,\n              Y = cf[[1]] + cf[[2]]*ses.mid + cf[[3]]*ses.high +\n                cf[[4]]*catholic + cf[[5]]*ses.mid*catholic + cf[[6]]*ses.high*catholic )\nplt\n\n  ses.mid ses.high catholic         Y\n1       0        0        0  9.191504\n2       1        0        0 11.472385\n3       0        1        0 14.263697\n4       0        0        1 12.631403\n5       1        0        1 13.936291\n6       0        1        1 15.232849\n\n\nAnd plot\n\nplt$catholic = factor( plt$catholic, \n                       labels=c(\"public\",\"catholic\"),\n                       levels=c(0,1) )\nplt$ses = \"low\"\nplt$ses[plt$ses.mid==1] = \"mid\"\nplt$ses[plt$ses.high==1] = \"high\"\nplt$ses = factor( plt$ses, levels=c(\"low\",\"mid\",\"high\") )\nggplot( plt, aes( ses, Y, col=catholic, group=catholic ) ) +\n    geom_line() + geom_point()\n\n\n\n\nNote the very important group=catholic line that tells the plot to group everyone by catholic. If not, it will get confused and note that since ses is categorical, try to group on that. Then it cannot make a line since each group has only a single point.\n\n\n\n12.2.3 Plotting individual school regression lines\nWe can plot the individual lines by hand-calculating the school level slopes and intercepts. This code shows how:\n\ncoefs = coef( M1 )$id\nhead( coefs )\n\n     (Intercept)      ses sectorcatholic ses:sectorcatholic\n1224   11.084408 2.863501       2.129531          -1.313363\n1288   12.761032 3.099743       2.129531          -1.313363\n1296    9.193415 2.597052       2.129531          -1.313363\n1308   12.709882 3.092535       2.129531          -1.313363\n1317   10.719013 2.812016       2.129531          -1.313363\n1358   11.478455 2.919031       2.129531          -1.313363\n\ncoefs = rename( coefs, \n                gamma.00 = `(Intercept)`,\n                gamma.10 = `ses`,\n                gamma.01 = `sectorcatholic`,\n                gamma.11 = `ses:sectorcatholic` )\ncoefs$id = rownames( coefs )\ncoefs = merge( coefs, sdat, by=\"id\" )\ncoefs = mutate( coefs,\n                beta.0 = gamma.00 + gamma.01 * (sector==\"catholic\"),\n                beta.1 = gamma.10 + gamma.11 * (sector==\"catholic\") )\n\nNote how we have to add up our gammas to get our betas for each school. See our final betas, one set for each school:\n\nhead( dplyr::select( coefs, -gamma.00, -gamma.10, -gamma.01, -gamma.11 ) )\n\n    id size   sector meanses    beta.0   beta.1\n1 1224  842   public  -0.428 11.084408 2.863501\n2 1288 1855   public   0.128 12.761032 3.099743\n3 1296 1719   public  -0.420  9.193415 2.597052\n4 1308  716 catholic   0.534 14.839413 1.779172\n5 1317  455 catholic   0.351 12.848543 1.498653\n6 1358 1430   public  -0.014 11.478455 2.919031\n\n\nNow let’s plot a subsample of 20 schools\n\nset.seed( 102030 )\nsub20 = sample( unique( dat$id ), 20 )\n\ncoefs.20 = filter( coefs, id %in% sub20 )\n\nggplot( coefs.20, aes( group=id ) ) +\n  geom_abline( aes( slope=beta.1, intercept=beta.0, col=sector) ) +\n  coord_cartesian( xlim=c(-2.5,2), ylim=range(dat$mathach) )\n\n\n\n\nCommentary: We need to specify the size of the plot since we have no data, just the intercepts and slopes. We are using the Emperical Bayes estimates of the random effects added to our school level fixed effects to get the \\(\\hat{\\beta}_{0j}, \\hat{\\beta}_{1j}\\) which define the school-specific regression line for school \\(j\\).\nOur two types of school are clearly separated. Catholic schools have higher average performance, and less of a ses-achievement relationship. Since we have merged in our school level data, we can color the lines by catholic vs public, making our plot easier to read.\n\n\n12.2.4 Plotting with predict()\nA more general plotitng approach is to plot using predict(), where for each student we predict the outcome.\n\ndat$math.hat = predict( M1 )\n\nNow let’s plot a subsample of 20 schools\n\ndat.20 = filter( dat, id %in% sub20 )\n\nggplot( dat.20, aes( ses, math.hat, group=id, col=sector ) ) +\n  geom_line()\n\n\n\n\nBut look at how the lines don’t go the full distance. What ggplot is doing is plotting the individual students, and connecting them with a line. We can see this by plotting the students as well, like this:\n\nggplot( dat.20, aes( ses, math.hat, group=id, col=sector ) ) +\n  geom_line() +\n  geom_point()\n\n\n\n\nWe have a predicted outcome for each student, which removes the student residual, giving just the school trends. If we don’t have students for some range of ses for a school, we won’t have points in our plot for that range for that school. The lines thus give the ranges (left to right) of the ses values in each school.\n\n\n12.2.5 Making our lines go the same length with expand.grid()\nThe way we fix this is we, for each school, make a bunch of fake students with different SES and predict along all those fake students. This will give us equally spaced lines.\nThat being said: the shorter lines above are also informative, as they give you a sense of what the range of ses for each school actually is. Which approach is somewhat a matter of taste.\nWe can generate fake children of each group for each school using expand.grid(). This method will generate a dataframe with all combinations of the given variables supplied. Here we make all combinations of ses, for a set of ses values, and school id.\n\nsynth.dat = expand_grid( id = unique( dat$id ),\n                         ses = seq( -2.5, 2, length.out=9 ) )\nhead( synth.dat )\n\n# A tibble: 6 × 2\n  id       ses\n  &lt;chr&gt;  &lt;dbl&gt;\n1 1224  -2.5  \n2 1224  -1.94 \n3 1224  -1.38 \n4 1224  -0.812\n5 1224  -0.25 \n6 1224   0.312\n\n\nThe seq() command makes an evenly spaced sequence of numbers going from the first to the last, with 9 numbers. E.g.,\n\nseq( 1, 10, length.out=4 )\n\n[1]  1  4  7 10\n\n\nWe then merge our school info back in to get sector for each school id:\n\nsynth.dat = merge( synth.dat, sdat, by=\"id\", all.x=TRUE )\n\nWe finally predict for each school, predicting outcome for our fake kids in each school.\n\nsynth.dat$math.hat = predict( M1, newdata=synth.dat )\n\nWe have predictions just as above, just for students that we set for each school. The school random effects and everything remain because we are using the original school ids.\nUsing our new data, plot 20 random schools–this code is the same as in the prior subsection.\n\nsynth.dat.20 = filter( synth.dat, id %in% sub20 )\n\nggplot( synth.dat.20, aes( ses, math.hat, group=id, col=sector ) ) +\n  geom_line()\n\n\n\n\nBut see our equally spaced students?\n\nggplot( synth.dat.20, aes( ses, math.hat, group=id, col=sector ) ) +\n  geom_line() +\n  geom_point()\n\n\n\n\nWhy do this? The predict() approach allows us to avoid working with the gammas and adding them up like we did above. This is a flexible and powerful approach that avoids a lot of work in many cases. In the next section we illustrate by fitting curves rather than lines. This would be very hard to do directly.\n\n\n12.2.6 Superfancy extra bonus plotting of complex models!\nWe can use predict for weird nonlinear relationships also. This will be important for longitudinal data. To illustrate we fit a model that allows a quadradic relationship between ses and math achievement.\n\ndat$ses2 = dat$ses^2\nM2 = lmer( mathach ~ 1 + (ses + ses2)*sector + meanses + (1 + ses|id), data=dat )\n\ndisplay( M2 )\n\nlmer(formula = mathach ~ 1 + (ses + ses2) * sector + meanses + \n    (1 + ses | id), data = dat)\n                    coef.est coef.se\n(Intercept)         12.17     0.21  \nses                  2.79     0.15  \nses2                 0.04     0.13  \nsectorcatholic       1.23     0.33  \nmeanses              3.14     0.38  \nses:sectorcatholic  -1.35     0.22  \nses2:sectorcatholic  0.06     0.21  \n\nError terms:\n Groups   Name        Std.Dev. Corr \n id       (Intercept) 1.53          \n          ses         0.23     0.49 \n Residual             6.07          \n---\nnumber of obs: 7185, groups: id, 160\nAIC = 46539.7, DIC = 46495.9\ndeviance = 46506.8 \n\n\nTo fit a quadratic model we need our quadratic ses term, which we make by hand. We could also have used I(ses^2) in the lmer() command directly, but people don’t tend to find that easy to read.\nAnd here we predict and plot:\n\nsynth.dat = expand.grid( id = unique( dat$id ),\n                         ses= seq( -2.5, 2, length.out=9 ) )\nsynth.dat$ses2 = synth.dat$ses^2\nsynth.dat = merge( synth.dat, sdat, by=\"id\", all.x=TRUE )\n\nNote how we make our ses2 variable out of ses just like we did above.\n\nsynth.dat$math.hat = predict( M2, newdata=synth.dat )\n\nsynth.dat.20 = filter( synth.dat, id %in% sub20 )\n\nggplot( synth.dat.20, aes( ses, math.hat, group=id, col=sector ) ) +\n  geom_line()\n\n\n\n\nThis code is the same as above. The prediction handles all our model complexity for us.\nAgain, we have our equally spaced students:\n\nggplot( synth.dat.20, aes( ses, math.hat, group=id, col=sector ) ) +\n  geom_line() +\n  geom_point()"
  },
  {
    "objectID": "plot_expand_grid.html#longitudinal-data",
    "href": "plot_expand_grid.html#longitudinal-data",
    "title": "12  Example of making plots with expand.grid",
    "section": "12.3 Longitudinal Data",
    "text": "12.3 Longitudinal Data\nWe next do the above, but for longitudinal data. The story is basically the same.\n\n12.3.1 The data\nWe use the “US Sustaining Effects Study” taken from Raudenbush and Bryk (we have not seen these data in class). We have kids in grades nested in schools. So longitudinal data with a clustering on top of that.\n\nhead( dat )\n\n       CHILDID     SCHOOLID YEAR GRADE   MATH FEMALE SIZE RACEETH\n1 101480302    3440         -0.5     1 -1.694      1  588   black\n2 101480302    3440          0.5     2 -0.211      1  588   black\n3 101480302    3440          1.5     3 -0.403      1  588   black\n4 101480302    3440          2.5     4  0.501      1  588   black\n5 173559292    2820         -0.5     1 -0.194      0  678   white\n6 173559292    2820          0.5     2  2.140      0  678   white\n\n\n\n\n12.3.2 A model\nWe will be using the following 3-level quadradic growth model:\n\nM4 = lmer( MATH ~ 1 + (YEAR + I(YEAR^2)) * (FEMALE * RACEETH ) + \n                (YEAR|CHILDID:SCHOOLID) + (YEAR|SCHOOLID), data=dat )\ndisplay( M4 )\n\nlmer(formula = MATH ~ 1 + (YEAR + I(YEAR^2)) * (FEMALE * RACEETH) + \n    (YEAR | CHILDID:SCHOOLID) + (YEAR | SCHOOLID), data = dat)\n                                 coef.est coef.se\n(Intercept)                      -0.90     0.06  \nYEAR                              0.76     0.02  \nI(YEAR^2)                        -0.04     0.01  \nFEMALE                            0.02     0.05  \nRACEETHhispanic                   0.23     0.10  \nRACEETHwhite                      0.79     0.10  \nFEMALE:RACEETHhispanic           -0.01     0.12  \nFEMALE:RACEETHwhite              -0.34     0.12  \nYEAR:FEMALE                       0.01     0.02  \nYEAR:RACEETHhispanic              0.10     0.03  \nYEAR:RACEETHwhite                 0.07     0.03  \nI(YEAR^2):FEMALE                  0.01     0.01  \nI(YEAR^2):RACEETHhispanic        -0.01     0.01  \nI(YEAR^2):RACEETHwhite           -0.02     0.01  \nYEAR:FEMALE:RACEETHhispanic      -0.01     0.04  \nYEAR:FEMALE:RACEETHwhite         -0.02     0.04  \nI(YEAR^2):FEMALE:RACEETHhispanic  0.00     0.02  \nI(YEAR^2):FEMALE:RACEETHwhite     0.02     0.02  \n\nError terms:\n Groups           Name        Std.Dev. Corr \n CHILDID:SCHOOLID (Intercept) 0.79          \n                  YEAR        0.11     0.55 \n SCHOOLID         (Intercept) 0.34          \n                  YEAR        0.10     0.31 \n Residual                     0.54          \n---\nnumber of obs: 7230, groups: CHILDID:SCHOOLID, 1721; SCHOOLID, 60\nAIC = 16259.7, DIC = 16009.6\ndeviance = 16109.7 \n\n\nWe are just taking the model as given; this document is about showing the fit of this model. In particular, if you haven’t seen 3-level models before, just consider the above as some complex model; the nice thing about predict() is you don’t even need to understand the model you are using! Note we do have a lot of fixed effect interaction terms, allowing for systematically different trajectories for groups of kids that are grouped on recorded race and gender.\n\n\n12.3.3 The simple predict() approach\nWe can use our model to predict outcomes for each timepoint in the data. This will smooth out the time to time variation.\n\ndat$Yhat = predict( M4 )\nggplot( dat, aes( YEAR, Yhat, group=CHILDID ) ) +\n  facet_grid( RACEETH ~ FEMALE ) +\n  geom_line( alpha=0.25 )\n\n\n\n\nNote how the growth lines don’t go across all years for all kids. This is because we were missing data for those kids in the original dataset at those timepoints, so we didn’t predict outcomes when we used the predict() function, above.\nTo fix this we will add in those missing timepoints so we get predictions for all kids for all timepoints.\n\n\n12.3.4 The expand.grid() function\nWe now want different trajectories for the different groups. We can generate fake children of each group for each school using expand.grid(). This method will generate a dataframe with all combinations of the given variables supplied. Here we make all combinations of year, gender, and race/ethnic group for each school.\n\nsynth.dat = expand.grid( CHILDID = -1,\n                         SCHOOLID = levels( dat$SCHOOLID ),\n                         YEAR = unique( dat$YEAR ),\n                         FEMALE = c( 0, 1 ),\n                         RACEETH = levels( dat$RACEETH ) )\nhead( synth.dat )\n\n  CHILDID     SCHOOLID YEAR FEMALE RACEETH\n1      -1 2020         -0.5      0   black\n2      -1 2040         -0.5      0   black\n3      -1 2180         -0.5      0   black\n4      -1 2330         -0.5      0   black\n5      -1 2340         -0.5      0   black\n6      -1 2380         -0.5      0   black\n\nnrow( synth.dat )\n\n[1] 2160\n\n\nThe CHILDID = -1 line means we are making up a new child (not using one of the real ones) so the child random effects will be set to 0 in the predictions.\nOnce we have our dataset, we use predict to calculate the predicted outcomes for each student type for each year timepoint for each school:\n\nsynth.dat = mutate( synth.dat, MATH = predict( M4, \n                                               newdata=synth.dat,\n                                               allow.new.levels = TRUE) )\n\nNow we can plot with our new predictions\n\nggplot( synth.dat, aes( YEAR, MATH, group=SCHOOLID ) ) +\n  facet_grid( RACEETH ~ FEMALE ) +\n  geom_line( alpha=0.5 )\n\n\n\n\nHere we are seeing the different school trajectories for the six types of kid defined by our student-level demographics.\nOr, for a subset of schools\n\nsynth.dat = mutate( synth.dat, GENDER = ifelse( FEMALE, \"female\", \"male\" ) )\nkeepers = sample( unique( synth.dat$SCHOOLID ), 12 )\ns2 = filter( synth.dat, SCHOOLID %in% keepers )\nggplot( s2, aes( YEAR, MATH, col=RACEETH, lty=GENDER ) ) +\n  facet_wrap( ~ SCHOOLID ) +\n  geom_line( alpha=0.5) + geom_point( alpha=0.5 )\n\n\n\n\nHere we see the six lines for the six groups within each school, plotted in little tiles, one for each school.\n\n\n12.3.5 Population aggregation\nYou can also aggregate these predictions. This is the easiest way to get what collection of schools, averaging over their random effects, looks like.\nAggregate with the group_by() and the summarise() methods:\n\nagg.dat = synth.dat %&gt;% group_by( GENDER, RACEETH, YEAR ) %&gt;%\n  dplyr::summarise( MATH = mean( MATH ) )\n\n`summarise()` has grouped output by 'GENDER', 'RACEETH'. You can override using\nthe `.groups` argument.\n\nggplot( agg.dat, aes( YEAR, MATH, col=RACEETH, lty=GENDER ) ) +\n  geom_line( alpha=0.5) + geom_point( alpha=0.5 )\n\n\n\n\nOr do this via predict directly, using the prior ideas\n\nsynth.dat.agg = expand.grid( CHILDID = -1,\n                             SCHOOLID = -1,\n                             YEAR = unique( dat$YEAR ),\n                             FEMALE = c( 0, 1 ),\n                             RACEETH = levels( dat$RACEETH ) )\nnrow( synth.dat.agg )\n\n[1] 36\n\nsynth.dat.agg = mutate( synth.dat.agg, \n                        MATH = predict( M4, \n                                        newdata=synth.dat.agg,\n                                        allow.new.levels = TRUE) )\nsynth.dat.agg = mutate( synth.dat.agg, GENDER = ifelse( FEMALE, \"female\", \"male\" ) )\n\nggplot( synth.dat.agg, aes( YEAR, MATH, col=RACEETH, lty=GENDER ) ) +\n  geom_line( alpha=0.5) + geom_point( alpha=0.5 )\n\n\n\n\nThe above plot suggests that the gender gap only exists for the white children. It also shows that there are racial gaps, and that the Black children appear to be falling further behind as time passes.\nThis block of code is stand-alone, showing the making of fake data and plotting of predictions all in one go. Especially for glms, where there are nonlinearities due to the link function, this will give you the “typical” units, whereas the aggregation method will average over your individuals in the sample.\nFinally, we can also make tables to calculate observed gaps (although in many cases you can just read this sort of thing off the regression table). First spread our data to get columns for each race\n\ns3 = spread( synth.dat.agg, key=\"RACEETH\", value=\"MATH\" )\nhead( s3 )\n\n  CHILDID SCHOOLID YEAR FEMALE GENDER     black  hispanic      white\n1      -1       -1 -2.5      0   male -3.062597 -3.140761 -2.5729327\n2      -1       -1 -2.5      1 female -3.022567 -3.090730 -2.7217865\n3      -1       -1 -1.5      0   male -2.129829 -2.071721 -1.4888230\n4      -1       -1 -1.5      1 female -2.110196 -2.048891 -1.7416614\n5      -1       -1 -0.5      0   male -1.284951 -1.107972 -0.5317700\n6      -1       -1 -0.5      1 female -1.268487 -1.096511 -0.8412427\n\n\nThen summarise:\ntab = s3 %&gt;% group_by( YEAR ) %&gt;% \n  summarise( gap.black.white = mean( white ) - mean( black ),\n             gap.hispanic.white = mean( white ) - mean( hispanic ),\n             gap.black.hispanic = mean( hispanic ) - mean( black ) )\nknitr::kable( tab, digits=2 )\n\n\n\n\nYEAR\ngap.black.white\ngap.hispanic.white\ngap.black.hispanic\n\n\n\n\n-2.5\n0.40\n0.47\n-0.07\n\n\n-1.5\n0.50\n0.45\n0.06\n\n\n-0.5\n0.59\n0.42\n0.17\n\n\n0.5\n0.65\n0.38\n0.27\n\n\n1.5\n0.69\n0.34\n0.35\n\n\n2.5\n0.70\n0.29\n0.41\n\n\n\n\nThis again shows widening gap between Black and White students, and the closing gap of Hispanic and White students.\n\n\n12.3.6 Plotting random effects by Level 2 variable\nYou can also look at estimated random effects as a function of level 2 variables. For example, we can see if there is a pattern of average math score for students by year.\n\nranef = ranef( M4 )$SCHOOLID\nranef$SCHOOLID = rownames( ranef )\nschools = dat %&gt;% group_by( SCHOOLID ) %&gt;%\n  summarise( n = n(),\n             size = SIZE[[1]] )\nschools = merge( schools, ranef, by=\"SCHOOLID\" )\nhead( schools )\n\n      SCHOOLID   n size (Intercept)        YEAR\n1 2020          97  380  0.40323695  0.15257155\n2 2040          89  502  0.11549112  0.07547119\n3 2180         168  777 -0.08149965 -0.08226575\n4 2330         150  800  0.32372001 -0.04389156\n5 2340         220 1133 -0.05151492 -0.01128209\n6 2380          87  439 -0.17018815  0.10802722\n\nggplot( schools, aes( size, `(Intercept)` ) ) +\n  geom_point() +\n  geom_smooth(method=\"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nWe see a possible negative trend."
  },
  {
    "objectID": "ggeffects.html#fit-a-series-of-models",
    "href": "ggeffects.html#fit-a-series-of-models",
    "title": "13  Easy Graphing with ggeffects",
    "section": "13.1 Fit a Series of Models",
    "text": "13.1 Fit a Series of Models\nWe can fit 2- and 3-way interactions, but they can be hard to interpret from the coefficients alone (unless you have a lot of practice).\n\nm1 &lt;- lmer(mathach ~ ses + (1|schoolid), hsb)\nm2 &lt;- lmer(mathach ~ ses + sector + (1|schoolid), hsb)\nm3 &lt;- lmer(mathach ~ ses*sector + (1|schoolid), hsb)\nm4 &lt;- lmer(mathach ~ ses*sector*female + (1|schoolid), hsb)\n\n# tabulate results with tab_model\ntab_model(m1, m2, m3, m4,\n          p.style = \"stars\",\n          show.ci = FALSE,\n          show.se = TRUE)\n\n\n\n\n \nmathach\nmathach\nmathach\nmathach\n\n\nPredictors\nEstimates\nstd. Error\nEstimates\nstd. Error\nEstimates\nstd. Error\nEstimates\nstd. Error\n\n\n(Intercept)\n12.66 ***\n0.19\n11.72 ***\n0.23\n11.80 ***\n0.23\n12.41 ***\n0.25\n\n\nses\n2.39 ***\n0.11\n2.37 ***\n0.11\n2.95 ***\n0.14\n2.73 ***\n0.19\n\n\nsector\n\n\n2.10 ***\n0.34\n2.14 ***\n0.34\n2.16 ***\n0.38\n\n\nses × sector\n\n\n\n\n-1.31 ***\n0.21\n-1.26 ***\n0.30\n\n\nfemale\n\n\n\n\n\n\n-1.16 ***\n0.21\n\n\nses × female\n\n\n\n\n\n\n0.34 \n0.26\n\n\nsector × female\n\n\n\n\n\n\n-0.05 \n0.35\n\n\n(ses × sector) × female\n\n\n\n\n\n\n-0.05 \n0.40\n\n\nRandom Effects\n\n\n\nσ2\n37.03\n37.04\n36.84\n36.63\n\n\n\nτ00\n4.77 schoolid\n3.69 schoolid\n3.69 schoolid\n3.40 schoolid\n\n\nICC\n0.11\n0.09\n0.09\n0.09\n\n\nN\n160 schoolid\n160 schoolid\n160 schoolid\n160 schoolid\n\nObservations\n7185\n7185\n7185\n7185\n\n\nMarginal R2 / Conditional R2\n0.077 / 0.182\n0.114 / 0.195\n0.118 / 0.199\n0.127 / 0.202\n\n\n* p&lt;0.05   ** p&lt;0.01   *** p&lt;0.001"
  },
  {
    "objectID": "ggeffects.html#graph-the-results-with-ggeffects",
    "href": "ggeffects.html#graph-the-results-with-ggeffects",
    "title": "13  Easy Graphing with ggeffects",
    "section": "13.2 Graph the Results with ggeffects",
    "text": "13.2 Graph the Results with ggeffects\nIf we just call ggeffect on the model object, we get a bunch of predicted values”\n\nggeffect(m1)\n\n$ses\n# Predicted values of mathach\n\nses | Predicted |         95% CI\n--------------------------------\n -4 |      3.10 | [ 2.19,  4.00]\n -3 |      5.49 | [ 4.77,  6.21]\n -2 |      7.88 | [ 7.32,  8.43]\n -1 |     10.27 | [ 9.85, 10.69]\n  0 |     12.66 | [12.29, 13.03]\n  1 |     15.05 | [14.62, 15.47]\n  2 |     17.44 | [16.88, 17.99]\n  3 |     19.83 | [19.10, 20.55]\n\nattr(,\"class\")\n[1] \"ggalleffects\" \"list\"        \nattr(,\"model.name\")\n[1] \"m1\"\n\n\nWe can pipe that into plot to get a nice plot:\n\nggeffect(m1) |&gt; \n  plot()\n\n$ses\n\n\n\n\n\nWith multiple covariates, we can call the terms argument. The first input is on X, the second is mapped to color, the third to facet. This makes visualizing the interactions super easy! Any covariates included in the model but not included in terms are held constant at their means.\n\nggeffect(m2, terms = c(\"ses\", \"sector\")) |&gt; \n  plot(ci = FALSE, add.data = TRUE)\n\n\n\nggeffect(m3, terms = c(\"ses\", \"sector\")) |&gt; \n  plot(ci = FALSE)\n\n\n\nggeffect(m4, terms = c(\"ses\", \"sector\", \"female\")) |&gt; \n  plot(ci = FALSE)"
  },
  {
    "objectID": "double_plot.html",
    "href": "double_plot.html",
    "title": "14  Plotting Two Datasets at Once",
    "section": "",
    "text": "It’s easy (though not always advisable) to plot two data sets at once with ggplot. First, we load tidyverse and our HSB data. We then create a school-level aggregate data set of just the mean SES values.\n\nlibrary(tidyverse)\nlibrary(haven)\n\n# clear memory\nrm(list = ls())\n\ntheme_set(theme_classic())\n\n# load HSB data\nhsb &lt;- read_dta(\"data/hsb.dta\") |&gt; \n  select(mathach, ses, schoolid)\n\nsch &lt;- hsb |&gt; \n  group_by(schoolid) |&gt; \n  summarise(mean_ses = mean(ses),\n            mean_mathach = mean(mathach))\n\nLet’s say we wanted to plot both the individual students and the school means. This is easy enough to do separately:\n\nggplot(hsb, aes(x = ses, y = mathach)) +\n  geom_point(alpha = 0.1)\n\n\n\nggplot(sch, aes(x = mean_ses, y = mean_mathach)) +\n  geom_point()\n\n\n\n\nWe can superimpose both plots as follows. Essentially, the first argument in ggplot provides the data, and by default, this is passed to all subsequent layers of the plot. We can override this behavior by specifying a different data set (and aesthetic mappings, if desired) within an individual layer of ggplot, such as geom_point.\n\nggplot(hsb, aes(x = ses, y = mathach)) +\n  geom_point(alpha = 0.1) +\n  geom_point(data = sch, aes(x = mean_ses, y = mean_mathach), color = \"red\")"
  },
  {
    "objectID": "mod_fit.html",
    "href": "mod_fit.html",
    "title": "Model Fitting and Interpretation",
    "section": "",
    "text": "This section covers model fitting and interpretation"
  },
  {
    "objectID": "broom.html#simple-demonstration",
    "href": "broom.html#simple-demonstration",
    "title": "15  Extracting model information with broom",
    "section": "15.1 Simple Demonstration",
    "text": "15.1 Simple Demonstration\nOne of my favorite R packages is broom, which has many awesome convenience functions for regression models, including MLMs. broom.mixed is an extension that works with lmer models.\n\n# load libraries\nlibrary(tidyverse)\nlibrary(broom.mixed)\nlibrary(haven)\nlibrary(knitr)\nlibrary(lme4)\n\n# clear memory\nrm(list = ls())\n\n# load HSB data\nhsb &lt;- read_dta(\"data/hsb.dta\")\n\n\n15.1.1 tidy\ntidy takes a model object and returns the output as a tidy tibble, which makes it very easy to work with. Compare the results below:\n\nols &lt;- lm(mathach ~ ses, hsb)\n\n# ugly!\nsummary(ols)\n\n\nCall:\nlm(formula = mathach ~ ses, data = hsb)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.4382  -4.7580   0.2334   5.0649  15.9007 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 12.74740    0.07569  168.42   &lt;2e-16 ***\nses          3.18387    0.09712   32.78   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.416 on 7183 degrees of freedom\nMultiple R-squared:  0.1301,    Adjusted R-squared:   0.13 \nF-statistic:  1075 on 1 and 7183 DF,  p-value: &lt; 2.2e-16\n\n# beautiful!\ntidy(ols)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    12.7     0.0757     168.  0        \n2 ses             3.18    0.0971      32.8 8.71e-220\n\n# even better\nols |&gt; tidy() |&gt; kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n12.75\n0.08\n168.42\n0\n\n\nses\n3.18\n0.10\n32.78\n0\n\n\n\n\n# works great for MLMs\nmlm &lt;- lmer(mathach ~ ses + mnses + (ses|schoolid), hsb)\n\ntidy(mlm)\n\n# A tibble: 7 × 6\n  effect   group    term                 estimate std.error statistic\n  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 fixed    &lt;NA&gt;     (Intercept)            12.7       0.151     84.2 \n2 fixed    &lt;NA&gt;     ses                     2.19      0.122     18.0 \n3 fixed    &lt;NA&gt;     mnses                   3.78      0.383      9.88\n4 ran_pars schoolid sd__(Intercept)         1.64     NA         NA   \n5 ran_pars schoolid cor__(Intercept).ses   -0.212    NA         NA   \n6 ran_pars schoolid sd__ses                 0.673    NA         NA   \n7 ran_pars Residual sd__Observation         6.07     NA         NA   \n\n\n\n\n15.1.2 glance\nWhat about model fit stats? That’s where glance comes in:\n\nglance(ols)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic   p.value    df  logLik    AIC    BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1     0.130         0.130  6.42     1075. 8.71e-220     1 -23549. 47104. 47125.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nglance(mlm) |&gt; \n  kable(digits = 2)\n\n\n\n\nnobs\nsigma\nlogLik\nAIC\nBIC\nREMLcrit\ndf.residual\n\n\n\n\n7185\n6.07\n-23280.71\n46575.42\n46623.58\n46561.42\n7178\n\n\n\n\n\n\n\n15.1.3 augment\nWhat about residuals? augment to the rescue:\n\nmlm |&gt; \n  ranef() |&gt; \n  augment() |&gt; \n  head() |&gt; \n  kable(digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngrp\nvariable\nlevel\nestimate\nqq\nstd.error\nlb\nub\n\n\n\n\nschoolid\n(Intercept)\n8367\n-4.14\n-0.18\n0.78\n-5.43\n-2.85\n\n\nschoolid\n(Intercept)\n4523\n-3.09\n0.02\n0.98\n-4.70\n-1.47\n\n\nschoolid\n(Intercept)\n6990\n-2.98\n-1.46\n0.78\n-4.26\n-1.70\n\n\nschoolid\n(Intercept)\n3705\n-2.81\n0.28\n1.09\n-4.61\n-1.02\n\n\nschoolid\n(Intercept)\n8854\n-2.57\n-0.85\n0.80\n-3.89\n-1.25\n\n\nschoolid\n(Intercept)\n9397\n-2.43\n-0.65\n0.92\n-3.94\n-0.92"
  },
  {
    "objectID": "broom.html#extracting-lmer-model-info",
    "href": "broom.html#extracting-lmer-model-info",
    "title": "15  Extracting model information with broom",
    "section": "15.2 Extracting lmer model info",
    "text": "15.2 Extracting lmer model info\n\n15.2.1 Obtaining Fixed Effects\nlmer models are reduced form, so fixed effects include both L1 and L2 predictors. tidy denotes the type of effect in a column called effect, where fixed means fixed, and ran_pars means random (standing for “random parameters”)\n\nmlm |&gt; \n  tidy() |&gt; \n  filter(effect == \"fixed\")\n\n# A tibble: 3 × 6\n  effect group term        estimate std.error statistic\n  &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 fixed  &lt;NA&gt;  (Intercept)    12.7      0.151     84.2 \n2 fixed  &lt;NA&gt;  ses             2.19     0.122     18.0 \n3 fixed  &lt;NA&gt;  mnses           3.78     0.383      9.88\n\n\nWe can use the [[]] notation or a pipeline to extract elements from the data frame:\n\n# within effect of SES\ntidy(mlm)[[2,4]]\n\n[1] 2.190349\n\n# contextual effect of SES\ntidy(mlm)[[3,4]]\n\n[1] 3.781243\n\n# using the variable names in a pipeline\nmlm |&gt; \n  tidy() |&gt; \n  filter(term == \"ses\") |&gt; \n  pull(estimate)\n\n[1] 2.190349\n\n\n\n\n15.2.2 Obtaining Random Effects\ntidy includes the random effects (SDs and correlations) right there in the output. For example, sd__ses is the SD of the SES slope.\n\n# display all random effects\nmlm |&gt; \n  tidy() |&gt; \n  filter(effect == \"ran_pars\")\n\n# A tibble: 4 × 6\n  effect   group    term                 estimate std.error statistic\n  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;                   &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 ran_pars schoolid sd__(Intercept)         1.64         NA        NA\n2 ran_pars schoolid cor__(Intercept).ses   -0.212        NA        NA\n3 ran_pars schoolid sd__ses                 0.673        NA        NA\n4 ran_pars Residual sd__Observation         6.07         NA        NA\n\n# pull single number\nmlm |&gt; \n  tidy() |&gt; \n  filter(term == \"sd__ses\") |&gt; \n  pull(estimate)\n\n[1] 0.6730818\n\n\n\n\n15.2.3 Obtaining Empirical Bayes Estimates of the Random Effects\nThis is best done in a pipeline. We first apply ranef, then augment and get the EB estimates in the estimate column, along with the std.error, confidence bounds, and qq statistics.\n\nmlm |&gt; \n  ranef() |&gt; \n  augment() |&gt; \n  head()\n\n       grp    variable level  estimate         qq std.error        lb\n1 schoolid (Intercept)  8367 -4.137656 -0.1811498 0.7845770 -5.428170\n2 schoolid (Intercept)  4523 -3.089835  0.0235018 0.9819306 -4.704967\n3 schoolid (Intercept)  6990 -2.981315 -1.4619679 0.7779876 -4.260991\n4 schoolid (Intercept)  3705 -2.811935  0.2776904 1.0911916 -4.606785\n5 schoolid (Intercept)  8854 -2.569302 -0.8528365 0.8045804 -3.892719\n6 schoolid (Intercept)  9397 -2.431031 -0.6452734 0.9163587 -3.938307\n          ub\n1 -2.8471413\n2 -1.4747032\n3 -1.7016394\n4 -1.0170840\n5 -1.2458846\n6 -0.9237553\n\n\n\n\n15.2.4 Intercept-Slope Correlation\nThe BLUPs are in long form. We can reshape to wide if we want to, for example, visualize the correlation between the random intercepts and slopes.\n\nblups &lt;- mlm |&gt; \n  ranef() |&gt; \n  augment() |&gt; \n  select(variable, level, estimate) |&gt; \n  pivot_wider(names_from = variable, values_from = estimate,\n              id_cols = level) |&gt; \n  rename(schoolid = 1, random_intercept = 2, random_slope = 3)\n\nhead(blups)\n\n# A tibble: 6 × 3\n  schoolid random_intercept random_slope\n  &lt;fct&gt;               &lt;dbl&gt;        &lt;dbl&gt;\n1 8367                -4.14       0.159 \n2 4523                -3.09       0.272 \n3 6990                -2.98      -0.0353\n4 3705                -2.81      -0.0968\n5 8854                -2.57       0.377 \n6 9397                -2.43       0.174 \n\nggplot(blups, aes(x = random_intercept, y = random_slope)) +\n  geom_point()\n\n\n\n\n\n\n15.2.5 Caterpillar Plots\nThe included information makes it easy to construct caterpillar plots!\n\nri &lt;- mlm |&gt; \n  ranef() |&gt; \n  augment() |&gt; \n  filter(variable == \"(Intercept)\")\n\nggplot(ri, aes(x = level, y = estimate,\n               ymin = lb,\n               ymax = ub)) +\n  geom_point() +\n  geom_errorbar() +\n  theme_classic()\n\n\n\n\n\n\n15.2.6 Fitted Values\nUsing augment directly on the lmer object gives us fitted values (.fitted) and residuals (.resid). We can use this for residual plots or for plotting lines for each school.\n\nmlm |&gt; \n  augment() |&gt; \n  head()\n\n# A tibble: 6 × 15\n  mathach     ses  mnses schoolid .fitted .resid   .hat   .cooksd .fixed   .mu\n    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1    5.88 -1.53   -0.434     1224    7.29 -1.41  0.0325 0.000629    7.68  7.29\n2   19.7  -0.588  -0.434     1224    9.43 10.3   0.0177 0.0175      9.74  9.43\n3   20.3  -0.528  -0.434     1224    9.57 10.8   0.0173 0.0188      9.87  9.57\n4    8.78 -0.668  -0.434     1224    9.25 -0.468 0.0183 0.0000376   9.57  9.25\n5   17.9  -0.158  -0.434     1224   10.4   7.49  0.0164 0.00863    10.7  10.4 \n6    4.58  0.0220 -0.434     1224   10.8  -6.24  0.0170 0.00619    11.1  10.8 \n# ℹ 5 more variables: .offset &lt;dbl&gt;, .sqrtXwt &lt;dbl&gt;, .sqrtrwt &lt;dbl&gt;,\n#   .weights &lt;dbl&gt;, .wtres &lt;dbl&gt;\n\n# fitted lines\nmlm |&gt; \n  augment() |&gt; \n  ggplot(aes(x = ses, y = .fitted, group = schoolid)) +\n  geom_line( alpha=0.5 )\n\n\n\n# residuals\nmlm |&gt; \n  augment() |&gt; \n  ggplot(aes(y = .resid, x = .fitted)) +\n  geom_hline(yintercept = 0, color = \"red\") +\n  geom_point(alpha = 0.2)"
  },
  {
    "objectID": "broom.html#additional-resources",
    "href": "broom.html#additional-resources",
    "title": "15  Extracting model information with broom",
    "section": "15.3 Additional Resources",
    "text": "15.3 Additional Resources\nI’ve recently discovered the packaged mixedup that has some excellent convenience functions for extracting info from lmer models: https://m-clark.github.io/mixedup/index.html"
  },
  {
    "objectID": "lmer_extract.html#introduction",
    "href": "lmer_extract.html#introduction",
    "title": "16  How to extract information from fitted lmer models",
    "section": "16.1 Introduction",
    "text": "16.1 Introduction\nThis document walks through various R code to pull information out of a multilevel model (and OLS models as well, since the methods generally work on everything). For illustration, we will use a random-slope model on the HS&B dataset with some level 1 and level 2 fixed effects.\n\n16.1.1 Libraries\nWe use the following libraries in this file:\n\nlibrary( lme4 )\nlibrary( foreign ) ## to load data\nlibrary( arm )\nlibrary( tidyverse )\n\n\n\n16.1.2 Loading the data\nLoading the data is simple. We read student and school level data and merge:\n\ndat = read.spss( \"data/hsb1.sav\", to.data.frame=TRUE )\nsdat = read.spss( \"data/hsb2.sav\", to.data.frame=TRUE )\n\nre-encoding from CP1252\n\ndat = merge( dat, sdat, by=\"id\", all.x=TRUE )\nhead( dat, 3 )\n\n    id minority female    ses mathach size sector pracad disclim himinty\n1 1224        0      1 -1.528   5.876  842      0   0.35   1.597       0\n2 1224        0      1 -0.588  19.708  842      0   0.35   1.597       0\n3 1224        0      0 -0.528  20.349  842      0   0.35   1.597       0\n  meanses\n1  -0.428\n2  -0.428\n3  -0.428"
  },
  {
    "objectID": "lmer_extract.html#fitting-and-viewing-the-model",
    "href": "lmer_extract.html#fitting-and-viewing-the-model",
    "title": "16  How to extract information from fitted lmer models",
    "section": "16.2 Fitting and viewing the model",
    "text": "16.2 Fitting and viewing the model\nNow we fit the random slope model with the level-2 covariates:\n\nM1 = lmer( mathach ~ 1 + ses + meanses + (1 + ses|id), data=dat )\n\nTo get an overview of what our fitted model is, use arm’s display() method:\n\ndisplay( M1 )\n\nlmer(formula = mathach ~ 1 + ses + meanses + (1 + ses | id), \n    data = dat)\n            coef.est coef.se\n(Intercept) 12.65     0.15  \nses          2.19     0.12  \nmeanses      3.78     0.38  \n\nError terms:\n Groups   Name        Std.Dev. Corr  \n id       (Intercept) 1.64           \n          ses         0.67     -0.21 \n Residual             6.07           \n---\nnumber of obs: 7185, groups: id, 160\nAIC = 46575.4, DIC = 46552.4\ndeviance = 46556.9 \n\n\n\n16.2.1 The summary() method\nWe can also look at the messier default summary() command, which gives you more output. The real win is if we use the lmerTest library and fit our model with that package loaded, our summary() is more exciting and has \\(p\\)-values:\n\nlibrary( lmerTest )\nM1 = lmer( mathach ~ 1 + ses + meanses + (1 + ses|id), data=dat )\nsummary( M1 )\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: mathach ~ 1 + ses + meanses + (1 + ses | id)\n   Data: dat\n\nREML criterion at convergence: 46561.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.1671 -0.7270  0.0163  0.7547  2.9646 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n id       (Intercept)  2.695   1.6417        \n          ses          0.453   0.6731   -0.21\n Residual             36.796   6.0659        \nNumber of obs: 7185, groups:  id, 160\n\nFixed effects:\n            Estimate Std. Error       df t value Pr(&gt;|t|)    \n(Intercept)  12.6513     0.1506 152.9599  84.000   &lt;2e-16 ***\nses           2.1903     0.1218 178.2055  17.976   &lt;2e-16 ***\nmeanses       3.7812     0.3826 181.7675   9.883   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n        (Intr) ses   \nses     -0.080       \nmeanses -0.028 -0.256\n\n\nIf we just print the object, e.g., by typing the name of the model on the console, we get minimal information:\n\nM1\n\nLinear mixed model fit by REML ['lmerModLmerTest']\nFormula: mathach ~ 1 + ses + meanses + (1 + ses | id)\n   Data: dat\nREML criterion at convergence: 46561.42\nRandom effects:\n Groups   Name        Std.Dev. Corr \n id       (Intercept) 1.6417        \n          ses         0.6731   -0.21\n Residual             6.0659        \nNumber of obs: 7185, groups:  id, 160\nFixed Effects:\n(Intercept)          ses      meanses  \n     12.651        2.190        3.781"
  },
  {
    "objectID": "lmer_extract.html#obtaining-fixed-effects",
    "href": "lmer_extract.html#obtaining-fixed-effects",
    "title": "16  How to extract information from fitted lmer models",
    "section": "16.3 Obtaining Fixed Effects",
    "text": "16.3 Obtaining Fixed Effects\nR thinks of models in reduced form. Thus when we get the fixed effects we get both the level-1 and level-2 fixed effects\n\nfixef( M1 )\n\n(Intercept)         ses     meanses \n  12.651300    2.190350    3.781218 \n\n\nThe above is a vector of numbers. Each element is named, but we can index them as so:\n\nfixef( M1 )[2]\n\n    ses \n2.19035 \n\n\nWe can also use the [[]] which means “give me that element not as a list but as just the element!” When in doubt, if you want one thing out of a list or vector, use [[]] instead of []:\n\nfixef( M1 )[[2]]\n\n[1] 2.19035\n\n\nSee how it gives you the number without the name here?"
  },
  {
    "objectID": "lmer_extract.html#variance-and-covariance-estimates-of-random-effects",
    "href": "lmer_extract.html#variance-and-covariance-estimates-of-random-effects",
    "title": "16  How to extract information from fitted lmer models",
    "section": "16.4 Variance and Covariance estimates of Random Effects",
    "text": "16.4 Variance and Covariance estimates of Random Effects\nWe can get the Variance-Covariance matrix of the random effects with VarCorr.\n\nVarCorr( M1 )\n\n Groups   Name        Std.Dev. Corr  \n id       (Intercept) 1.64174        \n          ses         0.67309  -0.212\n Residual             6.06594        \n\n\nIt displays nicely if you just print it out, but inside it are covariance matrices for each random effect group. (In our model we only have one group, id.) These matrices also have correlation matrices for reference. Here is how to get these pieces:\n\nvc = VarCorr( M1 )$id\nvc\n\n            (Intercept)        ses\n(Intercept)   2.6953203 -0.2339045\nses          -0.2339045  0.4530494\nattr(,\"stddev\")\n(Intercept)         ses \n  1.6417431   0.6730894 \nattr(,\"correlation\")\n            (Intercept)        ses\n(Intercept)   1.0000000 -0.2116707\nses          -0.2116707  1.0000000\n\n\nYou might be wondering what all the attr stuff is. R can “tack on” extra information to a variable via “attributes”. Attributes are not part of the variable exactly, but they follows their variable around. The attr (for attribute) method is a way to get these extra bits of information. In the above, R is tacking the correlation matrix on to the variance-covariance matrix to save you the trouble of calculating it yourself. Get it as follows:\n\nattr( vc, \"correlation\" )\n\n            (Intercept)        ses\n(Intercept)   1.0000000 -0.2116707\nses          -0.2116707  1.0000000\n\n\nYou can also just use the vc object as a matrix. Here we take the diagonal of it\n\ndiag( vc )\n\n(Intercept)         ses \n  2.6953203   0.4530494 \n\n\nIf you want an element from a matrix use row-column indexing like so:\n\nvc[1,2]\n\n[1] -0.2339045\n\n\nfor row 1 and column 2.\n\n16.4.0.1 The sigma.hat() and sigma() methods\nIf you just want the variances and standard deviations of your random effects, use sigma.hat(). This also gives you the residual standard deviation as well. The output is a weird object, with a list of things that are themselves lists in it. Let’s examine it. First we look at what the whole thing is:\n\nsigma.hat( M1 )\n\n$sigma\n$sigma$data\n[1] 6.065939\n\n$sigma$id\n(Intercept)         ses \n  1.6417431   0.6730894 \n\n\n$cors\n$cors$data\n[1] NA\n\n$cors$id\n            (Intercept)        ses\n(Intercept)   1.0000000 -0.2116707\nses          -0.2116707  1.0000000\n\nnames( sigma.hat( M1 ) )\n\n[1] \"sigma\" \"cors\" \n\nsigma.hat( M1 )$sigma\n\n$data\n[1] 6.065939\n\n$id\n(Intercept)         ses \n  1.6417431   0.6730894 \n\n\nOur standard deviations of the random effects are\n\nsigma.hat( M1 )$sigma$id\n\n(Intercept)         ses \n  1.6417431   0.6730894 \n\n\nWe can get our residual variance by this weird thing (we are getting data from the sigma inside of sigma.hat( M1 )):\n\nsigma.hat( M1 )$sigma$data\n\n[1] 6.065939\n\n\nBut here is an easier way using the sigma() utility function:\n\nsigma( M1 )\n\n[1] 6.065939"
  },
  {
    "objectID": "lmer_extract.html#obtaining-emperical-bayes-estimates-of-the-random-effects",
    "href": "lmer_extract.html#obtaining-emperical-bayes-estimates-of-the-random-effects",
    "title": "16  How to extract information from fitted lmer models",
    "section": "16.5 Obtaining Emperical Bayes Estimates of the Random Effects",
    "text": "16.5 Obtaining Emperical Bayes Estimates of the Random Effects\nRandom effects come out of the ranef() method. Each random effect is its own object inside the returned object. You refer to these sets of effects by name. Here our random effect is called id.\n\nests = ranef( M1 )$id\nhead( ests )\n\n     (Intercept)         ses\n1224 -0.26204371  0.08765385\n1288  0.03805199  0.11841937\n1296 -1.91525901  0.03572247\n1308  0.30485682 -0.10500515\n1317 -1.15834807 -0.10815301\n1358 -0.98212459  0.44612877\n\n\nGenerally, what you get back from these calls is a new data frame with a row for each group. The rows are named with the original id codes for the groups, but if you want to connect it back to your group-level information you are going to want to merge stuff. To do this, and to keep things organized, I recommend adding the id as a column to your dataframe:\n\nnames(ests) = c( \"u0\", \"u1\" )\nests$id = rownames( ests )\nhead( ests )\n\n              u0          u1   id\n1224 -0.26204371  0.08765385 1224\n1288  0.03805199  0.11841937 1288\n1296 -1.91525901  0.03572247 1296\n1308  0.30485682 -0.10500515 1308\n1317 -1.15834807 -0.10815301 1317\n1358 -0.98212459  0.44612877 1358\n\n\nWe also renamed our columns of our dataframe to give them names nicer than (Intercept). You can use these names if you wish, however. You just need to quote them with back ticks (this code is not run):\n\nhead( ests$`(Intercept)` )\n\n\n16.5.1 The coef() method\nWe can also get a slighly different (but generally easier to use) version these things through coef(). What coef() does is give you the estimated regression lines for each group in your data by combining the random effect for each group with the corresponding fixed effects. Note how in the following the meanses coefficient is the same, but the others vary due to the random slope and random intercept.\n\ncoefs = coef( M1 )$id\nhead( coefs )\n\n     (Intercept)      ses  meanses\n1224    12.38926 2.278004 3.781218\n1288    12.68935 2.308769 3.781218\n1296    10.73604 2.226072 3.781218\n1308    12.95616 2.085345 3.781218\n1317    11.49295 2.082197 3.781218\n1358    11.66918 2.636479 3.781218\n\n\nNote that if we have level 2 covariates in our model, they are not incorperated in the intercept and slope via coef(). We have to do that by hand:\n\nnames( coefs ) = c( \"beta0.adj\", \"beta.ses\", \"beta.meanses\" )\ncoefs$id = rownames( coefs )\ncoefs = merge( coefs, sdat, by=\"id\" )\ncoefs = mutate( coefs, beta0 = beta0.adj + beta.meanses * meanses )\ncoefs$beta.meanses = NULL\n\nHere we added in the impact of mean ses to the intercept (as specified by our model). Now if we look at the intercepts (the beta0 variables) they will incorperate the level 2 covariate effects. If we then plotted a line using beta0 and beta.ses for each school, we would get the estimated lines for each school including the school-level covariate impacts."
  },
  {
    "objectID": "lmer_extract.html#standard-errors",
    "href": "lmer_extract.html#standard-errors",
    "title": "16  How to extract information from fitted lmer models",
    "section": "16.6 Standard errors",
    "text": "16.6 Standard errors\nWe can get an object with all the standard errors of the coefficients, including the individual Emperical Bayes estimates for the individual random effects. This is a lot of information. We first look at the Standard Errors for the fixed effects, and then for the random effects. Standard errors for the variance terms are not given (this is tricker to calculate).\n\n16.6.1 Fixed effect standard errors\n\nses = se.coef( M1 )\nnames( ses )\n\n[1] \"fixef\" \"id\"   \n\n\nOur fixed effect standard errors:\n\nses$fixef\n\n[1] 0.1506106 0.1218474 0.3826085\n\n\nYou can also get the uncertainty estimates of your fixed effects as a variance-covariance matrix:\n\nvcov( M1 )\n\n3 x 3 Matrix of class \"dpoMatrix\"\n             (Intercept)          ses      meanses\n(Intercept)  0.022683560 -0.001465374 -0.001619405\nses         -0.001465374  0.014846788 -0.011954182\nmeanses     -0.001619405 -0.011954182  0.146389292\n\n\nThe standard errors are the diagonal of this matrix, square-rooted. See how they line up?:\n\nsqrt( diag( vcov( M1 ) ) )\n\n(Intercept)         ses     meanses \n  0.1506106   0.1218474   0.3826085 \n\n\n\n\n16.6.2 Random effect standard errors\nOur random effect standard errors for our EB estimates:\n\nhead( ses$id )\n\n     (Intercept)       ses\n1224   0.7845859 0.5804186\n1288   0.9819216 0.6277115\n1296   0.7779963 0.5766319\n1308   1.0911690 0.6556607\n1317   0.8045695 0.6188535\n1358   0.9163545 0.6173954\n\n\nWarning: these come as a matrix, not data frame. It is probably best to do this:\n\nSEs = as.data.frame( se.coef( M1 )$id )\nhead( SEs )\n\n     (Intercept)       ses\n1224   0.7845859 0.5804186\n1288   0.9819216 0.6277115\n1296   0.7779963 0.5766319\n1308   1.0911690 0.6556607\n1317   0.8045695 0.6188535\n1358   0.9163545 0.6173954"
  },
  {
    "objectID": "lmer_extract.html#confidence-intervals-and-uncertainty",
    "href": "lmer_extract.html#confidence-intervals-and-uncertainty",
    "title": "16  How to extract information from fitted lmer models",
    "section": "16.7 Confidence intervals and uncertainty",
    "text": "16.7 Confidence intervals and uncertainty\nWe can compute profile confidence intervals (warnings have been suppressed)\n\nconfint( M1 )\n\n                 2.5 %     97.5 %\n.sig01       1.4012799  1.8897548\n.sig02      -0.8733603  0.1945989\n.sig03       0.2274189  0.9849964\n.sigma       5.9659922  6.1689341\n(Intercept) 12.3559620 12.9462385\nses          1.9512025  2.4296954\nmeanses      3.0278219  4.5329237"
  },
  {
    "objectID": "lmer_extract.html#fitted-values",
    "href": "lmer_extract.html#fitted-values",
    "title": "16  How to extract information from fitted lmer models",
    "section": "16.8 Fitted values",
    "text": "16.8 Fitted values\nFitted values are the predicted value for each individual given the model.\n\nyhat = fitted( M1 )\nhead( yhat )\n\n        1         2         3         4         5         6 \n 7.290105  9.431429  9.568109  9.249189 10.410971 10.821011 \n\n\nResiduals are the difference between predicted and observed:\n\nresids = resid( M1 )\nhead( resids )\n\n         1          2          3          4          5          6 \n-1.4141055 10.2765710 10.7808908 -0.4681887  7.4870293 -6.2380113 \n\n\nWe can also predict for hypothetical new data. Here we predict the outcome for a random student with ses of -1, 0, and 1 in a school with mean ses of 0:\n\nndat = data.frame( ses = c( -1, 0, 1 ), meanses=c(0,0,0), id = -1 )\npredict( M1, newdata=ndat, allow.new.levels=TRUE )\n\n       1        2        3 \n10.46095 12.65130 14.84165 \n\n\nThe allow.new.levels=TRUE bit says to predict for a new school (our fake school id of -1 in ndat above). In this case it assumes the new school is typical, with 0s for the random effect residuals.\nIf we predict for a current school, the random effect estimates are incorporated:\n\nndat$id = 1296\npredict( M1, newdata=ndat )\n\n        1         2         3 \n 8.509969 10.736041 12.962114"
  },
  {
    "objectID": "lmer_extract.html#appendix-the-guts-of-the-object",
    "href": "lmer_extract.html#appendix-the-guts-of-the-object",
    "title": "16  How to extract information from fitted lmer models",
    "section": "16.9 Appendix: the guts of the object",
    "text": "16.9 Appendix: the guts of the object\nWhen we fit our model and store it in a variable, R stores a lot of stuff. The following lists some other functions that pull out bits and pieces of that stuff.\nFirst, to get the model matrix (otherwise called the design matrix)\n\nmm = model.matrix( M1 )\nhead( mm )\n\n  (Intercept)    ses meanses\n1           1 -1.528  -0.428\n2           1 -0.588  -0.428\n3           1 -0.528  -0.428\n4           1 -0.668  -0.428\n5           1 -0.158  -0.428\n6           1  0.022  -0.428\n\n\nThis can be useful for predicting individual group mean outcomes, for example.\nWe can also ask questions such as number of groups, number of individuals:\n\nngrps( M1 )\n\n id \n160 \n\nnobs( M1 )\n\n[1] 7185\n\n\nWe can list all methods for the object (merMod is a more generic version of lmerMod and has a lot of methods we can use)\n\nclass( M1 )\n\n[1] \"lmerModLmerTest\"\nattr(,\"package\")\n[1] \"lmerTest\"\n\nmethods(class = \"lmerMod\")\n\n [1] coerce      coerce&lt;-    contest     contest1D   contestMD   display    \n [7] getL        mcsamp      se.coef     show        sim         standardize\nsee '?methods' for accessing help and source code\n\nmethods(class = \"merMod\")\n\n [1] anova          as.function    coef           confint        cooks.distance\n [6] deviance       df.residual    display        drop1          extractAIC    \n[11] extractDIC     family         fitted         fixef          formula       \n[16] fortify        getData        getL           getME          hatvalues     \n[21] influence      isGLMM         isLMM          isNLMM         isREML        \n[26] logLik         mcsamp         model.frame    model.matrix   ngrps         \n[31] nobs           plot           predict        print          profile       \n[36] ranef          refit          refitML        rePCA          residuals     \n[41] rstudent       se.coef        show           sigma.hat      sigma         \n[46] sim            simulate       standardize    summary        terms         \n[51] update         VarCorr        vcov           weights       \nsee '?methods' for accessing help and source code"
  },
  {
    "objectID": "fixed_effects.html#the-language-of-fixed-effects",
    "href": "fixed_effects.html#the-language-of-fixed-effects",
    "title": "17  Clarification on Fixed Effects and Identification",
    "section": "17.1 The language of “Fixed Effects”",
    "text": "17.1 The language of “Fixed Effects”\nI wanted to follow-up on a couple of things that I had written down but not sent out.\nPeople will talk about “fixed effects” in (at least) two ways. The first is when you have a dummy variable for each of your clusters, and you are using OLS regression (not multilevel modeling). In this case you are estimating a parameter for each cluster, and we refer to that collection of estimates and parameters that go with these cluster level dummy variables as “fixed effects” and the model is a “fixed effects model.” The second is when you are using multilevel modeling, such as the following:\nM0 &lt;- lmer(Y ~ 1 + var1 + var2 + var3 + (var1|id), data)\nWhen we fit the above model, we will be estimating a grand intercept, and three coefficients for the three variables. Call these \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), and \\(\\beta_3\\). We are also estimating a random intercept and random slope for var1, with each group defined by the id variable having its own random intercept and slope. These are described by a variance-covariance matrix that we have been describing with \\(\\tau_{00}, \\tau_{01}, \\tau_{11}\\).\nNow, the \\(\\beta\\) are the fixed part, or fixed effects, of the model. The \\(\\tau\\) describe the random part or random effects. This is why, in R, we say fixef(M0) to get the \\(\\beta\\). If we say ranef(M0) we get the Empirical Bayes estimates of the random parts for each cluster. If we say coef(M0) R adds all this together to give the sum of the fixed part and random part, for each cluster defined by id.\nRead Gelman and Hill 12.3 for more on this sticky language. G&H do not like “fixed effects” as a description because it is so vague."
  },
  {
    "objectID": "fixed_effects.html#underidentification",
    "href": "fixed_effects.html#underidentification",
    "title": "17  Clarification on Fixed Effects and Identification",
    "section": "17.2 Underidentification",
    "text": "17.2 Underidentification\nIf we fit a model with a dummy variable for each cluster, and a level to variable that does not vary within cluster, we say our model is “underidentified.” We say it is underidentified because no matter how much data we have, we will always have an infinite number of parameter values that can describe our model equally well. For example, say our level 2 variable is a dummy variable (e.g., sector). Then a model where we add five to the coefficient of the level 2 variable, and subtract five from all of the fixed effects for the clusters with sector=1 will fit our data just as well as one where we don’t. We can’t tell the difference! Hence we do not have enough to “identify” the parameter values."
  },
  {
    "objectID": "fixed_effects.html#further-reading",
    "href": "fixed_effects.html#further-reading",
    "title": "17  Clarification on Fixed Effects and Identification",
    "section": "17.3 Further Reading",
    "text": "17.3 Further Reading\n(Antonakis, Bastardoz, and Rönkkö 2019)\n\n\n\n\nAntonakis, John, Nicolas Bastardoz, and Mikko Rönkkö. 2019. “On Ignoring the Random Effects Assumption in Multilevel Models: Review, Critique, and Recommendations.” Organizational Research Methods 24 (2): 443–83. https://doi.org/10.1177/1094428119877457."
  },
  {
    "objectID": "interpreting_coefficients.html#interpreting-your-models-they-wont-interpret-themselves",
    "href": "interpreting_coefficients.html#interpreting-your-models-they-wont-interpret-themselves",
    "title": "18  Interpreting Coefficients",
    "section": "18.1 Interpreting your models (they won’t interpret themselves!)",
    "text": "18.1 Interpreting your models (they won’t interpret themselves!)\nSo, multilevel models sure are great, but they can also make interpretations much more challenging. You’ve done OLS regression, so you have an understanding of how to interpret regression coefficients. However, adding additional levels means that some of our interpretations also need to change. This document is intended to provide a brief guide to how to do that.\n\nCoefficients and indices at various levels of the model\nBut before we even start, we need to talk about how we use different coefficients and letters at different levels of the model. There isn’t a single convention for how to do this, but we’ll try to be consistent at least in this class.\nWe’ll distinguish between two basic types of models, those that are multilevel and not longitudinal, and those that are longitudinal.\nAs a canonical example of the first type, let’s consider the model we use in class, namely\n\\[\\begin{aligned}\nmathach_{ij} &= \\beta_{0j[i]} + \\beta_{1j[i]}SES_i + \\varepsilon_i, \\\\\n\\beta_{0j} &= \\gamma_{00} + \\gamma_{01}sector_j + u_{0j},\\\\\n\\beta_{1j} &= \\gamma_{10} + \\gamma_{11}sector_j + u_{1j},\\\\\n\\varepsilon_i &\\sim Normal(0, \\sigma^2_\\varepsilon) \\\\\n\\begin{pmatrix}\nu_{0j}\\\\\nu_{1j}\\\\\n\\end{pmatrix} &\\sim  N\n\\begin{bmatrix}\n\\begin{pmatrix}\n0\\\\\n0\n\\end{pmatrix}\\!\\!,&\n\\begin{pmatrix}\n\\sigma^2_0 & \\rho\\sigma_0\\sigma_1\\\\\n\\rho\\sigma_0\\sigma_1 & \\sigma^2_1\n\\end{pmatrix}\n\\end{bmatrix}\n\\end{aligned}\\]\nHere are the features of the model to attend to. When referring to students (or other first-level units), we will use \\(i\\) as a subscript. \\(X_i\\) will indicate a measurement taken for the \\(i\\)th student. When referring to schools (or other second-level units), we will \\(j\\) as a subscript. \\(X_j\\) will indicate a measurement taken for the \\(j\\)th school. When we expand these models to include third-level units (e.g., districts), we will use the subscript \\(k\\) for these units. I don’t intend to go past that, although we could. When we introduce cross-classified models (i.e., models will non-nested hierarchies) we’ll pick subscripts that are intended to be evocative.\nWe’ll also try to be consistent when using coefficients. We’ll use the letter \\(\\beta\\) (beta) to indicate regression coefficients measured at the first level. We’ll use the letter \\(\\gamma\\) (gamma) to indicate regression coefficients measured at the second level. Eventually we’ll use the letter \\(\\xi\\) (xi, or ksi) to indicate regression coefficients measured at the third level.\nWhen we subscript regression coefficients, we’ll need a number of subscripts equal to the level of the model at which this coefficient has been entered. The first subscript will indicate the level-1 coefficient with which this particular coefficient is associated, the second subscript will indicates the level-2 coefficient with which it is associated, and so on. This means that each coefficient will have a number of subscripts equal to the level of the model. As a really complicated example, if a coefficient is labeled as \\(\\xi_{021}\\), this indicates that the coefficient is the first slope coefficient (the 1 at the end) in a model for the second level-2 slope coefficient (the 2 in the second position) in a model for the level-1 intercept. Similarly, the first subscript in a random effect will indicate the level-1 coefficient with which it is associated, and the second will indicate the level-2 coefficient with which is is associated. Random effects will always have one fewer subscript than the coefficients at that level. As you can imagine, subscripts quickly get out of hand as we introduce more and more levels to a model.\nWe’ll use \\(\\sigma^2_p\\) to indicate the variance of the level-2 residual for the \\(p\\)th random effect (starting at 0 for the intercept). I’m not yet sure how to do the subscripting at level-3, and for now am hoping to just wing it. The correlation between the \\(p\\)th and \\(q\\)th random effects will be subscripted \\(pq\\), and correlations will always be identified with a \\(\\rho\\) (rho, not p).\nLongitudinal models are similar, except for the subscripting. I’ll always (probably) subscript the first level with \\(t\\), for time. The second level will become \\(i\\) (assuming that we’re looking at growth in students or other individuals), followed by \\(j\\) for the third level (we probably won’t include a fourth level).\n\n\nInterpreting fixed effects\nOkay, that was complicated, although I think writing down definitions and rules is often more challenging than applying them. Now let’s practice some interpretations, going back to our model.\nAt the first level, we interpret (almost) exactly as we would in a standard regression model. If we have\n\\[mathach_{ij} = \\beta_{0j[i]} + \\beta_{1j[i]}SES_i + \\varepsilon_i,\\]\nthen we interpret \\(\\beta_{0j}\\) as the predicted value of \\(mathach\\) for a student of 0 SES (which represents the grand mean) who is located in school \\(j\\). Because this is a multilevel model, different schools have different intercepts. Similarly, we can interpret \\(\\beta_{1j[i]}\\) as the expected difference in math achievement associated with a one-unit difference in SES for students in school \\(j\\). We don’t interpret it, but \\(\\varepsilon_i\\) indicates the difference between what we observed for this student and what we predicted based on her or his school and SES.\nWe interpret the level-2 units depending on the coefficients they predict. For the school-intercept we have\n\\[\\beta_{0j} = \\gamma_{00} + \\gamma_{01}sector_j + u_{0j}.\\]\nWe can interpret \\(\\gamma_{00}\\) as the predicted intercept for schools for which \\(sector = j\\) (i.e., public schools). We can interpret \\(\\gamma_{01}\\) as the predicted difference in school intercepts between Catholic and public schools. Although it’s less common, we can also interpret the residual for school \\(j\\), \\(u_{0j}\\), because you can’t tell me what to do. \\(u_{0j}\\) represents the difference between the observed/inferred intercept for school \\(j\\) and the predicted intercept.\nTurning to the model for the slope, we have\n\\[\\beta_{1j} = \\gamma_{10} + \\gamma_{11}sector_j + u_{0j}.\\]\nHere \\(\\gamma_{10}\\) is the predicted slope for SES in public schools, while \\(\\gamma_{11}\\) is the mean difference in slopes between Catholic and public school. Finally, \\(u_1j\\) is the difference between the slope observed/inferred for school \\(j\\) and the slope predicted by the model.\nWe can also interpret these coefficients at the student level. Rewrite the model by substituting \\(\\beta_{0j} = \\gamma_{00} + \\gamma_{01}sector_j + u_{0j}\\) and \\(\\beta_{1j} = \\gamma_{10} + \\gamma_{11}sector_j + u_{1j}\\) to obtain\n\\[\\begin{aligned}\nmathach_i &= \\gamma_{00} + \\gamma_{01}sector_{j[i]} + u_{0j[i]} + (\\gamma_{10} + \\gamma_{11}sector_{j[i]} + u_{1j[i]})SES_i + \\varepsilon_i \\\\\n&= \\gamma_{00} + \\gamma_{01}sector_{j[i]} + \\gamma_{10}SES_i + \\gamma_{11}sector_{j[i]}SES_i + (u_{0j[i]} + u_{1j[i]}SES_i + \\varepsilon_i).\n\\end{aligned}\\]\nNow we can interpret these coefficients as in a typical one-level linear regression model.\n\n\\(\\gamma_{00}\\) is the predicted mean value of \\(mathach\\) for students of \\(SES = 0\\) in public schools;\n\\(\\gamma_{01}\\) is the predicted difference in \\(mathach\\) between students of \\(SES = 0\\) in Catholic schools and similar peers in public schools;\n\\(\\gamma_{10}\\) is the predicted difference in \\(mathach\\) associated with a one-unit difference in SES for students in public schools; and\n\\(\\gamma_{11}\\) is the predicted difference in the above difference between students in Catholic schools and students in public schools.\n\nEither interpretation is acceptable, and you should base your decision on how you’re framing your question.\n\n\nInterpreting variance-covariance parameters\nNow we’re going to turn to the variance-covariance matrix for the random offsets, namely\n\\[\\begin{aligned}\n\\Sigma = \\begin{pmatrix}\nu_{0j}\\\\\nu_{1j}\\\\\n\\end{pmatrix} &\\sim  N\n\\begin{bmatrix}\n\\begin{pmatrix}\n0\\\\\n0\n\\end{pmatrix}\\!\\!,&\n\\begin{pmatrix}\n\\sigma^2_0 & \\rho\\sigma_0\\sigma_1\\\\\n\\rho\\sigma_0\\sigma_1 & \\sigma^2_1\n\\end{pmatrix}\n\\end{bmatrix}\n\\end{aligned}\\]\nThe variance of a random offset (e.g., \\(\\sigma_0^2\\), the variance of \\(u_{0j}\\)) represents how variable the coefficient associated with that coefficient is, conditional on the variables in the model. The correlations (e.g., \\(\\rho\\), the only correlation in this model) represent the tendency of the random offsets to covary, i.e., to be associated with each other."
  },
  {
    "objectID": "within_v_between.html#fitting-the-models",
    "href": "within_v_between.html#fitting-the-models",
    "title": "19  Within, Between, and Contextual Effects",
    "section": "19.1 Fitting the Models",
    "text": "19.1 Fitting the Models\n\nols &lt;- lm(mathach ~ ses, hsb)\nfe &lt;- lm(mathach ~ ses + factor(schoolid), hsb)\nri &lt;- lmer(mathach ~ ses + (1|schoolid), hsb)\nri_within &lt;- lmer(mathach ~ grp_center_ses + (1|schoolid), hsb)\nri_between &lt;- lmer(mathach ~ grp_mean_ses + (1|schoolid), hsb)\nre_wb &lt;- lmer(mathach ~ grp_center_ses + grp_mean_ses + (1|schoolid), hsb)\ncontextual &lt;- lmer(mathach ~ ses + grp_mean_ses + (1|schoolid), hsb)\n\ntab_model(ols, fe, ri, ri_within, ri_between, re_wb, contextual,\n          p.style = \"stars\",\n          show.ci = FALSE,\n          show.se = TRUE,\n          keep = \"ses\",\n          show.dev = TRUE,\n          dv.labels = c(\"OLS\",\n                        \"Fixed Effects\",\n                        \"Rand. Int.\",\n                        \"RI Within\",\n                        \"RI Between\",\n                        \"REWB\",\n                        \"Mundlak\"))\n\n\n\n\n \nOLS\nFixed Effects\nRand. Int.\nRI Within\nRI Between\nREWB\nMundlak\n\n\nPredictors\nEstimates\nstd. Error\nEstimates\nstd. Error\nEstimates\nstd. Error\nEstimates\nstd. Error\nEstimates\nstd. Error\nEstimates\nstd. Error\nEstimates\nstd. Error\n\n\nses\n3.18 ***\n0.10\n2.19 ***\n0.11\n2.39 ***\n0.11\n\n\n\n\n\n\n2.19 ***\n0.11\n\n\ngrp center ses\n\n\n\n\n\n\n2.19 ***\n0.11\n\n\n2.19 ***\n0.11\n\n\n\n\ngrp mean ses\n\n\n\n\n\n\n\n\n5.86 ***\n0.36\n5.87 ***\n0.36\n3.68 ***\n0.38\n\n\nRandom Effects\n\n\n\nσ2\n \n \n37.03\n37.01\n39.16\n37.02\n37.02\n\n\n\nτ00\n \n \n4.77 schoolid\n8.67 schoolid\n2.64 schoolid\n2.69 schoolid\n2.69 schoolid\n\n\nICC\n \n \n0.11\n0.19\n0.06\n0.07\n0.07\n\n\nN\n \n \n160 schoolid\n160 schoolid\n160 schoolid\n160 schoolid\n160 schoolid\n\nObservations\n7185\n7185\n7185\n7185\n7185\n7185\n7185\n\n\nR2 / R2 adjusted\n0.130 / 0.130\n0.235 / 0.218\n0.077 / 0.182\n0.044 / 0.225\n0.123 / 0.179\n0.167 / 0.224\n0.167 / 0.224\n\n\nDeviance\n295643.779\n259918.446\n46641.008\n46720.415\n46959.128\n46563.821\n46563.821\n\n\n* p&lt;0.05   ** p&lt;0.01   *** p&lt;0.001"
  },
  {
    "objectID": "within_v_between.html#interpretation",
    "href": "within_v_between.html#interpretation",
    "title": "19  Within, Between, and Contextual Effects",
    "section": "19.2 Interpretation",
    "text": "19.2 Interpretation\n\n19.2.1 OLS\nIgnoring school membership, students who are 1-unit higher in SES are predicted to score 3.18 points higher in math\n\n\n19.2.2 Fixed Effects\nHolding constant school, students who are 1-unit higher in SES are predicted to score 2.19 points higher in math\n\n\n19.2.3 Random Intercepts\nStudents who are 1-unit higher in SES are predicted to score 2.39 points higher in math; schools that are 1-unit higher in mean SES are predicted to have mean math scores 2.39 points higher.\n(A precision-weighted average of the within and between effects; Within: 2.19, Between: 5.86. If the RE assumption holds, these are the same in the population so we get more precision by averaging them together. However, in social science, they are rarely the same!)\n\n\n19.2.4 Random Intercepts, Within Effect\nHolding constant school, students who are 1-unit higher in SES are predicted to score 2.19 points higher in math. This is the same coefficient as the FE model, but in an RI framework. We have “controlled for school” manually by demeaning the SES variable.\n\n\n19.2.5 Random Intercepts, Between\nSchools that are 1-unit higher in mean SES are predicted to have mean math scores 5.86 points higher.\n\n\n19.2.6 Random Effects within and Between\nHolding constant school, students who are 1-unit higher in SES are predicted to score 2.19 points higher in math; schools that are 1-unit higher in mean SES are predicted to have mean math scores 5.86 points higher. We get the within and between effects in a single model!\n\n\n19.2.7 Contextual/Mundlak\nHolding constant school, students who are 1-unit higher in SES are predicted to score 2.19 points higher in math; holding constant student SES, a student that attends a school with 1-unit higher in mean SES are predicted to have mean math scores 3.68 points higher. The contextual effect is the difference in the within and between effects, and its significance test allows us to determine if it is necessary. Mathematically, the Mundlak model and REWB are identical, as you can see from the deviance statistics. You would choose one over the other depending on your preferred interpretation."
  },
  {
    "objectID": "within_v_between.html#further-reading",
    "href": "within_v_between.html#further-reading",
    "title": "19  Within, Between, and Contextual Effects",
    "section": "19.3 Further Reading",
    "text": "19.3 Further Reading\n(Antonakis, Bastardoz, and Rönkkö 2019)\n\n\n\n\nAntonakis, John, Nicolas Bastardoz, and Mikko Rönkkö. 2019. “On Ignoring the Random Effects Assumption in Multilevel Models: Review, Critique, and Recommendations.” Organizational Research Methods 24 (2): 443–83. https://doi.org/10.1177/1094428119877457."
  },
  {
    "objectID": "growth_models_predictors.html#tips-for-growth-models",
    "href": "growth_models_predictors.html#tips-for-growth-models",
    "title": "20  Predictors in Longitudinal Growth Models",
    "section": "20.1 Tips for growth models",
    "text": "20.1 Tips for growth models\nStart with an unconditional growth model, i.e., don’t include any level-1 or level-2 predictors. This model provides useful empirical evidence for determining a proper specification of the individual growth equation and baseline statistics for evaluating more complicated level-2 models.\nThe nature of the predictor in longitudinal analysis determines where it gets added to the model: Time-invariant predictors always go in level-2 (subject level) model Time-varying predictors can go in level-1 and/or level-2. The level of the predictor dictates which variance component it seeks to describe: Level-2 describes level-2 variances and Level-1 describes level-1 variances. Although the order in which you add these predictors (in a series of successive models) may not ultimately matter, general practice is to add level-2 (time-invariant) predictors first.\nHow to decide where to add predictors? One strategy:\n\nFirst fit an unconditional (i.e. no predictors) random intercept model. This isn’t really predictive, but we can use it as a baseline model that partitions variance into between and within-person variances. Singer & Willett (2003) call this the “unconditional means model”.\nCalculate the ICC\n\nIf most of the variance is between-persons in the random intercept (level-2), then you’ll use person-level predictors to reduce that variance (i.e., account for inter-person differences)\nIf most of the variance is within-person (level-1 residual variance), you’ll need time-level predictors to reduce that variance (i.e. account for intra-person differences)\n\n\nBecause the time-specific subscript t can only appear in the level-1 model, all time-varying predictors must appear in the level-1 individual growth model. That is, person-specific predictors that vary over time appear at level-1, not level-2. Time-invariant predictors go in level-2. Furthermore, because they are time-invariant, this means they have no within-person variation to allow for a level-2 residual; thus, the level-2 growth rate parameter corresponding to this time-invariant predictor will not have an error term (i.e. it’s assumed to be zero). Interpretation wise, this assumes the effect of a person-specific effect is constant across population members. For a time-varying predictor, however, the associated level-2 growth parameter equation would have a residual term. This allows the effect of the time-varying predictor to vary randomly across the individuals in the population.\nWith only a few measurement points per person, we often lack sufficient data to estimate many variance components. Thus, it’s suggested that we resist the temptation to automatically allow the effects of time-varying predictors to vary at level-2 unless you have a good reason, and enough data, to do so.\nSo far in class, we’ve seen person-specific variables appear in level-2 submodels as predictors for level-1 growth parameters. You might therefore think that substantive predictors must always appear at level-2, but this isn’t true!\nHow inclusion of predictors affect variance components: Generally, when we include time-invariant predictors:\n\nthe level-1 variance component, \\(\\sigma^2_e\\), remains pretty stable because time-invariant predictors can’t explain any within-person variation\nthe level-2 variance components, \\(\\tau_{00}\\) and \\(\\tau_{01}\\), will decrease if the time-invariant predictors explain some of the between-person variation in initial status or rates of change, respectively.\n\nWhen we include time-varying predictors:\n\nboth level-1 and level-2 variance components might be affected because time-varying predictors vary both within a person and between people\nwe can interpret the resulting decrease in the level-1 variance component as amount of variation in the outcome explained by the time-varying predictors; however, it isn’t meaningful to interpret subsequent changes in level-2 variance components because adding the time-varying predictor changes the meaning of the individual growth parameters, which consequently alters the meaning of the level-2 variances, so it doesn’t make sense to compare the magnitude of these level-2 variances across successive models."
  },
  {
    "objectID": "growth_models_predictors.html#additional-resources",
    "href": "growth_models_predictors.html#additional-resources",
    "title": "20  Predictors in Longitudinal Growth Models",
    "section": "20.2 Additional Resources",
    "text": "20.2 Additional Resources\n\nhttps://books.google.com/books?i d=PpnA1M8VwR8C&pg=PA168&lpg=PA168&dq=longitudinal+data+analysis+level1+level2+predictors&source=bl&ots=N4p8yFdyuL&sig=wWjmaEeqakD040s4B9-QquJF1eE&hl=en&sa=X&ved=0CCwQ6AEwAWoVChMI5ZLsjKDjyAIVzB0-Ch1s6wGV#v=onepage&q=longitudinal%20data%20analysis%20level1%20level2%20predictors&f=false\nhttp://jonathantemplin.com/files/mlm/mlm12uga/mlm12uga_section06.pdf\nhttp://www.lesahoffman.com/944/944_Lecture07_Time-Invariant.pdf"
  },
  {
    "objectID": "reg_assumptions.html#oh-assumptions",
    "href": "reg_assumptions.html#oh-assumptions",
    "title": "21  MLM Assumptions",
    "section": "21.1 Oh, assumptions",
    "text": "21.1 Oh, assumptions\nThere are generally two kinds of assumptions we should worry about the most: ommitted variable bias, and independence assumptions. The latter of these is one we should always think about.\nDo read Chapter 9 of R&B, paying attention to their examples and not so much to the mathematical formalism. It has some dense prose, but then moves to specific diagnostics that make what they are talking about much more clear (and it also provides things you can do to check assumptions). The MLM in Plain Language text has some simpler explanations. Also see below for some further notes."
  },
  {
    "objectID": "reg_assumptions.html#ommitted-variable-bias",
    "href": "reg_assumptions.html#ommitted-variable-bias",
    "title": "21  MLM Assumptions",
    "section": "21.2 Ommitted variable bias",
    "text": "21.2 Ommitted variable bias\nConsider the following numerical example:\n\nN = 100\ndat = data.frame( X1 = rnorm( N ) )\ndat = mutate( dat, \n              X2 = X1 + rnorm( N ),\n              Y = 3 + 0.5 * X1 + 1.5 * X2 + rnorm( N ) )\n\nThe above makes an X2 that is correlated with X1, and a Y that is a function of both. The true model here is \\[ Y_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\epsilon_{i} \\] with coefficents \\(\\beta = (3, 0.5, 1.5)\\).\nWe fit two models, one with both covariates, and one with only one:\n\nM0 = lm( Y ~ 1 + X1 + X2 , data = dat )\nM1 = lm( Y ~ 1 + X1, data = dat )\n\nOur results:\n\ntab_model(M0, M1, p.style = \"stars\",\n          show.ci = FALSE, show.se = TRUE)\n\n\n\n\n \nY\nY\n\n\nPredictors\nEstimates\nstd. Error\nEstimates\nstd. Error\n\n\n(Intercept)\n3.10 ***\n0.11\n2.95 ***\n0.18\n\n\nX1\n0.44 **\n0.15\n1.92 ***\n0.16\n\n\nX2\n1.51 ***\n0.11\n\n\n\n\nObservations\n100\n100\n\n\nR2 / R2 adjusted\n0.856 / 0.853\n0.584 / 0.580\n\n\n* p&lt;0.05   ** p&lt;0.01   *** p&lt;0.001\n\n\n\n\n\n\n\nNote our coefficient is completely wrong when we omit a correlated variable. This is omitted variable bias, and in terms of our assumptions we are in a circumstance where the true residuals in our model are not centered around 0 for all values of X1, since they include the X2 effect which is correlated with X1. We can see this as follows:\n\ndat = mutate( dat, e = Y - 3 - 0.5 * X1 )\nggplot( dat, aes( X1, e ) ) +\n    geom_point() +\n    geom_hline( yintercept = 0 )\n\n\n\n\nNote how our residuals (which includes X2) are positive for bigger X1, due to the correlation of X1 and X2.\nConclusion: On one hand, we have the wrong estimate for \\(\\beta_1\\). On the other, the estimate we do get is fine if we view it as the best description of the data. We just need to remember that the interpretation of our coefficient includes the confounding effect of X2 on X1.\nIn this vein:\nQ: You mentioned during class that we don’t care too much about assumptions when looking at trends in the data. However, if we are trying to draw causal inferences, do these assumptions become more important?\nA: Even with the assumptions causal inference would depend on the model. Modern causal inference has a hard time with this, so you need other strategies such as quasiexperimental design. Hence my focus on descriptive aspects of data analysis."
  },
  {
    "objectID": "reg_assumptions.html#independence-assumptions",
    "href": "reg_assumptions.html#independence-assumptions",
    "title": "21  MLM Assumptions",
    "section": "21.3 Independence assumptions",
    "text": "21.3 Independence assumptions\nThe independence assumptions are key. When we do not take violations of independence into account, we can be overly confident of our estimates.\nGenerally with MLM we should think of these assumptions in terms of how we sampled our data. If we sampled our data by sampling a collection of schools, and then individuals within those schools, then we have two levels. We then need to ask two questions:\n\nWere the schools sampled independently?\nWere the students sampled independently within the schools?\n\nIf yes to both, we have met both our independence assumptions! We have met them even if the students are clustered in classes within their schools. As long as we did not sample using those classes (or other clusters), we are ok as our sample of students will be representative of the school they are in.\nOne might then ask, more generally, if there is a problem with clustering if it’s not part of the sampling plan? E.g. if you sampled at the school level and surveyed all students, there is still natural clustering in classrooms: is that a problem? What about unobserved clustering like families, neighborhoods, etc. which are not part of sampling, but do exist naturally in populations?\nE.g., see this document which says clustered SEs are not necessary (in OLS) unless sampling was conducted at the cluster-level and that econometricians often overuse them.\nThis is indeed correct. That being said, we might want to make clusters to investigate how things vary across those clusters.\nQ: More generally, is there a problem with clustering if it’s not part of the sampling plan? E.g. if you sampled at the school level and surveyed all students, there is still natural clustering in classrooms: is that a problem? What about unobserved clustering like families, neighborhoods, etc. which are not part of sampling, but do exist naturally in populations? Q: I’ll let Luke respond to how this affects the assumptions later on Piazza. My intuition is that we want to cluster/add levels at these different levels if we believe the outcomes or predictors are correlated Q: Thanks. I remember reading this https://blogs.worldbank.org/impactevaluations/when-should-you-cluster-standard-errors-new-wisdom-econometrics-oracle a few years ago, which says clustered SEs are not necessary (in OLS) unless sampling was conducted at the cluster-level and that econometricians often overuse them, but I am wondering to what degree this holds in MLMs.\nA: This is correct. That being said, we might want to make clusters to investigate how things vary across those clusters."
  },
  {
    "objectID": "reg_assumptions.html#number-of-clusters-needed",
    "href": "reg_assumptions.html#number-of-clusters-needed",
    "title": "21  MLM Assumptions",
    "section": "21.4 Number of clusters needed?",
    "text": "21.4 Number of clusters needed?\nOk, this isn’t an assumption per se, but onwards!\nQ: Why should you worry if the number of group is small? (referencing the recap slide)\nA: With few clusters, estimation is hard just like having a small dataset with OLS. The variance parameters in particular are difficult.\nQ: When you say “at least 20” you mean for the number of j’s, right?\nA: Yes, number of clusters. Mostly Harmless Econometrics readers might recall a discussion of 42 clusters (8.2.3), which contributes to this debate of the appropriate level of j-units"
  },
  {
    "objectID": "reg_assumptions.html#testing-assumptions",
    "href": "reg_assumptions.html#testing-assumptions",
    "title": "21  MLM Assumptions",
    "section": "21.5 Testing assumptions",
    "text": "21.5 Testing assumptions\nQ: How do we test these assumptions?\nA: Often with plots, like with classic OLS. for example we can pot a histogram of the residuals and see if they are normally distributed."
  },
  {
    "objectID": "model_cheat_sheet.html#the-mathematical-model",
    "href": "model_cheat_sheet.html#the-mathematical-model",
    "title": "22  Connecting the three dots: An HSB Model",
    "section": "22.1 The mathematical model",
    "text": "22.1 The mathematical model\nLevel 1 models: \\[\n\\begin{aligned}\ny_{ij} &= \\beta_{0j} + \\beta_{1j} ses_{ij} + \\beta_2 female_{ij} +  \\epsilon_{ij} \\\\\n\\epsilon_{ij} &\\sim N( 0, \\sigma^2_y ) \\\\\n\\end{aligned}\n\\]\nLevel 2 models: \\[\n\\begin{aligned}\n\\beta_{0j} &= \\gamma_{00} + \\gamma_{01} sector_j + \\gamma_{02} meanSES_j + u_{0j} \\\\\n\\beta_{1j} &= \\gamma_{10} + \\gamma_{11} sector_j + u_{1j}\n\\end{aligned}\n\\] with \\[\n\\begin{pmatrix} u_{0j} \\\\\nu_{1j}\n\\end{pmatrix} \\sim  N\n\\begin{bmatrix}\n\\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix}\\!\\!,&\n\\begin{pmatrix}\n\\tau_{00} & \\tau_{01}\\\\\n& \\tau_{11} \\\\\n\\end{pmatrix}\n\\end{bmatrix} .\n\\]\nThe \\(\\tau_{01}\\) is the covariance of the random intercept and random slope. We usually look at the correlation of\n\\[ \\rho = \\frac{ \\tau_{01} }{ \\sqrt{ \\tau_{00} \\tau_{11} } } . \\]\nThe estimated \\(\\rho\\) is what R gives us in the printed output, rather than \\(\\tau_{01}\\).\nThe derivation of the reduced form is:\n\\[\n\\begin{aligned}\ny_{ij} &= \\beta_{0j} + \\beta_{1j} ses_{ij} + \\epsilon_{ij}\\\\\n&= \\left( \\gamma_{00} + \\gamma_{01} sector_j + \\gamma_{02} meanSES_j + u_{0j} \\right)+ (\\gamma_{10} + \\gamma_{11} sector_j + u_{1j}) ses_{ij} + \\beta_2 female_{ij} + \\epsilon_{ij} \\\\\n&= \\gamma_{00} + \\gamma_{01} sector_j + \\gamma_{02} meanSES_j  + u_{0j}  + \\gamma_{10}ses_{ij} + \\gamma_{11} sector_j ses_{ij} + u_{1j} ses_{ij} + \\beta_2 female_{ij} +  \\epsilon_{ij}  \\\\\n&= \\gamma_{00} + \\gamma_{01} sector_j + \\gamma_{02} meanSES_j +  \\gamma_{10}ses_{ij} + \\gamma_{11} sector_j ses_{ij} + \\beta_2 female_{ij}  + \\left(u_{0j} + u_{1j} ses_{ij} + \\epsilon_{ij} \\right)\n\\end{aligned}\n\\] This formula is what we will give to lmer() in R’s formula notation."
  },
  {
    "objectID": "model_cheat_sheet.html#the-lmer-code",
    "href": "model_cheat_sheet.html#the-lmer-code",
    "title": "22  Connecting the three dots: An HSB Model",
    "section": "22.2 The lmer code",
    "text": "22.2 The lmer code\n\nM1 = lmer( mathach ~ 1 + female + ses*sector + \n             meanses + (1+ses|id),\n           data = dat )\n\nThis code is the exact same model, using the fact that ses*sector means ses + sector + ses:sector. I.e., the above is exactly the same as this more explicitly written R code:\n\nM1 = lmer( mathach ~ 1 + sector + meanses + ses + sector:ses + female + (1+ses|id),\n           data = dat )\n\nEach term in the expanded formula corresponds to a math symbol in the mathematical model. The (1+ses|id) make our random effects, and tie to all the \\(\\tau\\) terms. The residual variance \\(\\sigma^2_y\\) is the only parameter not explicitly listed in the above model."
  },
  {
    "objectID": "model_cheat_sheet.html#the-output",
    "href": "model_cheat_sheet.html#the-output",
    "title": "22  Connecting the three dots: An HSB Model",
    "section": "22.3 The output",
    "text": "22.3 The output\n\ndisplay( M1 )\n\nlmer(formula = mathach ~ 1 + sector + meanses + ses + sector:ses + \n    female + (1 + ses | id), data = dat)\n            coef.est coef.se\n(Intercept) 12.79     0.21  \nsector       1.29     0.29  \nmeanses      3.04     0.37  \nses          2.73     0.14  \nfemale      -1.18     0.16  \nsector:ses  -1.31     0.21  \n\nError terms:\n Groups   Name        Std.Dev. Corr \n id       (Intercept) 1.45          \n          ses         0.18     0.65 \n Residual             6.05          \n---\nnumber of obs: 7185, groups: id, 160\nAIC = 46482.9, DIC = 46445.1\ndeviance = 46454.0 \n\n\nNow, using this output, we have estimates for all our mathematical modeling parameters:\n\n\\(\\gamma_{00} = 12.79\\) - The overall average math achievement for a student with 0 ses in a public school with 0 mean SES.\n\\(\\gamma_{01} = 1.29\\) - The average difference between otherwise equivilent catholic and public schools.\n\\(\\gamma_{02} = 3.04\\) - The impact on average achievement due to mean SES of schools. Higher SES schools have higher achievement.\n\\(\\gamma_{10} = 2.73\\) - The average slope of ses vs. math achievement in public schools.\n\\(\\beta_2 = -1.18\\) - The gender gap; girls have lower math scores on average.\n\\(\\gamma_{11} = -1.31\\) - The difference in slope between public and catholic schools (catholic schools have flatter slopes).\n\\(\\tau_{00} = 1.45^2\\) - Variation in overall intercept of schools (within category of public or catholic, and beyond mean SES).\n\\(\\tau_{11} = 0.18^2\\) - The variation in the random slopes for ses vs. math achievement.\n\\(\\rho = 0.65\\) - The random intercepts are correlated with random slopes. High achievement schools have more discrepancy between low and high ses students.\n\\(\\sigma_y = 6.05\\) - The unexplained student variation within school."
  },
  {
    "objectID": "model_representations.html#the-scenario",
    "href": "model_representations.html#the-scenario",
    "title": "23  Model Representations",
    "section": "23.1 The Scenario",
    "text": "23.1 The Scenario\nWe have a collection of schools that we have randomized into treatment and control conditions. The treatment condition is a novel reading program and the control condition is business as usual. We hope that the treatment accomplishes two things: raising reading level overall, and reducing the gap in reading level between “at-risk” kids and not at risk kids (we assume we have a at-risk status as a dummy variable, measured for all kids and treatment and control prior to treatment)."
  },
  {
    "objectID": "model_representations.html#the-two-level-random-intercept-model",
    "href": "model_representations.html#the-two-level-random-intercept-model",
    "title": "23  Model Representations",
    "section": "23.2 The Two-Level Random Intercept Model",
    "text": "23.2 The Two-Level Random Intercept Model\nWe will use the “double-indexing” that is the most common notation for multilevel models (not the Gelman and Hill bracket (\\(j[i]\\)) notation). Treatment is at the school level: let \\(Z_j\\) be an indicator of whether school \\(j\\) was treated (so a 0/1 variable). Then, for student \\(i\\) in school \\(j\\) we have \\[\\begin{aligned}\nY_{ij} &= \\alpha_{j} + \\beta_{1} R_{ij} + \\beta_{2} X_{ij} + \\epsilon_{ij} \\\\\n\\alpha_{j} &= \\gamma_{0} + \\gamma_{1} Z_{j} + \\gamma_{2} S_{j} + u_{j} \\\\\n\\end{aligned}\\] with \\(Y_{ij}\\) being the reading level of the student, \\(R_{ij}\\) being a dummy variable of student’s “at risk” status, \\(X_{ij}\\) being an important student demographic variable (e.g., prior reading level), and \\(S_j\\) being a school-level covariate (such as a school quality measure).\nThis is the two-level model. Level 1 is the first equation with the distribution on the residuals of \\(\\epsilon_{ij} \\sim N( 0, \\sigma^2 )\\). Level 2 is the second equation with a distribution of random effects of \\[u_{j} \\sim N( 0, \\sigma^2_\\alpha ) .\\] The \\(\\sigma^2_\\alpha\\) is the variance of the random intercept.\nCall the \\(\\beta_{0j}\\) the random intercept and \\(u_{j}\\) the random effect. The \\(u_{j}\\) is the residual of the level 2 model,. In R, we would say coef() for the intercept (including the mean \\(\\gamma_0\\)) and ranef() for the random effect. In math, coef() gives \\(\\gamma_0 + u_j\\) and ranef() gives only \\(u_j\\). Neither include the \\(\\gamma_1\\) or \\(\\gamma_2\\); these will be separate columns you get from coef().\n\n23.2.0.0.1 Remarks.\nWe have completely pooled the coefficient for \\(R_{ij}\\) and \\(X_{ij}\\): we are assuming all the schools have the same relationship between the outcome and these covariates.\nThe intercept \\(\\alpha_{j}\\) is the expected (predicted average) outcome of a not-at-risk student with \\(X_{ij} = 0\\). Different schools have different means. In particular, treatment schools have a mean of \\(\\gamma_1\\) more than control; this is the treatment impact.\n\n\n23.2.1 The Reduced Form Model\nIf we plug in our 2nd level into the first we get the following: \\[\\begin{aligned}\nY_{ij} &= \\beta_{0j} + \\beta_{1} R_{ij} + \\beta_{2} X_{ij} + \\epsilon_{ij} \\\\\n&= (\\gamma_{0} + \\gamma_{1} Z_{j} + \\gamma_{2} S_{j} + u_{j}) + \\beta_1 R_{ij} + \\beta_{2} X_{ij} + \\epsilon_{ij} \\\\\n&= \\gamma_{0} + \\gamma_{1} Z_{j} + \\gamma_{2} S_{j} + \\beta_{1} R_{ij} + \\beta_{2} X_{ij} + (u_{j} + \\epsilon_{ij})\n\\end{aligned}\\] The \\(u_{0j} + \\epsilon_{ij}\\) is our total random error. It is how much our prediction of a new, unknown student, would differ from their actual score if we didn’t know the school’s random effect. The rest of the model is the mean model or structural portion of the model.\nThis is also called the reduced form; it is what econometricians work with. They will write the entire residual as \\(\\varepsilon_{ij}\\), however: \\[\\begin{aligned}\nY_{ij} &= \\gamma_{0} + \\gamma_{1} Z_{j} + \\gamma_{2} S_{j} + \\beta_{1} R_{ij} + \\beta_{2} X_{ij} + \\varepsilon_{ij}\n\\end{aligned}\\]\n\n23.2.1.0.1 Remarks.\nThe reduced form helps us see our treatment effect more clearly. It is a shift in outcome of \\(\\gamma_1\\) for treated students.\nThe \\(\\gamma_{0}\\) is the overall mean reading level for students with \\(X_{ij}=0\\) for not-at-risk students (\\(R_{ij}=0\\)) in control schools with \\(S_j = 0\\).\nNote how we subscript school-level covariates with only a \\(j\\) vs. individual-level covariates get an \\(ij\\). If you want, you can index everything by \\(ij\\); the fact that \\(S_{ij}\\) will then be the same for all students \\(i\\) in school \\(j\\) is hidden in the data. But it does make it look very much like OLS with a weird error term: \\[Y_{ij} &= \\gamma_{0} + \\gamma_{1} Z_{ij} + \\gamma_{2} S_{ij} + \\beta_{1} R_{ij} + \\beta_{2} X_{ij} + (u_{j} + \\epsilon_{ij})\\]\nFinally, you might call all the different pieces by different letters to indicate whether you care about them or not. E.g., \\[Y_{ij} = \\mu + \\tau Z_{ij}  + \\beta_1 S_{ij} + \\beta_2 R_{ij} + \\beta_3 X_{ij} + (u_{0j} +  \\epsilon_{ij}) .\\] Here \\(\\tau\\) is our treatment effects of interest. The \\(\\beta\\)’s are just adjustments to be ignored. The \\(\\mu\\) is the grand mean (for those not treated, with \\(S_{ij} = 0\\) and \\(R_{ij} = 0\\) and \\(X_{ij} = 0\\)). People often use \\(\\mu\\) for mean and \\(\\tau\\) for treatment.\n\n\n\n23.2.2 Fitting it in lmer\nWe fit it as:\n    lmer( Y ~ R + Z + X + S + (1|id), data=dat )"
  },
  {
    "objectID": "model_representations.html#the-two-level-random-slopes-model",
    "href": "model_representations.html#the-two-level-random-slopes-model",
    "title": "23  Model Representations",
    "section": "23.3 The Two-Level Random Slopes Model",
    "text": "23.3 The Two-Level Random Slopes Model\nNow let’s get very complex to really unpack notational stuff. We are going to let treatment not only impact the average outcome in schools, but also allow treatment to differentially impact students who are “at risk”. I.e., we are going to have two treatment impacts, one for not at risk, and one for at risk. This is an interaction of risk status and treatment.\nFurthermore, we are going to let different schools have different gaps between at risk and not at risk, but allowing a random effect for the at risk coefficient.\nUsing our same variables as above, we have, for student \\(i\\) in school \\(j\\) \\[\\begin{aligned}\nY_{ij} &= \\beta_{0j} + \\beta_{1j} R_{ij} + \\beta_{2} X_{ij} + \\epsilon_{ij} \\\\\n\\beta_{0j} &= \\gamma_{00} + \\gamma_{01} Z_{j} + \\gamma_{02} S_{j} + u_{0j} \\\\\n\\beta_{1j} &= \\gamma_{10} + \\gamma_{11} Z_{j} + u_{1j} .\n\\end{aligned}\\]\nThis is the two-level model. Level 1 is the first equation with the distribution on the residuals of \\(\\epsilon_{ij} \\sim N( 0, \\sigma^2 )\\). Level 2 are the second and third equations, and the distribution of random effects of \\[\\begin{pmatrix} u_{0j} \\\\\nu_{1j}\n\\end{pmatrix} \\sim  N\n\\begin{bmatrix}\n\\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix}\\!\\!,&\n\\begin{pmatrix}\n\\tau_{00} & \\tau_{10} \\\\\n\\tau_{10} & \\tau_{11} \\\\\n\\end{pmatrix}\n\\end{bmatrix}\\\\[2\\jot]\\] The \\(\\tau_{00}\\) is the variance of the random intercept. \\(\\tau_{11}\\) is the variance of the random slope. \\(\\tau_{10}\\) is the covariance (not correlation) of the random effects. To get the correlation of random effects we have \\(\\rho = \\tau_{10} / \\sqrt{ \\tau_{00} } \\sqrt{ \\tau_{11} }\\). (Note that \\(\\tau_{10} = \\tau_{01}\\), meaning the covariance of A and B is the same as covariance of B and A, so we just write one of them.)\nCall the \\(\\beta_{0j}\\) the random intercept and \\(\\beta_{1j}\\) a random coefficient. We might call them both random coefficients. Call the \\(u_{0j}, u_{1j}\\), which are the residuals of the level 2 models, the random effects. In R, we would say coef() for the coefficients (including the means) and ranef() for the random effects.\n\n23.3.1 Remarks.\nWe have completely pooled the coefficient for \\(X_{ij}\\): we are assuming all the schools have the same relationship between the outcome and \\(X_{ij}\\). This is why we have no level 2 equation for \\(\\beta_{2}\\) and we do not index \\(\\beta_2\\) as \\(\\beta_{2j}\\).\nThe intercept \\(\\beta_{0j}\\) is the expected (predicted average) outcome of a not-at-risk student with \\(X_{ij} = 0\\). Different schools have different means.\nThe achievement gap of at-risk and not-at-risk students for control schools is measured by \\(\\gamma_{10}\\). For treatment schools it is \\(\\gamma_{10} + \\gamma_{11}\\).\nThe \\(\\gamma_{01}\\) is the average treatment effect for not-at-risk students. Then \\(\\gamma_{01} + \\gamma_{11}\\) is the average treatment effect for the at-risk students. If we find \\(\\gamma_{11} \\neq 0\\) then the average effects differ for our two types of students, and the change in the achievement gap induced by treatment is measured by \\(\\gamma_{11}\\).\nIn this model, the school-level covariate explains overall differences in reading between schools, but does not relate to the size of treatment impact in a school, or relate to the at-risk vs. not-at-risk achievement gap.\n\n\n23.3.2 The level 2 covariate matrix.\nSometimes people like to write the correlation matrix using other parameterizations. E.g., we might see \\[\\begin{pmatrix} u_{0j} \\\\\nu_{1j}\n\\end{pmatrix} \\sim  N\n\\begin{bmatrix}\n\\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix}\\!\\!,&\n\\begin{pmatrix}\n\\sigma^2_\\alpha & \\rho \\sigma_\\alpha \\sigma_R \\\\\n& \\sigma^2_R \\\\\n\\end{pmatrix}\n\\end{bmatrix}\\\\[2\\jot]\\] to indicate the cross-school variation in the intercept (\\(\\alpha\\)) and the risk gap (\\(R\\)). Now we specifically have written our correlation of random effects as \\(\\rho\\).\n\n\n23.3.3 The Reduced Form Model\nIf we plug in our 2nd level into the first we have to plug in both equations. If we do we get... a mess: \\[\\begin{aligned}\nY_{ij} &= \\beta_{0j} + \\beta_{1j} R_{ij} + \\beta_{2} X_{ij} + \\epsilon_{ij} \\\\\n&= (\\gamma_{00} + \\gamma_{01} Z_{j} + \\gamma_{02} S_{j} + u_{0j}) + (\\gamma_{10} + \\gamma_{11} Z_{j} + u_{1j}) R_{ij} + \\beta_{2} X_{ij} + \\epsilon_{ij} \\\\\n&= \\gamma_{00} + \\gamma_{01} Z_{j} + \\gamma_{02} S_{j} + u_{0j} + \\gamma_{10} R_{ij} + \\gamma_{11} Z_{j} R_{ij} + u_{1j} R_{ij} + \\beta_{2} X_{ij} + \\epsilon_{ij} \\\\\n&= \\gamma_{00} + \\gamma_{01} Z_{j} + \\gamma_{02} S_{j} + \\gamma_{10} R_{ij} + \\gamma_{11} Z_{j} R_{ij} + \\beta_{2} X_{ij} + u_{0j} +  u_{1j} R_{ij} + \\epsilon_{ij} \\\\\n&= \\gamma_{00} + (\\gamma_{01} + \\gamma_{11} R_{ij} ) Z_{j}  + \\gamma_{02} S_{j} + \\gamma_{10} R_{ij} + \\beta_{2} X_{ij} + (u_{0j} +  u_{1j} R_{ij} + \\epsilon_{ij}) \\\\\n\\end{aligned}\\] The \\(u_{0j} + u_{1j} R_{ij} + \\epsilon_{ij}\\) is our total random error. It is how much our prediction of a new, unknown student, would differ from their actual score if we didn’t know the school’s random effect. The rest of the model is the mean model or structural portion of the model.\nThis is our reduced form; it is what econometricians work with. They will write the entire residual as \\(\\varepsilon_{ij}\\), however: \\[\\begin{aligned}\nY_{ij} &= \\gamma_{00} + (\\gamma_{01} + \\gamma_{11} R_{ij} ) Z_{j}  + \\gamma_{02} S_{j} + \\gamma_{10} R_{ij} + \\beta_{2} X_{ij} + \\varepsilon_{ij} \\\\\n\\end{aligned}\\]\n\n23.3.3.0.1 Remarks.\nThe reduced form helps us see our treatment effects and treatment variation across groups more clearly. We can put both terms involving the treatment indicator in parenthesis (final line above) to show how treatment is different by \\(\\gamma_{11}\\) for the at-risk students.\nWe also see that the difference in treatment effects between at-risk and not at-risk is an interaction between student risk and treatment assignment of the school (note the \\(Z_j R_{ij}\\) term).\nThe \\(\\gamma_{00}\\) is the overall mean reading level for students with \\(X_{ij}=0\\) for not-at-risk students in control schools. The \\(\\gamma_{10}\\) is the average difference between at-risk and not-at-risk students in control schools, across all schools.\nWe can rearrange our equations above to get \\[Y_{ij} = ( \\gamma_{00} + u_{0j}) + (\\gamma_{01} + \\gamma_{11} R_{ij} + u_{1j} ) Z_{j} + \\gamma_{02} S_{j} + \\gamma_{10} R_{ij} + \\beta_{2} X_{ij} + \\epsilon_{ij} .\\] This shows the random intercept and random slope all bundled up.\nAs with the intercept model, you might call all the different pieces by different letters to indicate whether you care about them or not. E.g., \\[Y_{ij} = \\mu + \\tau Z_{ij}  + \\beta_1 S_{ij} + \\beta_2 R_{ij} + \\beta_3 X_{ij} + \\tau_{R} Z_{ij} R_{ij}  + (u_{0j} +  u_{1j} R_{ij} + \\epsilon_{ij}) .\\] Here \\(\\tau\\) and \\(\\tau_R\\) are our treatment effects of interest. The \\(\\beta\\)’s are just adjustments to be ignored. The \\(\\mu\\) is the grand mean. This model is the same as above, we are just changing names around.\n\n\n\n23.3.4 Fitting it in lmer\nWe fit it as:\n    lmer( Y ~ R * Z + X + S + (R|id), data=dat )\nTwo other ways of saying the same thing:\n    lmer( Y ~ 1 + R * Z + X + S + (1 + R|id), data=dat )\nand\n    lmer( Y ~ 1 + Z + S + R + Z:R + X + (1 + R|id), data=dat )\n\n\n23.3.5 Remarks.\nSee how the reduced form and lmer() align, especially if we write out what R automatically does with R * Z (R will expand R*Z into R + Z + R:Z automatically).\n\n\n23.3.6 Another form for the two-level model\nThe above is the “double-subscript” way of writing a model. By contrast, Gelman and Hill index with a nifty “bracket notation.” First, let \\(j[i]\\) indicate the school student \\(i\\) is attending. Then we have: \\[\\begin{aligned}\nY_{i} &= \\beta_{0j[i]} + \\beta_{1j[i]} R_{i} + \\beta_{2} X_{i} + \\epsilon_{i} \\\\\n\\beta_{0j} &= \\gamma_{00} + \\gamma_{01} Z_{j} + \\gamma_{02} S_{j} + u_{0j} \\\\\n\\beta_{1j} &= \\gamma_{10} + \\gamma_{11} Z_{j} + u_{1j},\n\\end{aligned}\\] This is basically identical to the above, but if you are not familiar with the bracketing then things can get messy.\nThe advantage of this is we can then imagine each student gets their own unique id, \\(i\\), and then we can query where that student is via \\(j[i]\\). This can be useful when looking at crossed effects models, where units have different random effects for different things (e.g., for a test we might have observation \\(k\\) corresponding to a single answer for a test question, with \\(i[k]\\) being the student who answered it and \\(q[k]\\) being the question item)."
  },
  {
    "objectID": "interpreting_glms.html#poisson-regression-models",
    "href": "interpreting_glms.html#poisson-regression-models",
    "title": "24  Interpreting GLMs",
    "section": "24.1 Poisson regression models",
    "text": "24.1 Poisson regression models\nPoisson regression is sometimes used to model count data. The canonical form of a Poisson (log-linear) regression model is \\[\\log(E[Y|X]) = \\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p\\] \\[Y \\sim Poisson(E[Y|X])\\]\nThe Poisson distribution has only one parameter, the mean, which is also the variance of the distribution. So in estimating \\(E[Y|X]\\), we are also estimating \\(Var(Y|X)\\). This is a potential drawback to the Poisson model, because there is no variance parameter to estimate, and so incorrect models can give wildly inaccurate standard errors (frequently unrealistically small). A better model is a quasi-Poisson model, for which the variance is proportional to the mean, but not necessarily equal to it. The negative binomial regression model is also commonly used to address over-dispersed count data where the variance exceeds the mean.\nThe canonical link function for Poisson outcomes is the natural logarithm. When we use a log-link, we can write\n\\[E[Y|X] = e^{\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p}.\\]\nWe can interpret \\(\\beta_0\\) as follows: for observations which are 0 on all of the predictors, we estimate that the mean (expected) value of the outcome will be \\(e^{\\beta_0}\\).\nWe can interpret \\(\\beta_1\\) as follows: adjusting for the other predictors, a one-unit difference in \\(X_1\\) predicts a \\((e^{\\beta_1}-1)\\times100 \\%\\) difference in the outcome.\nGenerally, when using a log-link, we assume that differences in the predictors are associated with multiplicative differences in the outcome.\nSome advantages to using an exponential link are\n\nthe model is mathematically more tractable and simpler to fit\nthe model parameters are easy to interpret\nthe mean of \\(Y\\) is guaranteed to be positive for all values of \\(X\\), which is required by the Poisson distribution\n\nWe can fit a Poisson log-linear regression by writing\nglm(Y \\(\\sim\\) X, family = poisson(link = ‘log’))\nTo fit a quasi-Poisson model, write\nglm(Y \\(\\sim\\) X, family = quasipoisson(link = ‘log’))\nTo fit a negative binomial regression model, write (after loading the MASS library)\nglm.nb(Y \\(\\sim\\) X, link=‘log’)\nTo fit a Poisson regression with an identity link (where coefficients are interpreted as expected differences in the outcome associated with unit differences in the predictor), write\nglm(Y \\(\\sim\\) X, family = poisson(link = ‘identity’))\nTo fit a Poisson regression with a square root link, which is vaguely like a compromise between an identity link and a log link (and is harder to interpret than either), write\nglm(Y \\(\\sim\\) X, family = poisson(link = ‘sqrt’))\nTo fit a Poisson log-linear model with a random intercept and slope, write\nglmer(Y \\(\\sim\\) X + (X|grp), family = poisson(link = ‘log’))"
  },
  {
    "objectID": "interpreting_glms.html#dichotomous-regression-models",
    "href": "interpreting_glms.html#dichotomous-regression-models",
    "title": "24  Interpreting GLMs",
    "section": "24.2 Dichotomous regression models",
    "text": "24.2 Dichotomous regression models\nWhen predicting either successes and failures, or proportions, we can use a model with a binomial outcome. Here we’ll focus on models where the data is represented as individual successes and failures. The canonical model for these data is logistic regression, where\n\\[logit(E[Y|X]) \\equiv \\log\\left(\\frac{P(Y=1|X)}{1-P(Y=1|X)}\\right) = \\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p\\] \\[Y \\sim Binomial(1, E[Y|X])\\]\nWe can rewrite this model as\n\\[odds(Y) = \\frac{P(Y=1|X)}{1-P(Y=1|X)} = e^{\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p}\\]\nor\n\\[P(Y=1|X) = \\frac{e^{\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p}}{1 + e^{\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p}}\\]\nWe can interpret \\(\\beta_0\\) as follows: for observations which are 0 on all of the predictors, we estimate that the mean value of the outcome will be \\(\\frac{e^{\\beta_0}}{1 + e^{\\beta_0}}\\). That is, we estimate that the probability of the outcome being a ‘success’ (assuming ‘success’ is coded as a 1) will be \\(\\frac{e^{\\beta_0}}{1 + e^{\\beta_0}}\\).\nWe can interpret \\(\\beta_1\\) as follows: adjusting for the other predictors, a one-unit difference in \\(X_1\\) predicts a \\(\\beta_1\\) difference in the log-odds of the outcome being one, or a \\((e^{\\beta_1}-1)\\times100\\%\\) difference in the odds of the outcome. Unfortunately, the change in probability of a unit change depends on where the starting point is, so there is no easy way to interpret these coefficients in terms of direct probability. One can calculate the estimated change for specific units, however, and look at the distribution of those changes.\nOther possible link functions include the probit (which uses a Normal CDF to link \\(\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p\\) to \\(P(Y=1|X)\\)), or the complementary log-log (which allows \\(P(Y = 1|X)\\) to be asymmetric in the predictors), among others.\n\n24.2.1 Example\n\n\n24.2.2 How to fit a GLM\nWe can fit a logistic regression model by writing\nglm(Y \\(\\sim\\) X, family = binomial(link = ‘logit’))\nWe can fit a probit regression model by writing\nglm(Y \\(\\sim\\) X, family = binomial(link = ‘probit’))\nWe can fit a complementary log-log model by writing\nglm(Y \\(\\sim\\) X, family = binomial(link = ‘cloglog’))\nWe can allow a random slope and intercept by writing\nglmer(Y \\(\\sim\\) X + (X|grp), family = binomial(link = ‘logit’))"
  },
  {
    "objectID": "glm_vs_transform.html#making-and-graphing-the-data",
    "href": "glm_vs_transform.html#making-and-graphing-the-data",
    "title": "25  GLMs vs. Transformations",
    "section": "25.1 Making and Graphing the Data",
    "text": "25.1 Making and Graphing the Data\nLet’s start by making some fake data. Here’s the data-generating function, which suggests that a 1-unit increase in x will increase the expected count by \\(e^.5 = 1.65\\).\n\\[\ny = Poisson(e^{0.5x})\n\\]\nIn the graph, we can see that the relationship between x and y is clearly non linear!\n\nlibrary(tidyverse)\nlibrary(sjPlot)\nlibrary(ggeffects)\n\ntheme_set(theme_classic())\n\nrm(list = ls())\n\ndat &lt;- tibble(\n  x = runif(1000, 0, 5),\n  y = rpois(1000, exp(0.5*x))\n)\n\nggplot(dat, aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(se = FALSE)\n\n\n\n\nLet’s plot log_y + 1 on x. Amazing! The relationship is basically linear, which suggests that a 1-unit increase in x has some multiplicative effect on y.\n\nggplot(dat, aes(x = x, y = log(y + 1))) +\n  geom_point() +\n  geom_smooth(se = FALSE)"
  },
  {
    "objectID": "glm_vs_transform.html#fitting-the-regression-models",
    "href": "glm_vs_transform.html#fitting-the-regression-models",
    "title": "25  GLMs vs. Transformations",
    "section": "25.2 Fitting the Regression Models",
    "text": "25.2 Fitting the Regression Models\nLet’s use OLS and Poisson regression to fit the data. We see a few things:\n\nThe Poisson model fits drastically better, both in terms of \\(R^2\\) and the coefficients are close to the data-generating values\nThe transformed OLS model understates the slope\nBoth models have (seemingly) similar interpretations: a 1-unit increase in x causes an \\(e^\\beta\\) increase in y. How is this possible?\n\nSo what’s going on?\nThe answer is that there is a very subtle difference between transformed OLS and Poisson regression. In transformed OLS, we are modeling the mean of the log of Y, or \\(E(ln(y|x))\\). In Poisson, we’re modeling the log of the mean of Y, or \\(ln(E(y|x))\\). These are not equivalent! In essence, Poisson regression is a model for the arithmetic mean, whereas OLS is a model for the geometric mean. This means that when we exponentiate the Poisson model, we can get predicted counts, but this is not true of the OLS model.\n\nm1 &lt;- lm(log(y + 1) ~ x, dat)\nm2 &lt;- glm(y ~ x, dat, family = poisson)\n\ntab_model(m1, m2,\n          p.style = \"stars\",\n          show.ci = FALSE,\n          show.se = TRUE,\n          digits = 3,\n          transform = NULL,\n          dv.labels = c(\"Log(Y+1)\", \"Poisson\"))\n\n\n\n\n \nLog(Y+1)\nPoisson\n\n\nPredictors\nEstimates\nstd. Error\nLog-Mean\nstd. Error\n\n\n(Intercept)\n0.375 ***\n0.029\n-0.074 \n0.045\n\n\nx\n0.420 ***\n0.010\n0.516 ***\n0.012\n\n\nObservations\n1000\n1000\n\n\nR2 / R2 adjusted\n0.631 / 0.631\n0.906\n\n\n* p&lt;0.05   ** p&lt;0.01   *** p&lt;0.001"
  },
  {
    "objectID": "glm_vs_transform.html#more-intuition-an-example-with-means",
    "href": "glm_vs_transform.html#more-intuition-an-example-with-means",
    "title": "25  GLMs vs. Transformations",
    "section": "25.3 More Intuition: An Example with Means",
    "text": "25.3 More Intuition: An Example with Means\nLet’s create a super simple data set, s.\n\ns &lt;- c(1, 10, 100)\n\nIt’s clearly skewed. But I can still take the mean. I could take the arithmetic mean, or the geometric mean. These are clearly different quantities.\n\nmean(s) # arithmetic\n\n[1] 37\n\nexp(mean(log((s)))) # geometric\n\n[1] 10\n\n\nThe idea of Poisson is to take the log of the mean and fit a linear model for that:\n\nlog_mean &lt;- log(mean(s))\nlog_mean\n\n[1] 3.610918\n\n\nThe idea of transformed OLS is to take the mean of the log and fit a linear model for that:\n\nmean_log &lt;- mean(log(s))\nmean_log\n\n[1] 2.302585\n\n\nWhen I exponentiate the log of the mean, I get back the original arithmetic mean. This is what Poisson is doing:\n\nexp(log_mean)\n\n[1] 37\n\n\nWhen I exponentiate the mean of the log, I get back the original geometric mean. This is what transformed OLS is doing:\n\nexp(mean_log)\n\n[1] 10"
  },
  {
    "objectID": "glm_vs_transform.html#further-reading",
    "href": "glm_vs_transform.html#further-reading",
    "title": "25  GLMs vs. Transformations",
    "section": "25.4 Further Reading",
    "text": "25.4 Further Reading\nhttps://www.theanalysisfactor.com/the-difference-between-link-functions-and-data-transformations/"
  },
  {
    "objectID": "lr_test.html#r-setup",
    "href": "lr_test.html#r-setup",
    "title": "26  Likelihood Ratio Tests",
    "section": "26.1 R Setup",
    "text": "26.1 R Setup\n\n# load libraries\nlibrary(tidyverse)\nlibrary(lme4)\nlibrary(haven)\nlibrary(sjPlot)\n\n# clear memory\nrm(list = ls())\n\nselect &lt;- dplyr::select\n\n# load HSB data\nhsb &lt;- read_dta(\"data/hsb.dta\") |&gt; \n  select(mathach, ses, schoolid)"
  },
  {
    "objectID": "lr_test.html#why-lr-tests",
    "href": "lr_test.html#why-lr-tests",
    "title": "26  Likelihood Ratio Tests",
    "section": "26.2 Why LR Tests?",
    "text": "26.2 Why LR Tests?\nOur fixed effects coefficients have SEs, z-statistics, and p-values, which allow us to easily test the null hypothesis that the slopes are 0 in the population. No such quantities, however, are provided for the random effects of our model. We can use LR tests to address this issue and test the statistical significance of the various random portions of our model.\nWe can also use LR tests on fixed effects or sets of fixed effects (like a nested F-test in OLS), but 99.9% of the time, the conclusion will be the same as using the z-statistics.\nLR tests require that the models are nested, meaning that they use the same data, and one model can be expressed as a constrained version of the other."
  },
  {
    "objectID": "lr_test.html#hsb-example",
    "href": "lr_test.html#hsb-example",
    "title": "26  Likelihood Ratio Tests",
    "section": "26.3 HSB Example",
    "text": "26.3 HSB Example\nWe fit 3 models:\n\nRandom intercept model\nRandom slope model\nRandom slope model with no correlation between intercepts and slopes\n\nWe can see from the model output that the point estimates for the random slope variance \\(\\tau_{11}\\) and the correlation \\(\\rho_{01}\\) are non-zero, but how can we get p-values for these quantities?\n\nm1 &lt;- lmer(mathach ~ ses + (1|schoolid), hsb)\nm2 &lt;- lmer(mathach ~ ses + (ses|schoolid), hsb)\nm3 &lt;- lmer(mathach ~ ses + (ses||schoolid), hsb)\n\ntab_model(m1, m2, m3,\n          p.style = \"stars\",\n          show.se = TRUE,\n          show.ci = FALSE,\n          dv.labels = c(\"RI\", \"RS\", \"No Rho\"))\n\n\n\n\n \nRI\nRS\nNo Rho\n\n\nPredictors\nEstimates\nstd. Error\nEstimates\nstd. Error\nEstimates\nstd. Error\n\n\n(Intercept)\n12.66 ***\n0.19\n12.67 ***\n0.19\n12.65 ***\n0.19\n\n\nses\n2.39 ***\n0.11\n2.39 ***\n0.12\n2.40 ***\n0.12\n\n\nRandom Effects\n\n\n\nσ2\n37.03\n36.83\n36.82\n\n\n\nτ00\n4.77 schoolid\n4.83 schoolid\n4.85 schoolid\n\n\nτ11\n \n0.41 schoolid.ses\n0.42 schoolid.ses\n\n\nρ01\n \n-0.11 schoolid\n \n\n\nICC\n0.11\n0.12\n0.12\n\n\nN\n160 schoolid\n160 schoolid\n160 schoolid\n\nObservations\n7185\n7185\n7185\n\n\nMarginal R2 / Conditional R2\n0.077 / 0.182\n0.077 / 0.189\n0.077 / 0.185\n\n\n* p&lt;0.05   ** p&lt;0.01   *** p&lt;0.001\n\n\n\n\n\n\n\n26.3.1 Are random ses slopes necessary?\nWe use anova to perform the LR test comparing m1 and m2, and we see that the random slopes are not statistically significant.\n\nanova(m1, m2)\n\nData: hsb\nModels:\nm1: mathach ~ ses + (1 | schoolid)\nm2: mathach ~ ses + (ses | schoolid)\n   npar   AIC   BIC logLik deviance  Chisq Df Pr(&gt;Chisq)\nm1    4 46649 46677 -23320    46641                     \nm2    6 46648 46690 -23318    46636 4.5354  2     0.1035\n\n\n\n\n26.3.2 Is there a correlation between the random intercept and slope for ses?\nSimilarly, we see that the correlation is non-significant\n\nanova(m2, m3)\n\nData: hsb\nModels:\nm3: mathach ~ ses + ((1 | schoolid) + (0 + ses | schoolid))\nm2: mathach ~ ses + (ses | schoolid)\n   npar   AIC   BIC logLik deviance  Chisq Df Pr(&gt;Chisq)\nm3    5 46647 46681 -23318    46637                     \nm2    6 46648 46690 -23318    46636 0.2762  1     0.5992"
  },
  {
    "objectID": "lr_test.html#technical-notes",
    "href": "lr_test.html#technical-notes",
    "title": "26  Likelihood Ratio Tests",
    "section": "26.4 Technical Notes",
    "text": "26.4 Technical Notes\nTL/DR: The traditional LR test provided by anova is likely to be conservative for testing the significance of variance components. For the purposes of this course, it is fine.\nThere is a lot of multilevel literature arguing that testing a null hypothesis on variance components with LR tests is not the best approach. The reason is that variances cannot be negative, so the null hypothesis exists on the “boundary of the parameter space” and therefore is “likely to be conservative” (to use the warning that Stata gives you, i.e., the p-values are too high). The true distribution of a 0 variance component is not a normal distribution, but a mixture distribution with half of the probability mass at 0 and the other half \\(\\chi^2\\). When you’re testing the significance of the random intercepts model, you can divide the p-value by 2 to get the right answer (Stata default), though for more complex models it’s not so simple. There are some R packages that use simulation-based approaches to provide more robust results such as pbkrtest::PBmodcomp, but we won’t go into them here. See RH&S pp. 88-89 for a more thorough discussion of this issue. Despite this, the standard LR test remains common in practice."
  },
  {
    "objectID": "anova.html#r-setup",
    "href": "anova.html#r-setup",
    "title": "27  Pretty ANOVA Tables with kable",
    "section": "27.1 R Setup",
    "text": "27.1 R Setup\nWe load the tidyverse and knitr. The kable function from knitr makes our tables look nice!\n\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(broom)"
  },
  {
    "objectID": "anova.html#create-fake-data",
    "href": "anova.html#create-fake-data",
    "title": "27  Pretty ANOVA Tables with kable",
    "section": "27.2 Create fake data",
    "text": "27.2 Create fake data\nWe create a data set called a that has 100 observations and specifies our outcome Y as a funciton of two uncorrelated variables A and B\n\na &lt;- tibble( A = rnorm( 100 ),\n            B = rnorm( 100 ),\n            Y = A * 0.2 + B * 0.5 + rnorm( 100, 0, 1 ) )"
  },
  {
    "objectID": "anova.html#run-the-models",
    "href": "anova.html#run-the-models",
    "title": "27  Pretty ANOVA Tables with kable",
    "section": "27.3 Run the Models",
    "text": "27.3 Run the Models\nWe fit two models, one with A and B, the other with just A.\n\nM1 &lt;- lm( Y~ A + B, data = a )\nM2 &lt;- lm( Y ~ A, data = a )"
  },
  {
    "objectID": "anova.html#comparing-the-models",
    "href": "anova.html#comparing-the-models",
    "title": "27  Pretty ANOVA Tables with kable",
    "section": "27.4 Comparing the Models",
    "text": "27.4 Comparing the Models\nWe use the anova function to compare the two models (see also the chapter on Likelihood Ratio tests). We see that B improves the model fit significantly.\n\naa = anova( M2, M1 )\naa\n\nAnalysis of Variance Table\n\nModel 1: Y ~ A\nModel 2: Y ~ A + B\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     98 151.22                                  \n2     97 100.17  1    51.049 49.434 2.883e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\naa |&gt; \n  tidy() |&gt; \n  kable()\n\n\n\n\nterm\ndf.residual\nrss\ndf\nsumsq\nstatistic\np.value\n\n\n\n\nY ~ A\n98\n151.2178\nNA\nNA\nNA\nNA\n\n\nY ~ A + B\n97\n100.1688\n1\n51.04901\n49.43408\n0"
  },
  {
    "objectID": "anova.html#compare-to-the-significance-test-on-b",
    "href": "anova.html#compare-to-the-significance-test-on-b",
    "title": "27  Pretty ANOVA Tables with kable",
    "section": "27.5 Compare to the Significance test on B",
    "text": "27.5 Compare to the Significance test on B\nNote that the p value for B is identical to the ANOVA results above. Why bother with ANOVA? It can test more complex hypotheses as well (multiple coefficients, random effects, etc.)\n\nM1 |&gt; \n  tidy() |&gt; \n  kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.0240502\n0.1019864\n-0.2358173\n0.8140716\n\n\nA\n0.2183483\n0.0987106\n2.2120045\n0.0293134\n\n\nB\n0.6922804\n0.0984620\n7.0309371\n0.0000000"
  },
  {
    "objectID": "aic_check.html",
    "href": "aic_check.html",
    "title": "28  AIC, BIC, and Deviance",
    "section": "",
    "text": "In this section, we briefly walk through how to find AIC, BIC, and Deviance to compare models. We have a simple multilevel dataset (we generate through a utility package, blkvar, that Miratrix and the C.A.R.E.S. lab has used to explore how multilevel modeling works in practice), and generate a few variables that we will use as predictors. Only the fourth variable is actually useful for prediction! Let’s see if our AIC, etc., measures identify which model is superior.\nTo install a “working package” we use devtools:\n\ndevtools::install_github(\"https://github.com/lmiratrix/blkvar\" )\n\n\nlibrary( blkvar )\ndd = generate_multilevel_data( J = 40 )\nhead( dd )\n\n  sid         Y0         Y1 Z       Yobs         W\n1   1 -0.2948552  0.5221488 1  0.5221488 0.8399913\n2   1 -0.3908916  0.4261124 1  0.4261124 0.8399913\n3   1 -0.5174259  0.2995781 0 -0.5174259 0.8399913\n4   1 -0.8278492 -0.0108452 0 -0.8278492 0.8399913\n5   1  1.4104759  2.2274799 0  1.4104759 0.8399913\n6   1 -0.1807651  0.6362389 1  0.6362389 0.8399913\n\ndd$X1 = rnorm( nrow(dd) )\ndd$X2 = rnorm( nrow(dd) )\ndd$X3 = rnorm( nrow(dd) )\ndd$X4 = dd$Yobs + rnorm(nrow(dd))\n \nM1 = lmer( Yobs ~ 1 + (1|sid), data=dd )\nM2 = lmer( Yobs ~ 1 + X1 + X2 + X3 + (1|sid), data=dd )\nM3 = lmer( Yobs ~ 1 + X1 + X2 + X3 + X4 + (1|sid), data=dd )\n\nlibrary( arm )\n\nLoading required package: MASS\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\n\narm (Version 1.13-1, built: 2022-8-25)\n\n\nWorking directory is /Users/lmiratrix/Desktop/MLM\n\ndisplay(M1)\n\nlmer(formula = Yobs ~ 1 + (1 | sid), data = dd)\ncoef.est  coef.se \n   -0.03     0.11 \n\nError terms:\n Groups   Name        Std.Dev.\n sid      (Intercept) 0.65    \n Residual             0.59    \n---\nnumber of obs: 452, groups: sid, 40\nAIC = 914.7, DIC = 903.5\ndeviance = 906.1 \n\ndisplay(M2)\n\nlmer(formula = Yobs ~ 1 + X1 + X2 + X3 + (1 | sid), data = dd)\n            coef.est coef.se\n(Intercept) -0.03     0.11  \nX1           0.00     0.03  \nX2          -0.03     0.03  \nX3          -0.04     0.03  \n\nError terms:\n Groups   Name        Std.Dev.\n sid      (Intercept) 0.65    \n Residual             0.59    \n---\nnumber of obs: 452, groups: sid, 40\nAIC = 933.7, DIC = 885\ndeviance = 903.4 \n\nlibrary( texreg )\n\nVersion:  1.38.6\nDate:     2022-04-06\nAuthor:   Philip Leifeld (University of Essex)\n\nConsider submitting praise using the praise or praise_interactive functions.\nPlease cite the JSS article in your publications -- see citation(\"texreg\").\n\nscreenreg( list( M1, M2, M3 ) )\n\n\n=====================================================\n                      Model 1   Model 2   Model 3    \n-----------------------------------------------------\n(Intercept)             -0.03     -0.03     -0.04    \n                        (0.11)    (0.11)    (0.08)   \nX1                                 0.00     -0.02    \n                                  (0.03)    (0.03)   \nX2                                -0.03     -0.02    \n                                  (0.03)    (0.03)   \nX3                                -0.04     -0.05    \n                                  (0.03)    (0.02)   \nX4                                           0.27 ***\n                                            (0.02)   \n-----------------------------------------------------\nAIC                    914.74    933.73    798.87    \nBIC                    927.09    958.41    827.66    \nLog Likelihood        -454.37   -460.86   -392.43    \nNum. obs.              452       452       452       \nNum. groups: sid        40        40        40       \nVar: sid (Intercept)     0.43      0.42      0.22    \nVar: Residual            0.34      0.34      0.26    \n=====================================================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05"
  },
  {
    "objectID": "worked_examples.html",
    "href": "worked_examples.html",
    "title": "Worked Examples",
    "section": "",
    "text": "This section provides several worked examples of various MLM techniques."
  },
  {
    "objectID": "hsb_ex.html#r-setup",
    "href": "hsb_ex.html#r-setup",
    "title": "29  HSB Example",
    "section": "29.1 R Setup",
    "text": "29.1 R Setup\n\nlibrary(foreign) #this lets us read in spss files\nlibrary(tidyverse) #this is a broad package that allows us to do lots of data management-y things (and ggplot!)\nlibrary(lme4) #this allows us to run MLM\nlibrary(arm) #this allows us to display MLM\nlibrary( lmerTest ) # this puts p-values on the summary() command for fixed effects"
  },
  {
    "objectID": "hsb_ex.html#load-hsb-data",
    "href": "hsb_ex.html#load-hsb-data",
    "title": "29  HSB Example",
    "section": "29.2 Load HS&B data",
    "text": "29.2 Load HS&B data\n\n# Read student data\nstud.dat = read.spss( \"data/hsb1.sav\", to.data.frame=TRUE )\n\n# Read in school data\nsch.dat = read.spss( \"data/hsb2.sav\", to.data.frame=TRUE )\n\n# Make single data frame with all variables, keep all students even if they\n# don't match to a school\ndat = merge( stud.dat, sch.dat, by=\"id\", all.x=TRUE )"
  },
  {
    "objectID": "hsb_ex.html#table-4.1-descriptive-summaries",
    "href": "hsb_ex.html#table-4.1-descriptive-summaries",
    "title": "29  HSB Example",
    "section": "29.3 Table 4.1 Descriptive summaries",
    "text": "29.3 Table 4.1 Descriptive summaries\n\n## Get mean and SD of the Level 1 variables, rounded to 2 decimal places\n# math achievement\nround(mean(dat$mathach),2)\n\n[1] 12.75\n\nround(sd(dat$mathach),2)\n\n[1] 6.88\n\n# ses\nround(mean(dat$ses),2)\n\n[1] 0\n\nround(sd(dat$ses),2)\n\n[1] 0.78\n\n## Get mean and SD of Level 2 variables, round to 2 decimal places\n# NOTE: we are getting these from the SCHOOL-LEVEL FILE\n# sector\nround(mean(sch.dat$sector),2) # this answers \"what percent of schools are catholic?\"\n\n[1] 0.44\n\nround(sd(sch.dat$sector),2)\n\n[1] 0.5\n\n# mean ses\nround(mean(sch.dat$meanses),2) # this answers \"what is the average of the school-average SES values?\"\n\n[1] 0\n\nround(sd(sch.dat$meanses),2)\n\n[1] 0.41\n\n# NOTE: if we used the student-level or \"dat\" file, we would be answering the\n# following questions:\n# * what percent of students attend a catholic school?\n# * what is the average student ses? &lt;- this would match what we calculated\n# ourselves if we had the entire school in our sample"
  },
  {
    "objectID": "hsb_ex.html#table-4.2-one-way-anova-i.e-uncontrolled-random-intercept",
    "href": "hsb_ex.html#table-4.2-one-way-anova-i.e-uncontrolled-random-intercept",
    "title": "29  HSB Example",
    "section": "29.4 Table 4.2: One-Way ANOVA (i.e uncontrolled random intercept)",
    "text": "29.4 Table 4.2: One-Way ANOVA (i.e uncontrolled random intercept)\n\n## Fit the model described \nmod4.2 &lt;- lmer(mathach ~ 1 + (1|id), data=dat)\n# Peek at the results\ndisplay(mod4.2)\n\nlmer(formula = mathach ~ 1 + (1 | id), data = dat)\ncoef.est  coef.se \n   12.64     0.24 \n\nError terms:\n Groups   Name        Std.Dev.\n id       (Intercept) 2.93    \n Residual             6.26    \n---\nnumber of obs: 7185, groups: id, 160\nAIC = 47122.8, DIC = 47114.8\ndeviance = 47115.8 \n\n## Extract the fixed effect coefficient (and it's standard error)\nfixef(mod4.2) # extracts the fixed effect coefficient(s)\n\n(Intercept) \n   12.63697 \n\nse.coef(mod4.2)$fixef #extracts the standard errors for the fixed effect(s)\n\n[1] 0.2443936\n\n## Extract the variance components\n# Note: in the model display, we see the SDs, not the variance\nVarCorr(mod4.2)\n\n Groups   Name        Std.Dev.\n id       (Intercept) 2.9350  \n Residual             6.2569  \n\n# To get the variances, we extract each part and square it\n# variance of random intercept\n(sigma.hat(mod4.2)$sigma$id)^2\n\n(Intercept) \n   8.614025 \n\n# variance of level 1 residual (easier to extract)\nsigma(mod4.2)^2 \n\n[1] 39.14832\n\n# could also use the more complicated formula that we used with the intercept.\n# If we do, we get the same thing\nsigma.hat(mod4.2)$sigma$data^2\n\n[1] 39.14832\n\n# Inference on the need for a random intercept\n# Thus uses the book's way of calculating a test statistic with a\n# chi-squared distribution.\n\nschools = dat %&gt;% group_by( id ) %&gt;%\n  summarise( nj = n(),\n             Y.bar.j = mean( mathach ) )\ngamma.00 = fixef( mod4.2 )[[1]]\nsigma.2 = sigma(mod4.2)^2 \nH = sum( schools$nj * (schools$Y.bar.j - gamma.00)^2 / sigma.2 )\nH\n\n[1] 1660.232\n\n# our p-value\npchisq( H, df = nrow( schools ) - 1, lower.tail = FALSE )\n\n[1] 4.770612e-248\n\n# calculating the ICC\ntau.00 = VarCorr(mod4.2)$id[1,1]\nrho.hat = tau.00 / (tau.00 + sigma.2 )\nrho.hat\n\n[1] 0.1803518\n\n# Calculating reliability for each school mean. (Here it is purely a function of\n# students in the school.  More students, more info, and thus more reliable.)\nsigma.2 = sigma(mod4.2)^2 \ntau.00 = VarCorr(mod4.2)$id[1,1]\nlambda = tau.00 / ( tau.00 + sigma.2 / schools$nj )\nmean( lambda )\n\n[1] 0.9013773\n\n# A bonus graph of the reliabilities\nqplot( lambda )"
  },
  {
    "objectID": "hsb_ex.html#table-4.3-means-as-outcomes-model",
    "href": "hsb_ex.html#table-4.3-means-as-outcomes-model",
    "title": "29  HSB Example",
    "section": "29.5 Table 4.3 Means as Outcomes Model",
    "text": "29.5 Table 4.3 Means as Outcomes Model\n\n# (i.e. random intercept with Level 2 predictor)\n## Fit the model described \nmod4.3 &lt;- lmer(mathach ~ 1 + meanses + (1|id), data=dat)\n\n# Peek at the results\ndisplay(mod4.3)\n\nlmer(formula = mathach ~ 1 + meanses + (1 | id), data = dat)\n            coef.est coef.se\n(Intercept) 12.65     0.15  \nmeanses      5.86     0.36  \n\nError terms:\n Groups   Name        Std.Dev.\n id       (Intercept) 1.62    \n Residual             6.26    \n---\nnumber of obs: 7185, groups: id, 160\nAIC = 46969.3, DIC = 46956.9\ndeviance = 46959.1 \n\n## Extract the fixed effect coefficients (and standard errors/t-statistics)\nfixef(mod4.3) # extracts the fixed effect coefficients\n\n(Intercept)     meanses \n  12.649435    5.863538 \n\n# NOTE: you can call them separately by \"indexing\" them\n# just the intercept\nfixef(mod4.3)[1]\n\n(Intercept) \n   12.64944 \n\n# just coefficient on mean ses\nfixef(mod4.3)[2]\n\n meanses \n5.863538 \n\nse.coef(mod4.3)$fixef #extracts the standard errors for the fixed effect(s)\n\n[1] 0.1492801 0.3614580\n\n## Calculate (or extract) the t-ratio (aka the t-statistic)\n\n# NOTE: the author's don't present this for the intercept, because we often\n# don't care. But it is presented here for completeness\n\n# tstats for intercept\nfixef(mod4.3)[1]/se.coef(mod4.3)$fixef[1]\n\n(Intercept) \n   84.73622 \n\n# tstat mean ses\nfixef(mod4.3)[2]/se.coef(mod4.3)$fixef[2]\n\n meanses \n16.22191 \n\n# tstat extracted - this does both variables at once! \ncoef(summary(mod4.3))[,\"t value\"]\n\n(Intercept)     meanses \n   84.73622    16.22191 \n\n# NOTE: Let's look at what is happening here:\ncoef(summary(mod4.3)) # gives us all the fixed effect statistics we could want\n\n             Estimate Std. Error       df  t value      Pr(&gt;|t|)\n(Intercept) 12.649435  0.1492801 153.7425 84.73622 6.032591e-131\nmeanses      5.863538  0.3614580 153.4067 16.22191  4.267894e-35\n\n# the [ ] is called \"indexing\" - it's a way of subsetting data by telling R\n# which [rows,columns] you want to see we are telling R that we want ALL rows \"[\n# ,\" but only the column labeled \"t value\"\n\n## Extract the variance components\n# Note: in the model display, we see the SDs, not the variance\nVarCorr(mod4.3)\n\n Groups   Name        Std.Dev.\n id       (Intercept) 1.6244  \n Residual             6.2576  \n\n# To get the variances, we extract each part and square it\n# variance of random intercept\n(sigma.hat(mod4.3)$sigma$id)^2\n\n(Intercept) \n   2.638708 \n\n# variance of level 1 residual\nsigma(mod4.3)^2 \n\n[1] 39.15708\n\n# Range of plausible values for school means for schools with mean SES of 0:\n# See page 73-74)\nfixef( mod4.3 )[[1]] + c(-1.96, 1.96) * (sigma.hat(mod4.3)$sigma$id)\n\n[1]  9.465592 15.833279\n\n# Compare to our model without mean ses\nfixef( mod4.2 )[[1]] + c(-1.96, 1.96) * (sigma.hat(mod4.2)$sigma$id)\n\n[1]  6.884441 18.389507\n\n# Proportion reduction in variance or \"variance explained\" at level 2\ntau.00.anova = (sigma.hat(mod4.2)$sigma$id)^2\ntau.00.meanses = (sigma.hat(mod4.3)$sigma$id)^2\n(tau.00.anova-tau.00.meanses) / tau.00.anova\n\n(Intercept) \n   0.693673 \n\n## Inference on the random effects\nschools = merge( schools, sch.dat, by=\"id\" )\ngamma.00 = fixef( mod4.3 )[[1]]\ngamma.01 = fixef( mod4.3 )[[2]]\nschools = mutate( schools, resid = Y.bar.j - gamma.00 - gamma.01*meanses )\nH = sum( schools$nj * schools$resid^2 ) / sigma(mod4.3)^2 \nH\n\n[1] 633.5175\n\npchisq( H, nrow( schools ) - 2, lower.tail = FALSE )\n\n[1] 3.617696e-58\n\n## Reliability revisited (from pg 75)\nmod4.3\n\nLinear mixed model fit by REML ['lmerModLmerTest']\nFormula: mathach ~ 1 + meanses + (1 | id)\n   Data: dat\nREML criterion at convergence: 46961.28\nRandom effects:\n Groups   Name        Std.Dev.\n id       (Intercept) 1.624   \n Residual             6.258   \nNumber of obs: 7185, groups:  id, 160\nFixed Effects:\n(Intercept)      meanses  \n     12.649        5.864  \n\nu.hat = coef( mod4.3 )$id\nhead( u.hat )\n\n     (Intercept)  meanses\n1224    12.32688 5.863538\n1288    12.71898 5.863538\n1296    10.70101 5.863538\n1308    12.92208 5.863538\n1317    11.48086 5.863538\n1358    11.73878 5.863538\n\nsigma.2 = sigma(mod4.3)^2 \ntau.00 = VarCorr(mod4.3)$id[1,1]\nsigma.2\n\n[1] 39.15708\n\ntau.00\n\n[1] 2.638708\n\n# These are the individual reliabilities---how well we can separate schools with the same Mean SES\n# (So it is _conditional_ on the mean SES of the schools.)\nlambda.j = tau.00 / (tau.00 + (sigma.2 / schools$nj))\nmean( lambda.j )\n\n[1] 0.7400747"
  },
  {
    "objectID": "hsb_ex.html#table-4.4-random-coefficient-model-i.e.-random-slope",
    "href": "hsb_ex.html#table-4.4-random-coefficient-model-i.e.-random-slope",
    "title": "29  HSB Example",
    "section": "29.6 Table 4.4 Random coefficient model (i.e. random slope)",
    "text": "29.6 Table 4.4 Random coefficient model (i.e. random slope)\n\n# group-mean center ses  \ndat &lt;- dat %&gt;% group_by( id ) %&gt;% \n  mutate( ses_grpcenter = ses - mean(ses) )\n\n## Fit the model described \nmod4.4 &lt;- lmer(mathach ~ 1 + ses_grpcenter + ( 1 + ses_grpcenter | id ), data=dat)\n# Peek at the results\ndisplay(mod4.4)\n\nlmer(formula = mathach ~ 1 + ses_grpcenter + (1 + ses_grpcenter | \n    id), data = dat)\n              coef.est coef.se\n(Intercept)   12.64     0.24  \nses_grpcenter  2.19     0.13  \n\nError terms:\n Groups   Name          Std.Dev. Corr \n id       (Intercept)   2.95          \n          ses_grpcenter 0.83     0.02 \n Residual               6.06          \n---\nnumber of obs: 7185, groups: id, 160\nAIC = 46726.2, DIC = 46707.7\ndeviance = 46711.0 \n\n## Extract the fixed effect coefficients (and standard errors/t-statistics)\ncoef(summary(mod4.4)) #this reproduces the whole first panel, though methods used above also work\n\n               Estimate Std. Error       df  t value      Pr(&gt;|t|)\n(Intercept)   12.636193  0.2445047 156.7512 51.68077 2.286892e-100\nses_grpcenter  2.193196  0.1282589 155.2166 17.09976  1.582355e-37\n\n## Extract the variance components\n# Note: in the model display, we see the SDs, not the variance\nVarCorr(mod4.4) \n\n Groups   Name          Std.Dev. Corr \n id       (Intercept)   2.94636       \n          ses_grpcenter 0.83307  0.019\n Residual               6.05807       \n\n# variance of random effects\n(sigma.hat(mod4.4)$sigma$id)^2\n\n  (Intercept) ses_grpcenter \n    8.6810437     0.6939974 \n\n# NOTE: to extract one or the other, you can use indexing\n(sigma.hat(mod4.4)$sigma$id[1])^2 #this is just the intercept random effect\n\n(Intercept) \n   8.681044 \n\n# variance of level 1 residual\nsigma(mod4.4)^2\n\n[1] 36.70019"
  },
  {
    "objectID": "hsb_ex.html#table-4.5-intercepts-and-slopes-as-outcomes-model",
    "href": "hsb_ex.html#table-4.5-intercepts-and-slopes-as-outcomes-model",
    "title": "29  HSB Example",
    "section": "29.7 Table 4.5 Intercepts and Slopes as Outcomes Model",
    "text": "29.7 Table 4.5 Intercepts and Slopes as Outcomes Model\n\n## Fit the model described \nmod4.5 &lt;- lmer(mathach ~ 1 + meanses + sector + ses_grpcenter*(meanses + sector) + ( 1 + ses_grpcenter | id ), data=dat)\n\n# NOTE: The code above allows the coefficients to appear in the same order as in Table 4.5\n\n# R automatically includes the main effects, so this model can be written more\n# concisely as shown below:\n#\n# lmer(mathach ~ 1 + ses_grpcenter*(meanses + sector) + ( 1 + ses_grpcenter | id ), data=dat)\n\n# Peek at the results\ndisplay(mod4.5)\n\nlmer(formula = mathach ~ 1 + meanses + sector + ses_grpcenter * \n    (meanses + sector) + (1 + ses_grpcenter | id), data = dat)\n                      coef.est coef.se\n(Intercept)           12.10     0.20  \nmeanses                5.33     0.37  \nsector                 1.23     0.31  \nses_grpcenter          2.94     0.16  \nmeanses:ses_grpcenter  1.04     0.30  \nsector:ses_grpcenter  -1.64     0.24  \n\nError terms:\n Groups   Name          Std.Dev. Corr \n id       (Intercept)   1.54          \n          ses_grpcenter 0.32     0.39 \n Residual               6.06          \n---\nnumber of obs: 7185, groups: id, 160\nAIC = 46523.7, DIC = 46489.2\ndeviance = 46496.4 \n\n## Extract the fixed effect coefficients (and standard errors/t-statistics)\n#this reproduces the whole first panel, though methods used above also work\ncoef(summary(mod4.5))\n\n                       Estimate Std. Error       df   t value      Pr(&gt;|t|)\n(Intercept)           12.095997  0.1987329 159.9143 60.865590 1.625101e-112\nmeanses                5.332898  0.3691567 150.9836 14.446161  2.944282e-30\nsector                 1.226453  0.3062674 149.6139  4.004518  9.756638e-05\nses_grpcenter          2.938785  0.1550889 139.2934 18.949039  2.197495e-40\nmeanses:ses_grpcenter  1.038918  0.2988941 160.5429  3.475873  6.550388e-04\nsector:ses_grpcenter  -1.642619  0.2397854 143.3351 -6.850371  2.009492e-10\n\n# NOTE: there is a slight descrepancy in the estimate for meanses:ses_grpcenter and \n# the t-statistics for meanses:ses_grpcenter and sector:ses_grpcenter; nothing that \n# changes the interpretations, however.\n\n\n# Testing the need for sector  (see page 82)\n# (We use a likelihood ratio test with the anova() function)\nmod4.5.null &lt;- lmer(mathach ~ 1 + meanses + ses_grpcenter*(meanses) + ( 1 + ses_grpcenter | id ), data=dat)\nanova( mod4.5, mod4.5.null )\n\nData: dat\nModels:\nmod4.5.null: mathach ~ 1 + meanses + ses_grpcenter * (meanses) + (1 + ses_grpcenter | id)\nmod4.5: mathach ~ 1 + meanses + sector + ses_grpcenter * (meanses + sector) + (1 + ses_grpcenter | id)\n            npar   AIC   BIC logLik deviance  Chisq Df Pr(&gt;Chisq)    \nmod4.5.null    8 46568 46623 -23276    46552                         \nmod4.5        10 46516 46585 -23248    46496 55.941  2  7.122e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Testing the need for random slope  (see page 84)\n# (We use a likelihood ratio test with the anova() function)\nmod4.5.null.slope &lt;- lmer(mathach ~ 1 + meanses + sector + ses_grpcenter*(meanses + sector) + ( 1 | id ), data=dat) \nanova( mod4.5, mod4.5.null.slope )\n\nData: dat\nModels:\nmod4.5.null.slope: mathach ~ 1 + meanses + sector + ses_grpcenter * (meanses + sector) + (1 | id)\nmod4.5: mathach ~ 1 + meanses + sector + ses_grpcenter * (meanses + sector) + (1 + ses_grpcenter | id)\n                  npar   AIC   BIC logLik deviance  Chisq Df Pr(&gt;Chisq)\nmod4.5.null.slope    8 46513 46568 -23249    46497                     \nmod4.5              10 46516 46585 -23248    46496 1.0039  2     0.6054"
  },
  {
    "objectID": "hsb_ex.html#figure-4.1",
    "href": "hsb_ex.html#figure-4.1",
    "title": "29  HSB Example",
    "section": "29.8 Figure 4.1",
    "text": "29.8 Figure 4.1\nNOTE: Figure 4.1 is a graphical display using the results from Model/Table 4.5\nThe solid line represents the slope of the gamma-01 coefficient; this is the same in public and catholic schools. The dotted lines represent the the slope for individual schools with “prototypical” values of meanses (-1,0,1 standard deviations from mean)\n\n# to calculate this, we should note a few values: \navg_meanses &lt;- mean(dat$meanses) #average of mean ses var\nhigh_meanses &lt;- mean(dat$meanses) + sd(dat$meanses) # 1 sd above avg meanses\nlow_meanses &lt;- mean(dat$meanses) - sd(dat$meanses) # 1 sd below avg meanses\n\nfake.students = expand.grid( id = -1,\n                             meanses = c( low_meanses, avg_meanses, high_meanses ),\n                             sector = c( 0, 1 ),\n                             ses_grpcenter = c( -1, 0, 1 ) )\nfake.students = mutate( fake.students, ses = meanses + ses_grpcenter )\nfake.students$mathach = predict( mod4.5, newdata=fake.students, allow.new.levels = TRUE )\nfake.schools = filter( fake.students, ses_grpcenter == 0 )\n\nggplot( fake.students, aes( ses, mathach ) ) + \n  facet_wrap( ~ sector ) +\n  geom_line( aes( group=meanses ), lty = 2 ) +\n  geom_line( data=fake.schools, aes( x = ses, y = mathach ) ) +\n  geom_point( data=fake.schools, aes( x = ses, y = mathach ) )"
  },
  {
    "objectID": "hsb_ex.html#set-up-for-remaining-tablesfigures-of-chapter",
    "href": "hsb_ex.html#set-up-for-remaining-tablesfigures-of-chapter",
    "title": "29  HSB Example",
    "section": "29.9 Set-up for remaining tables/figures of chapter",
    "text": "29.9 Set-up for remaining tables/figures of chapter\nIn order to create table 4.6 and the following 2 graphs, we will need to prepare a new dataset. These next lines of code do that.\n\n## Start with school level data frame and keep variables interesting to our model comparison\nmod.comp &lt;- dplyr::select( sch.dat, id, meanses, sector )\n\n## Add in number of observations per school \nn_j &lt;- dat %&gt;% group_by( id ) %&gt;%\n  dplyr::summarise(n_j = n())\n\nmod.comp &lt;- merge(mod.comp, n_j, by=\"id\")\nhead( mod.comp )\n\n    id meanses sector n_j\n1 1224  -0.428      0  47\n2 1288   0.128      0  25\n3 1296  -0.420      0  48\n4 1308   0.534      1  20\n5 1317   0.351      1  48\n6 1358  -0.014      0  30\n\n## Run site-specific OLS for each school and save estimates \n\n# Calculate global (not group) centered ses\ndat$ses_centered &lt;- dat$ses - mean(dat$ses)\n\n# This is the \"for loop\" method of generating an estimate for each of many small\n# worlds (schools). See lecture 2.3 code for the \"tidyverse\" way.\nest.ols &lt;- matrix(nrow=160,ncol=2) #create a matrix to store estimates \nse.ols &lt;- matrix(nrow=160,ncol=2) #create matrix to store standard errors\n\nfor (i in 1:length(unique(dat$id))){ #looping across the 160 different values of id\n    id &lt;- unique(dat$id)[i] #pick the value of id we want\n    mod &lt;- lm(mathach ~ 1 + ses_grpcenter, data=dat[dat$id==id,]) #run the model on students in that 1 school\n    est.ols[i,] &lt;- coef( mod ) #save the setimates in the matrix we created\n    se.ols[i,] &lt;- se.coef( mod ) # and the SEs\n}\n\n#convert the matrix to a dataframe and attach the schoolid info\nest.ols &lt;- as.data.frame(est.ols)\nest.ols$id &lt;- sch.dat$id\nnames(est.ols) &lt;- c( 'b0_ols', 'b1_ols', 'id' )\n\n#store standard errors for later\nse.ols &lt;- as.data.frame(se.ols)\nse.ols$id &lt;- sch.dat$id\nnames(se.ols) &lt;- c( 'se_b0_ols', 'se_b1_ols', 'id' )\n\nmod.comp &lt;- merge(mod.comp, est.ols, by='id')\nmod.comp &lt;- merge(mod.comp, se.ols, by='id' )\nhead( mod.comp )\n\n    id meanses sector n_j    b0_ols    b1_ols se_b0_ols se_b1_ols\n1 1224  -0.428      0  47  9.715447 2.5085817 1.0954478  1.765216\n2 1288   0.128      0  25 13.510800 3.2554487 1.3637656  2.079675\n3 1296  -0.420      0  48  7.635958 1.0759591 0.7740752  1.209016\n4 1308   0.534      1  20 16.255500 0.1260242 1.4045813  3.003437\n5 1317   0.351      1  48 13.177688 1.2739128 0.7902486  1.435942\n6 1358  -0.014      0  30 11.206233 5.0680087 0.8994345  1.391550\n\n# We are done running OLS on each of our schools and storing the results.\n\n## Extract site-specific coefficients from \"unconditional model\" (model 4.4)\nest4.4 &lt;- coef(mod4.4)$id\nnames(est4.4) &lt;- c('b0_uncond', 'b1_uncond') #rename\nest4.4$id = rownames( est4.4 )\n\n## Extract site-specific coefficients from the \"conditional model\" (model 4.5)\nest4.5 &lt;- coef(mod4.5)$id\nhead( est4.5 )\n\n     (Intercept)  meanses   sector ses_grpcenter meanses:ses_grpcenter\n1224    12.02263 5.332898 1.226453      2.933689              1.038918\n1288    12.55180 5.332898 1.226453      2.979174              1.038918\n1296    10.38509 5.332898 1.226453      2.744066              1.038918\n1308    12.12710 5.332898 1.226453      2.923822              1.038918\n1317    10.56530 5.332898 1.226453      2.806582              1.038918\n1358    11.60500 5.332898 1.226453      2.961265              1.038918\n     sector:ses_grpcenter\n1224            -1.642619\n1288            -1.642619\n1296            -1.642619\n1308            -1.642619\n1317            -1.642619\n1358            -1.642619\n\nest4.5$id = rownames( est4.5 )\n\n# Now we need to calculate the point estimates using our individual regression equations\n# including our level-2 values for each school\n# (This is a bit of a pain.)\nest4.5 = merge( est4.5, mod.comp, by=\"id\", suffixes = c( \"\", \".v\" ) )\nhead( est4.5 )\n\n    id (Intercept)  meanses   sector ses_grpcenter meanses:ses_grpcenter\n1 1224    12.02263 5.332898 1.226453      2.933689              1.038918\n2 1288    12.55180 5.332898 1.226453      2.979174              1.038918\n3 1296    10.38509 5.332898 1.226453      2.744066              1.038918\n4 1308    12.12710 5.332898 1.226453      2.923822              1.038918\n5 1317    10.56530 5.332898 1.226453      2.806582              1.038918\n6 1358    11.60500 5.332898 1.226453      2.961265              1.038918\n  sector:ses_grpcenter meanses.v sector.v n_j    b0_ols    b1_ols se_b0_ols\n1            -1.642619    -0.428        0  47  9.715447 2.5085817 1.0954478\n2            -1.642619     0.128        0  25 13.510800 3.2554487 1.3637656\n3            -1.642619    -0.420        0  48  7.635958 1.0759591 0.7740752\n4            -1.642619     0.534        1  20 16.255500 0.1260242 1.4045813\n5            -1.642619     0.351        1  48 13.177688 1.2739128 0.7902486\n6            -1.642619    -0.014        0  30 11.206233 5.0680087 0.8994345\n  se_b1_ols\n1  1.765216\n2  2.079675\n3  1.209016\n4  3.003437\n5  1.435942\n6  1.391550\n\nest4.5 = mutate( est4.5, \n                 b0_cond = `(Intercept)` + sector * sector.v + meanses * meanses.v,\n                 b1_cond = ses_grpcenter + `sector:ses_grpcenter` * sector.v + `meanses:ses_grpcenter` * meanses.v )\n\nest4.5 = dplyr::select( est4.5, id, b0_cond, b1_cond )\n\n\n## Combine the MLM estimates into 1 dataset with ids\nest.mlm &lt;- merge( est4.4, est4.5, by=\"id\" )\n\n# Merge all the estimates together by school id\nmod.comp &lt;- merge(mod.comp,est.mlm,by = 'id',all=TRUE)\n\nhead( mod.comp )\n\n    id meanses sector n_j    b0_ols    b1_ols se_b0_ols se_b1_ols b0_uncond\n1 1224  -0.428      0  47  9.715447 2.5085817 1.0954478  1.765216  9.956953\n2 1288   0.128      0  25 13.510800 3.2554487 1.3637656  2.079675 13.386036\n3 1296  -0.420      0  48  7.635958 1.0759591 0.7740752  1.209016  8.039091\n4 1308   0.534      1  20 16.255500 0.1260242 1.4045813  3.003437 15.622073\n5 1317   0.351      1  48 13.177688 1.2739128 0.7902486  1.435942 13.132771\n6 1358  -0.014      0  30 11.206233 5.0680087 0.8994345  1.391550 11.387452\n  b1_uncond   b0_cond  b1_cond\n1  2.262837  9.740146 2.489033\n2  2.375964 13.234409 3.112156\n3  1.872247  8.145275 2.307720\n4  2.050193 16.201317 1.835985\n5  1.997129 13.663596 1.528623\n6  2.738390 11.530341 2.946721"
  },
  {
    "objectID": "hsb_ex.html#table-4.6-comparing-site-specific-estimates-from-different-models",
    "href": "hsb_ex.html#table-4.6-comparing-site-specific-estimates-from-different-models",
    "title": "29  HSB Example",
    "section": "29.10 Table 4.6 Comparing site-specific estimates from different models",
    "text": "29.10 Table 4.6 Comparing site-specific estimates from different models\n\n## Create the list of rows that B&R include in the table p. 87\nkeeprows &lt;- c(4, 15, 17, 22, 27, 53, 69, 75, 81, 90, 135, 153)\n\n## Limit data to the rows of interest, and print the columns in Table 4.6 in the correct order\ntab4.6 &lt;- mod.comp[keeprows, c('b0_ols','b1_ols','b0_uncond','b1_uncond','b0_cond','b1_cond','n_j','meanses','sector') ]\n\n\n## Print Table 4.6 -- the Empirical Bayes from conditional model (b0_cond, b1_cond) are waaaaaay off\nround(tab4.6,2)\n\n    b0_ols b1_ols b0_uncond b1_uncond b0_cond b1_cond n_j meanses sector\n4    16.26   0.13     15.62      2.05   16.20    1.84  20    0.53      1\n15   15.98   2.15     15.74      2.19   16.01    1.84  53    0.52      1\n17   18.11   0.09     17.41      1.95   17.25    3.71  29    0.69      0\n22   11.14  -0.78     11.22      1.15   10.89    0.63  67   -0.62      1\n27   13.40   4.10     13.32      2.54   12.95    3.00  38   -0.06      0\n53    9.52   3.74      9.76      2.75    9.37    2.42  51   -0.64      0\n69   11.47   6.18     11.64      2.72   11.92    3.03  25    0.08      0\n75    9.06   1.65      9.28      2.01    9.30    0.67  63   -0.59      1\n81   15.42   5.26     15.25      3.14   15.53    1.91  66    0.43      1\n90   12.14   1.97     12.18      2.14   12.34    3.03  50    0.19      0\n135   4.55   0.25      6.42      1.92    8.55    2.63  14    0.03      0\n153  10.28   0.76     10.71      2.06    9.67    2.37  19   -0.59      0"
  },
  {
    "objectID": "hsb_ex.html#figure-4.2-scatter-plots-of-the-estimates-from-2-unconstrained-models",
    "href": "hsb_ex.html#figure-4.2-scatter-plots-of-the-estimates-from-2-unconstrained-models",
    "title": "29  HSB Example",
    "section": "29.11 Figure 4.2 : Scatter plots of the estimates from 2 unconstrained models",
    "text": "29.11 Figure 4.2 : Scatter plots of the estimates from 2 unconstrained models\n\n## Panel (a) and Panel (b) are plotted on the same graph \nggplot(data=mod.comp,aes()) + \n  geom_point(aes(x=b1_ols,y=b0_ols),color='black',alpha=0.7) + \n  geom_point(aes(x=b1_uncond,y=b0_uncond),color='blue',alpha=0.7) + \n  labs(title=\"Black=OLS; Blue=Unconditional EB\") +\n  xlim(-5,8) + ylim(2,20)"
  },
  {
    "objectID": "hsb_ex.html#figure-4.3-scatter-plots-of-residuals-from-the-ols-constrained-mlm-model",
    "href": "hsb_ex.html#figure-4.3-scatter-plots-of-residuals-from-the-ols-constrained-mlm-model",
    "title": "29  HSB Example",
    "section": "29.12 Figure 4.3 : Scatter plots of residuals from the OLS & Constrained MLM model",
    "text": "29.12 Figure 4.3 : Scatter plots of residuals from the OLS & Constrained MLM model\n\n## Luke: Equation 4.271 and 4.27b (p. 92) are allegedly how we calculate the intercept and slope residuals \n## But I'm not sure where the estimates for the gamma-hat terms come from; the OLS model only includes\n## individual-level ses\n\n# trying it here with the predictions from conditional EB\nfes = fixef( mod4.5 )\nfes\n\n          (Intercept)               meanses                sector \n            12.095997              5.332898              1.226453 \n        ses_grpcenter meanses:ses_grpcenter  sector:ses_grpcenter \n             2.938785              1.038918             -1.642619 \n\nmod.comp = mutate( mod.comp,\n                   u0_ols = b0_ols - (fes[1] + fes[2]*meanses + fes[3]*sector),\n                   u1_ols = b1_ols - (fes[4] + fes[5]*meanses + fes[6]*sector)  )\n\n\n## Panel (a) and (b) plotted on same graph\n\nmod.comp = mutate( mod.comp, \n                   u0_cond = b0_cond - (fes[1] + fes[2]*meanses + fes[3]*sector),\n                   u1_cond = b1_cond - (fes[4] + fes[5]*meanses + fes[6]*sector)  )\n\nhead( mod.comp )\n\n    id meanses sector n_j    b0_ols    b1_ols se_b0_ols se_b1_ols b0_uncond\n1 1224  -0.428      0  47  9.715447 2.5085817 1.0954478  1.765216  9.956953\n2 1288   0.128      0  25 13.510800 3.2554487 1.3637656  2.079675 13.386036\n3 1296  -0.420      0  48  7.635958 1.0759591 0.7740752  1.209016  8.039091\n4 1308   0.534      1  20 16.255500 0.1260242 1.4045813  3.003437 15.622073\n5 1317   0.351      1  48 13.177688 1.2739128 0.7902486  1.435942 13.132771\n6 1358  -0.014      0  30 11.206233 5.0680087 0.8994345  1.391550 11.387452\n  b1_uncond   b0_cond  b1_cond      u0_ols      u1_ols     u0_cond      u1_cond\n1  2.262837  9.740146 2.489033 -0.09807014  0.01445354 -0.07337107 -0.005095579\n2  2.375964 13.234409 3.112156  0.73219201  0.18368221  0.45580075  0.040389349\n3  1.872247  8.145275 2.307720 -2.22022179 -1.42648036 -1.71090544 -0.194719195\n4  2.050193 16.201317 1.835985  0.08528223 -1.72492410  0.03109939 -0.014963432\n5  1.997129 13.663596 1.528623 -2.01661001 -0.38691354 -1.53070178 -0.132203128\n6  2.738390 11.530341 2.946721 -0.81510320  2.14376861 -0.49099553  0.022480378\n\nnrow( mod.comp )\n\n[1] 160\n\nggplot(data=mod.comp, aes( pch=as.factor(sector)) ) + \n         geom_point(aes(x=u1_ols, y=u0_ols),color='black', alpha=0.7) +   \n         geom_point(aes(x=u1_cond, y=u0_cond),color='blue', alpha=0.7) + \n         labs(title = \"Black: OLS, Blue: Conditional EB\") + \n         xlim(-6,6) + ylim(-8,8)\n\n\n\n# To get in two-panel format we need to get our data to long format\nmod.comp.ols = data.frame( sector = mod.comp$sector,\n                           u0 = mod.comp$u0_ols,\n                           u1 = mod.comp$u1_ols )\nmod.comp.EB = data.frame(  sector = mod.comp$sector,\n                           u0 = mod.comp$u0_cond,\n                           u1 = mod.comp$u1_cond )\nmod.comp.l = bind_rows( ols=mod.comp.ols, cond = mod.comp.EB, .id = \"method\" )\n\nggplot(data=mod.comp.l, aes( u1, u0, pch=as.factor(sector)) ) + \n  facet_wrap( ~ method ) +\n  geom_point()"
  },
  {
    "objectID": "hsb_ex.html#table-4.7-pg-94",
    "href": "hsb_ex.html#table-4.7-pg-94",
    "title": "29  HSB Example",
    "section": "29.13 Table 4.7 : pg 94",
    "text": "29.13 Table 4.7 : pg 94\n\n# This section is not very good--I would skip.\n# Generating confidence intervals for individual random intercepts and slopes is a weird business.\n\n# OLS First:\n\n# Doing it by fitting OLS on our subset\nsch.2305 = filter( dat, id == 2305 )\nhead( sch.2305 )\n\n# A tibble: 6 × 13\n# Groups:   id [1]\n  id    minority female    ses mathach  size sector pracad disclim himinty\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 2305         1      1 -0.738   16.4    485      1   0.69   -1.38       1\n2 2305         1      1 -1.18    12.8    485      1   0.69   -1.38       1\n3 2305         1      1 -0.308   15.3    485      1   0.69   -1.38       1\n4 2305         1      1 -0.358   12.7    485      1   0.69   -1.38       1\n5 2305         1      1 -1.52    10.2    485      1   0.69   -1.38       1\n6 2305         1      1 -0.518    8.94   485      1   0.69   -1.38       1\n# ℹ 3 more variables: meanses &lt;dbl&gt;, ses_grpcenter &lt;dbl&gt;, ses_centered &lt;dbl&gt;\n\nM.2305 = lm( mathach ~ ses_grpcenter, data=sch.2305 )\nM.2305\n\n\nCall:\nlm(formula = mathach ~ ses_grpcenter, data = sch.2305)\n\nCoefficients:\n  (Intercept)  ses_grpcenter  \n      11.1378        -0.7821  \n\nconfint( M.2305 )\n\n                  2.5 %    97.5 %\n(Intercept)    9.911824 12.363698\nses_grpcenter -2.665989  1.101767\n\nsch.8367 = filter( dat, id == 8367 )\nhead( sch.8367 )\n\n# A tibble: 6 × 13\n# Groups:   id [1]\n  id    minority female    ses mathach  size sector pracad disclim himinty\n  &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 8367         0      0 -0.228  15.9     153      0      0    1.79       0\n2 8367         0      1 -0.208   1.86    153      0      0    1.79       0\n3 8367         1      0  0.532  -2.83    153      0      0    1.79       0\n4 8367         0      1  0.662   2.12    153      0      0    1.79       0\n5 8367         0      0 -0.228   6.76    153      0      0    1.79       0\n6 8367         0      0 -1.08    0.725   153      0      0    1.79       0\n# ℹ 3 more variables: meanses &lt;dbl&gt;, ses_grpcenter &lt;dbl&gt;, ses_centered &lt;dbl&gt;\n\nM.8367 = lm( mathach ~ ses_grpcenter, data=sch.8367 )\nM.8367\n\n\nCall:\nlm(formula = mathach ~ ses_grpcenter, data = sch.8367)\n\nCoefficients:\n  (Intercept)  ses_grpcenter  \n       4.5528         0.2504  \n\nconfint( M.8367 )\n\n                  2.5 %   97.5 %\n(Intercept)    1.872974 7.232598\nses_grpcenter -3.431096 3.931845\n\n# Use SE from earlier to get confint\nest4.7 &lt;- mod.comp[c(22,135),]\nest4.7\n\n      id meanses sector n_j    b0_ols     b1_ols se_b0_ols se_b1_ols b0_uncond\n22  2305  -0.622      1  67 11.137761 -0.7821112 0.6138468  0.943289 11.222551\n135 8367   0.032      0  14  4.552786  0.2503748 1.2299413  1.689668  6.423938\n    b1_uncond   b0_cond   b1_cond    u0_ols    u1_ols    u0_cond     u1_cond\n22   1.149555 10.886600 0.6276417  1.132373 -1.432070  0.8812116 -0.02231763\n135  1.924903  8.549007 2.6307047 -7.713864 -2.721656 -3.7176433 -0.34132569\n\n# CI for intercept and slope using our normal and stored SEs.\n# (Not taking t distribution into account changes things, as does not\n# taking the uncertainty in the fixed effects for the EB CIs.  So this is\n# very approximate.)\nse_uncond = as.data.frame( se.coef(mod4.4)$id )\nhead( se_uncond )\n\n     (Intercept) ses_grpcenter\n1224   0.8464092     0.7189592\n1288   1.1205609     0.7593421\n1296   0.8382669     0.7111100\n1308   1.2307722     0.8005012\n1317   0.8382675     0.7377054\n1358   1.0354852     0.7489241\n\nnames( se_uncond ) = c(\"se_b0_uncond\",\"se_b1_uncond\" )\nse_cond = as.data.frame( se.coef(  mod4.5 )$id )\nnames( se_cond ) = c(\"se_b0_cond\",\"se_b1_cond\" )\nhead( se_cond )\n\n     se_b0_cond se_b1_cond\n1224  0.7662313  0.2929654\n1288  0.9521965  0.2988437\n1296  0.7601221  0.2923332\n1308  1.0176181  0.3025414\n1317  0.7603073  0.2940970\n1358  0.8982481  0.2971694\n\nse_uncond$id = rownames( se_uncond )\nse_cond$id = rownames( se_cond )\nest4.7 = merge( est4.7, se_uncond, by=\"id\" )\nest4.7 = merge( est4.7, se_cond, by=\"id\" )\n\nest4.7.int = mutate( est4.7, \n                 CI.low.ols = b0_ols + - 1.96 * se_b0_ols,\n                 CI.high.ols = b0_ols + 1.96 * se_b0_ols,\n                 CI.low.uncond = b0_uncond + - 1.96 * se_b0_uncond,\n                 CI.high.uncond = b0_uncond + 1.96 * se_b0_uncond,\n                 CI.low.cond = b0_cond + - 1.96 * se_b0_cond,\n                 CI.high.cond = b0_cond + 1.96 * se_b0_cond )\n\ndplyr::select( est4.7.int, starts_with(\"CI\" ) )\n\n  CI.low.ols CI.high.ols CI.low.uncond CI.high.uncond CI.low.cond CI.high.cond\n1   9.934621   12.340901      9.815648      12.629455    9.579800     12.19340\n2   2.142101    6.963471      3.642797       9.205078    6.361492     10.73652\n\nest4.7.slope = mutate( est4.7, \n                     CI.low.ols = b1_ols + - 1.96 * se_b1_ols,\n                     CI.high.ols = b1_ols + 1.96 * se_b1_ols,\n                     CI.low.uncond = b1_uncond + - 1.96 * se_b1_uncond,\n                     CI.high.uncond = b1_uncond + 1.96 * se_b1_uncond,\n                     CI.low.cond = b1_cond + - 1.96 * se_b1_cond,\n                     CI.high.cond = b1_cond + 1.96 * se_b1_cond )\n\ndplyr::select( est4.7.slope, starts_with(\"CI\" ) )\n\n  CI.low.ols CI.high.ols CI.low.uncond CI.high.uncond CI.low.cond CI.high.cond\n1  -2.630958    1.066735    -0.1675367       2.466647   0.0629627     1.192321\n2  -3.061375    3.562124     0.3960110       3.453795   2.0356645     3.225745"
  },
  {
    "objectID": "faraway_ex.html#r-setup",
    "href": "faraway_ex.html#r-setup",
    "title": "30  Faraway Example",
    "section": "30.1 R Setup",
    "text": "30.1 R Setup\n\nlibrary( arm )\nlibrary( ggplot2 )\nlibrary( plyr )\n\n# Install package from textbook to get the data by \n# running this line once.\n#install.packages( \"faraway\" )"
  },
  {
    "objectID": "faraway_ex.html#first-example",
    "href": "faraway_ex.html#first-example",
    "title": "30  Faraway Example",
    "section": "30.2 First Example",
    "text": "30.2 First Example\n\n# load the data\nlibrary(faraway)\ndata(psid)\nhead(psid)\n\n  age educ sex income year person\n1  31   12   M   6000   68      1\n2  31   12   M   5300   69      1\n3  31   12   M   5200   70      1\n4  31   12   M   6900   71      1\n5  31   12   M   7500   72      1\n6  31   12   M   8000   73      1\n\n# Make log-transform of income\npsid$log_income = with( psid, log( income + 100 ) )\n                            \n                            \n# Look at some plots\npsid.sub = subset( psid, person &lt; 21 )\nggplot( data=psid.sub, aes( x=year, y=income ) ) +\n    facet_wrap( ~ person ) +\n    geom_line()\n\n\n\nggplot( data=psid.sub, aes( x=year, y=log_income, group=person ) ) +\n    facet_wrap( ~ sex ) +\n    geom_line()\n\n\n\n# Simple regression on a single person\nlmod &lt;- lm( log_income ~ I(year-78), subset=(person==1), psid)\ncoef(lmod)\n\n (Intercept) I(year - 78) \n  9.40910950   0.08342068 \n\n# Now do linear regression on everyone\nsum.stat = ddply( psid, .(person), function( dat ) {\n    lmod &lt;- lm(log(income) ~ I(year-78), data=dat )\n    cc = coef(lmod)\n    names(cc) = c(\"intercept\",\"slope\")\n    c( cc, sex=dat$sex[[1]] )\n} )\nhead( sum.stat )\n\n  person intercept       slope sex\n1      1  9.399957  0.08426670   2\n2      2  9.819091  0.08281031   2\n3      3  7.893863  0.03131149   1\n4      4  7.853027  0.07585135   1\n5      5  8.033453 -0.04738677   1\n6      6  9.673443  0.08953380   2\n\nplot( slope ~ intercept, data=sum.stat, xlab=\"Intercept\",ylab=\"Slope\")\n\n\n\nboxplot( slope ~ sex, data=sum.stat )\n\n\n\n# Is rate of income growth different by sex?\nt.test( slope ~ sex, data=sum.stat )\n\n\n    Welch Two Sample t-test\n\ndata:  slope by sex\nt = 2.3786, df = 56.736, p-value = 0.02077\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n 0.00507729 0.05916871\nsample estimates:\nmean in group 1 mean in group 2 \n     0.08903346      0.05691046 \n\n# Is initial income different by sex?\nt.test( intercept ~ sex, data=sum.stat )\n\n\n    Welch Two Sample t-test\n\ndata:  intercept by sex\nt = -8.2199, df = 79.719, p-value = 3.065e-12\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -1.4322218 -0.8738792\nsample estimates:\nmean in group 1 mean in group 2 \n       8.229275        9.382325 \n\n# Fitting our model\nlibrary(lme4)\npsid$cyear &lt;- psid$year-78\nmmod &lt;- lmer(log(income) ~ cyear*sex + age + educ + (cyear|person), psid)\ndisplay(mmod)\n\nlmer(formula = log(income) ~ cyear * sex + age + educ + (cyear | \n    person), data = psid)\n            coef.est coef.se\n(Intercept)  6.67     0.54  \ncyear        0.09     0.01  \nsexM         1.15     0.12  \nage          0.01     0.01  \neduc         0.10     0.02  \ncyear:sexM  -0.03     0.01  \n\nError terms:\n Groups   Name        Std.Dev. Corr \n person   (Intercept) 0.53          \n          cyear       0.05     0.19 \n Residual             0.68          \n---\nnumber of obs: 1661, groups: person, 85\nAIC = 3839.8, DIC = 3751.2\ndeviance = 3785.5 \n\n# refit with the lmerTest library to get p-values\nlibrary( lmerTest )\nmmod &lt;- lmer(log(income) ~ cyear*sex + age + educ + (cyear|person), psid)\nsummary(mmod)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: log(income) ~ cyear * sex + age + educ + (cyear | person)\n   Data: psid\n\nREML criterion at convergence: 3819.8\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-10.2310  -0.2134   0.0795   0.4147   2.8254 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n person   (Intercept) 0.2817   0.53071      \n          cyear       0.0024   0.04899  0.19\n Residual             0.4673   0.68357      \nNumber of obs: 1661, groups:  person, 85\n\nFixed effects:\n             Estimate Std. Error        df t value Pr(&gt;|t|)    \n(Intercept)  6.674211   0.543323 81.176969  12.284  &lt; 2e-16 ***\ncyear        0.085312   0.008999 78.915123   9.480 1.14e-14 ***\nsexM         1.150312   0.121292 81.772542   9.484 8.06e-15 ***\nage          0.010932   0.013524 80.837433   0.808   0.4213    \neduc         0.104209   0.021437 80.722317   4.861 5.65e-06 ***\ncyear:sexM  -0.026306   0.012238 77.995359  -2.150   0.0347 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n           (Intr) cyear  sexM   age    educ  \ncyear       0.020                            \nsexM       -0.104 -0.098                     \nage        -0.874  0.002 -0.026              \neduc       -0.597  0.000  0.008  0.167       \ncyear:sexM -0.003 -0.735  0.156 -0.010 -0.011"
  },
  {
    "objectID": "faraway_ex.html#diagnostics",
    "href": "faraway_ex.html#diagnostics",
    "title": "30  Faraway Example",
    "section": "30.3 Diagnostics",
    "text": "30.3 Diagnostics\n\n# First add our residuals and fitted values to our original data\n# (We can do this since we have no missing data so the rows will line up\n# correctly)\npsid = transform( psid,  resid=resid( mmod ),\n                  fit = fitted( mmod ) )\nhead( psid )\n\n  age educ sex income year person log_income cyear       resid      fit\n1  31   12   M   6000   68      1   8.716044   -10  0.06719915 8.632316\n2  31   12   M   5300   69      1   8.594154    -9 -0.13201639 8.707478\n3  31   12   M   5200   70      1   8.575462    -8 -0.22622748 8.782641\n4  31   12   M   6900   71      1   8.853665    -7 -0.01852759 8.857804\n5  31   12   M   7500   72      1   8.935904    -6 -0.01030887 8.932967\n6  31   12   M   8000   73      1   8.999619    -5 -0.02093325 9.008130\n\n# Here is a qqplot for each sex\nggplot( data=psid ) +\n    facet_wrap( ~ sex ) +\n    stat_qq( aes( sample=resid ) )\n\n\n\n# If you want to add the lines, you have to do a little more work\nslopes = ddply( psid, .(sex), function( dat ) {\n    y &lt;- quantile(dat$resid, c(0.25, 0.75))\n    x &lt;- qnorm(c(0.25, 0.75))\n    slope &lt;- as.numeric( diff(y)/diff(x) )\n    int &lt;- y[[1]] - slope * x[[1]]\n    c( slope=slope, int=int )\n} )\nslopes\n\n  sex     slope        int\n1   F 0.4324568 0.10579138\n2   M 0.2473357 0.03321435\n\nggplot( data=psid ) +\n    facet_wrap( ~ sex ) +\n    stat_qq( aes( sample=resid ) ) +\n    geom_abline( data=slopes, aes( slope=slope, intercept=int ) )\n\n\n\n# This is doing it from the lattice pacage\nlibrary( lattice )\nqqmath(~resid(mmod) | sex, psid)\n\n\n\n# fancier with some lines.  The points should lie on the line\n# if we have normal residuals.  (We don't.)\nqqmath(~ resid(mmod)  | sex, data = psid,\n       panel = function(x, ...) {\n           panel.qqmathline(x, ...)\n           panel.qqmath(x, ...)\n       })\n\n\n\npsid$educ_levels = cut(psid$educ, c(0,8.5,12.5,20), labels=c( \"Less than HS\", \"HS\", \"Beyond HS\" ) )\nggplot( data=psid, aes( x=fit, y=resid ) ) +\n    facet_wrap( ~ educ_levels ) +\n    geom_point()"
  },
  {
    "objectID": "perf_ex.html#load-the-data",
    "href": "perf_ex.html#load-the-data",
    "title": "31  Example of a three-level model of clustered data",
    "section": "31.1 Load the data",
    "text": "31.1 Load the data\nWe first load the data. This is a dataset extensively discussed in Rabe-Hesketh and Skrondal. I am replicating the model they propose in chapter 8.4. This story is as follows: the data set is a collection of measurements for a test-retest of two peak expiratory flow measurement devices (in English, patients were told to exhale into a device to measure their lung capacity, and they did so twice for two different measurement devices). We want to understand whether the types of meter are different, and also understand variation in subjects lung capacities, and variation in the measurement error of the meters.\nIn the following we load the data and look at the first few lines. We see that each subject had two measurements from the standard and the mini Wright flow meter.\n\npefr = read.dta( \"data/pefr.dta\" )\n\nhead( pefr )\n\n  id wp1 wp2 wm1 wm2\n1  1 494 490 512 525\n2  2 395 397 430 415\n3  3 516 512 520 508\n4  4 434 401 428 444\n5  5 476 470 500 500\n6  6 557 611 600 625\n\n\nWe are going to view this as three-level data. We have multiple measurements nested inside device type nested inside subject. We might imagine that different subjects have different lung capacities. We also might imagine that different subjects are going to have different biases when using the two different meters. The two observations for each meter allows us to understand the variability of measurements for a single meter for a given subject, and looking at how these vary across subjects allows us to understand how much the biases move across individuals."
  },
  {
    "objectID": "perf_ex.html#reshape-the-data-optional-section",
    "href": "perf_ex.html#reshape-the-data-optional-section",
    "title": "31  Example of a three-level model of clustered data",
    "section": "31.2 Reshape the data (Optional section)",
    "text": "31.2 Reshape the data (Optional section)\nThis section illustrates some advanced reshaping techniques. In particular we reshape the data twice to deal with the time and the device as different levels.\nHere we go:\n\ndat = reshape( pefr, direction=\"long\", idvar = \"id\", \n               varying=list( c(\"wp1\",\"wm1\"), c(\"wp2\",\"wm2\") ),\n               times=c(\"wp\",\"wm\"),\n               timevar=\"device\",\n               v.names=c(\"time1\",\"time2\") )\n\nLet’s see what we got:\n\nhead( dat )\n\n     id device time1 time2\n1.wp  1     wp   494   490\n2.wp  2     wp   395   397\n3.wp  3     wp   516   512\n4.wp  4     wp   434   401\n5.wp  5     wp   476   470\n6.wp  6     wp   557   611\n\nsubset( dat, id==1 )\n\n     id device time1 time2\n1.wp  1     wp   494   490\n1.wm  1     wm   512   525\n\n\nThe second line above shows us our first person now as two new lines, one for each device. We see the measurements correspond to the first row of the original pefr data.\nNow we have a row for each person for each device. Next we unstack the time (and then look at what we got):\n\ndat = reshape( dat, direction=\"long\", idvar=c(\"id\",\"device\"),\n               varying=list( c(\"time1\",\"time2\") ),\n               v.names=c(\"flow\") )\nhead( dat )\n\n       id device time flow\n1.wp.1  1     wp    1  494\n2.wp.1  2     wp    1  395\n3.wp.1  3     wp    1  516\n4.wp.1  4     wp    1  434\n5.wp.1  5     wp    1  476\n6.wp.1  6     wp    1  557\n\n\nWe look at our second person to see if the measurements have the appropriate labels. They do.\n\nsubset( dat, id==2 )\n\n       id device time flow\n2.wp.1  2     wp    1  395\n2.wm.1  2     wm    1  430\n2.wp.2  2     wp    2  397\n2.wm.2  2     wm    2  415\n\nsubset( pefr, id==2 )\n\n  id wp1 wp2 wm1 wm2\n2  2 395 397 430 415\n\n\nAnother sanity check:\n\ntable( dat$id )\n\n\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 \n 4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4 \n\n\nWe have four measurements, still, for each person.\nWhen reshaping data, one typically has to fiddle with all of the commands and check the results a few times to get it right."
  },
  {
    "objectID": "perf_ex.html#plot-the-data",
    "href": "perf_ex.html#plot-the-data",
    "title": "31  Example of a three-level model of clustered data",
    "section": "31.3 Plot the data",
    "text": "31.3 Plot the data\nWe can look at the data. The following illustrates getting different colors and symbols depending on covariate information:\n\ndat$id = as.factor( dat$id )\ndat$device = as.factor( dat$device )\ndat$time = as.factor( dat$time )\nggplot( data=dat, aes( x=id, y=flow, col=device, pch=time ) ) + \n    geom_jitter( width=0.2, height=0 )"
  },
  {
    "objectID": "perf_ex.html#the-mathematical-model",
    "href": "perf_ex.html#the-mathematical-model",
    "title": "31  Example of a three-level model of clustered data",
    "section": "31.4 The mathematical model",
    "text": "31.4 The mathematical model\nLevel 1: We have for individual \\(i\\) using machine \\(j\\) at time \\(t\\): [ Y_{ijt} = +* t + _{ijt} ] The \\(\\beta_{1}\\) allows for a time effect of the second measurement being systematically lower or higher than the first. We pool this across all subjects and machines.\nLevel 2: Our machine-level intercepts for each subject are [ =* + D_j + u*{ij} ] with \\(D_j = 1\\{ j = wp \\}\\) being an indicator (dummy variable) for the second machine. The \\(\\gamma_1\\) allows a systematic bias for the two machines (so the wp machine could tend to give larger readings than the wm machine, for example). Overall, the above says each machine expected reading varies around the subject’s lung capacity, but that these expected readings will vary around the subjects true capacity by the \\(u_{ij}\\). Actual readings for subject \\(i\\) on machine \\(j\\) will hover around \\(\\beta_{ij}\\) if we had the subject test over and over, according to our model (not including fatigue captured by the time coefficient).\nLevel 3: Finally our subject intercepts are [ =* + w{i} . ] The overall population lung capacity is \\(\\mu\\). Subjects have larger or smaller lung capacity depending on their \\(w_{i}\\).\nThe \\(u_{ij}\\) and \\(w_i\\) are each normally distributed, and independent from each other.\nThe \\(w_i\\) are how the subjects vary (i.e., their different lung capacities). The \\(u_{ij}\\) are the individual biases of a machine for a given subject. Looking at our plot, we see that subjects vary a lot, and machines vary sometimes within a subject (the centers of the pairs of colored points tend to be close, but not always), and the residual variance tends to be small (colored points are close together)."
  },
  {
    "objectID": "perf_ex.html#fit-the-model",
    "href": "perf_ex.html#fit-the-model",
    "title": "31  Example of a three-level model of clustered data",
    "section": "31.5 Fit the model",
    "text": "31.5 Fit the model\n\nlibrary( lme4 )\nM1 = lmer( flow ~ device + time + (1|id) + (1|device:id), data=dat )\ndisplay( M1 )\n\nlmer(formula = flow ~ device + time + (1 | id) + (1 | device:id), \n    data = dat)\n            coef.est coef.se\n(Intercept) 454.43    27.84 \ndevicewp     -6.03     8.05 \ntime2        -1.03     4.37 \n\nError terms:\n Groups    Name        Std.Dev.\n device:id (Intercept)  19.72  \n id        (Intercept) 111.99  \n Residual               18.01  \n---\nnumber of obs: 68, groups: device:id, 34; id, 17\nAIC = 682.8, DIC = 709.1\ndeviance = 689.9 \n\n\nNow let’s connect some pieces:\n\nThe main effects estimate \\(\\mu = 455.46\\) and \\(\\gamma_1 = -6.03\\) and \\(\\beta_1 = -1.03\\).\nThe z-score of \\(z = -6.03 / 8.05 &lt; 1\\) means there is no evidence of systematic bias of one machine compared to the other.\nThe estimated standard deviation of actual lung capacity is 112.\nThe estimated standard deviation of how two different machines will measure the same person is \\(19.72\\). Different machines will tend to give different average measurements for the same subject.\nThe estimated standard deviation of how much a repeated measurement of the same machine on the same person will vary is 18. The machines are relatively precise, given the variation in the population.\nThe amount of variance explained by lung variation is \\(112^2 / (19.72^2 + 111.99^2 + 18.01^2) = 0.94636\\), i.e., most of it."
  },
  {
    "objectID": "perf_ex.html#appendix-optional-base-plot-package",
    "href": "perf_ex.html#appendix-optional-base-plot-package",
    "title": "31  Example of a three-level model of clustered data",
    "section": "31.6 Appendix (Optional): base plot package",
    "text": "31.6 Appendix (Optional): base plot package\nHere is how to build the plot without ggplot:\n\nplot( flow ~ as.numeric(id), data=dat, pch=ifelse( time==1, 21, 22 ),\n      col=ifelse( device==\"wp\", \"red\", \"green\" ) )\nlegend( \"bottomleft\", legend=c(\"WP-1\", \"WP-2\",\"WM-1\", \"WM-2\"),\n        pch=c(21,22,21,22),\n        col=c(\"red\",\"red\",\"green\",\"green\") )"
  },
  {
    "objectID": "kenya_ex.html#load-the-data",
    "href": "kenya_ex.html#load-the-data",
    "title": "32  Example of a three-level longitudinal model",
    "section": "32.1 Load the data",
    "text": "32.1 Load the data\nWe first load the data. Shoving a lot of things under the rug, we have five measurements on a collection of kids in Kenya across time. We are interested in the impact of improved nutrition. The children are clustered in schools. This gives a three-level structure. The schools were treated with different nutrition programs.\nIn the following we load the data and look at the first few lines. Lots of variables! The main ones are id (the identifier of the kid), treatment (the kind of treatment given to the school), schoolid (the identifier of the school), gender (the gender of the kid), and rn (the time variable). Our outcome is ravens (Raven’s colored progressive matrices asssessment).\n\nkenya = read.dta( \"data/kenya.dta\" )\n\n# look at first 9 variables\nhead( kenya[1:9], 3 )\n\n  id schoolid rn relyear ravens arithmetic vmeaning dstotal age_at_time0\n1  1        2  1   -0.15     15          5       25       6         7.19\n2  1        2  2    0.14     19          7       39       8         7.19\n3  1        2  3    0.46     21          7       33       7         7.19\n\n# what times do we have?\ntable( kenya$rn ) #time\n\n\n  1   2   3   4   5 \n546 546 546 546 546 \n\nlength( unique( kenya$id ) )\n\n[1] 546\n\nlength( unique( kenya$schoolid) )\n\n[1] 12\n\n\nWe see we have 546 kids and 12 schools."
  },
  {
    "objectID": "kenya_ex.html#plot-the-data",
    "href": "kenya_ex.html#plot-the-data",
    "title": "32  Example of a three-level longitudinal model",
    "section": "32.2 Plot the data",
    "text": "32.2 Plot the data\nWe can look at the data.\n\nggplot( data=kenya, aes( x=rn, y=ravens, group=id )  )+ \n            facet_wrap( ~ gender ) + \n            geom_line( alpha=0.3 )\n\nWarning: Removed 114 rows containing missing values (`geom_line()`).\n\n\n\n\n\nor\n\nggplot( data=kenya, aes( x=rn, y=ravens, group=id )  )+ \n            facet_wrap( ~ schoolid ) + \n            geom_line( alpha=0.3 )\n\nWarning: Removed 114 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\nid.sub = sample( unique( kenya$id), 12 )\nken.sub = subset( kenya, id %in% id.sub )\nggplot( data=ken.sub, aes( x=rn, y=ravens, group=id )  )+ \n            facet_wrap( ~ id ) + \n            geom_line( alpha=0.3 )\n\nWarning: Removed 5 rows containing missing values (`geom_line()`).\n\n\n\n\n\nWe have lots of noise! But there is also a trend.\nThe progression of marginal means show there is growth over time, on average:\n\nmosaic::favstats( ravens ~ rn, data=kenya )\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\n\n  rn min Q1 median Q3 max mean   sd   n missing\n1  1   1 16     17 19  28 17.3 2.56 537       9\n2  2   0 16     17 19  28 17.7 2.79 529      17\n3  3   0 16     18 20  30 18.3 3.04 523      23\n4  4   4 17     18 20  30 18.6 2.97 513      33\n5  5   7 17     19 21  31 19.5 3.10 496      50\n\n\n(Using the mosaic package we can do this.)\nThe above also shows that we have some missing data, more as the study progresses.\nWe drop these missing observations:\n\nkenya = subset( kenya, !is.na( ravens ) & !is.na( rn ) )\n\nWe have some treatments, which we order so control is first\n\nstr( kenya$treatment )\n\n Factor w/ 4 levels \"meat\",\"milk\",..: 1 1 1 1 1 4 4 4 4 4 ...\n\nlevels( kenya$treatment )\n\n[1] \"meat\"    \"milk\"    \"calorie\" \"control\"\n\nkenya$treatment = relevel( kenya$treatment, ref = \"control\" )\nlevels( kenya$treatment )\n\n[1] \"control\" \"meat\"    \"milk\"    \"calorie\""
  },
  {
    "objectID": "kenya_ex.html#the-mathematical-model",
    "href": "kenya_ex.html#the-mathematical-model",
    "title": "32  Example of a three-level longitudinal model",
    "section": "32.3 The mathematical model",
    "text": "32.3 The mathematical model\nLet’s fit a random slope model.\nLevel 1: We have for individual \\(i\\) in school \\(j\\) at time \\(t\\): [ Y_{ijt} = +* (t-L) + _{ijt} ]\nLevel 2: Each individual has their own growth curve. Their curve’s slope and intercepts varies around the school means: [ =* + gender{ij} + u_{0ij} ] [ = + gender*{ij} + u_{1ij} ] We also have that \\((u_{0ij}, u_{1ij})\\) are normally distributed with some 2x2 covariance matrix. We are forcing the impact of gender to be constant across schools.\nLevel 3: Finally our school mean slope and intercepts are [ =* + w_{0i} ] [ =* + meat_j +* milk_j + calorie_j + w*{1i} ] For the rate of growth at a school we allow different slopes for different treatments (compared to baseline). The milk, meat, and calorie are the three different treatments applied. We also have that \\((w_{0j}, w_{1j})\\) are normally distributed with some 2x2 covariance matrix: [\n\\[\\begin{pmatrix}w_{j0}\\\\\nw_{j1}\n\\end{pmatrix}\\]\nN((\n\\[\\begin{array}{c}\n0 \\\\\n0\n\\end{array}\\]\n), ) = N((\n\\[\\begin{array}{c}\n0 \\\\\n0\n\\end{array}\\]\n), _{sch} ) ]\nThe \\(\\mu_0\\) and \\(\\mu_1\\) are the slope and intercept for the overall population growth (this is what defines our marginal model).\nWe will use \\(L = 1\\) to center the data at the first time point (so our intercept is expected ravens score at onset of the study).\nConceptual question: Why do we not have treatment in the intercept for school? What would changing \\(L\\) do to our model and this reasoning?"
  },
  {
    "objectID": "kenya_ex.html#fit-the-model",
    "href": "kenya_ex.html#fit-the-model",
    "title": "32  Example of a three-level longitudinal model",
    "section": "32.4 Fit the model",
    "text": "32.4 Fit the model\n\nlibrary( lme4 )\nkenya$rn = kenya$rn - 1 # center by L=1\nM1 = lmer( ravens ~ 1 + rn + gender*rn + treatment:rn + (1+rn|schoolid) + (1+rn|id:schoolid), \n           data=kenya )\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nunable to evaluate scaled gradient\n\n\nWarning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, :\nModel failed to converge: degenerate Hessian with 1 negative eigenvalues\n\ndisplay( M1 )\n\nlmer(formula = ravens ~ 1 + rn + gender * rn + treatment:rn + \n    (1 + rn | schoolid) + (1 + rn | id:schoolid), data = kenya)\n                    coef.est coef.se\n(Intercept)         17.41     0.19  \nrn                   0.59     0.08  \ngendergirl          -0.30     0.20  \nrn:gendergirl       -0.14     0.08  \nrn:treatmentmeat     0.17     0.09  \nrn:treatmentmilk    -0.13     0.09  \nrn:treatmentcalorie -0.02     0.09  \n\nError terms:\n Groups      Name        Std.Dev. Corr  \n id:schoolid (Intercept) 1.40           \n             rn          0.43     -0.09 \n schoolid    (Intercept) 0.45           \n             rn          0.09     -1.00 \n Residual                2.31           \n---\nnumber of obs: 2598, groups: id:schoolid, 546; schoolid, 12\nAIC = 12545.9, DIC = 12474\ndeviance = 12496.0 \n\n\nNow let’s connect some pieces:\n\n\\(\\mu_{00} = 17.41\\) and \\(\\mu_{11} = 0.59\\). The initial score for boys is 17.4, on average, with an average gain of 0.59 per year for control schools.\n\\(\\gamma_{01} = -0.30\\) and \\(\\gamma_{11} = -0.14\\), giving estimates that girls score lower and gain slower than boys.\nThe school-level variation in initial expected Raven scores is 0.45 (this is the standard deviation of \\(w_{0i}\\)), relatively small compared to the individual variation of 1.40 (this is the standard deviation of \\(u_{0ij}\\)).\nThe correlation of the \\(u_{0ij}\\) and \\(u_{1ij}\\) is basically zero (estimated at -0.09).\nThe random effects for school has a covariance matrix \\(\\Sigma_{sch}\\) of [ _{sch} = ] The very negative correlation suggests an extrapolation effect, and that perhaps we could drop the random slope for schools.\nThe treatment effects are estimated as \\(\\mu_{11}=0.17, \\mu_{12}=-0.13\\), and \\(\\mu_{13}=-0.02\\).\n\nP-values for these will not be small, however, as the standard errors are all 0.09.\n\nWe could try to look at uncertainty on our parameters using the confint( M1 ) command, but it turns out that it crashes for this model. This can happen, and our -0.99 correlation gives a hint as to why. Let’s first drop the random slope and then try:\n\nM1B = lmer( ravens ~ rn + gender*rn + treatment:rn + (1|schoolid) + (1+rn|id:schoolid), \n           data=kenya )\ndisplay( M1B )\n\nlmer(formula = ravens ~ rn + gender * rn + treatment:rn + (1 | \n    schoolid) + (1 + rn | id:schoolid), data = kenya)\n                    coef.est coef.se\n(Intercept)         17.39     0.17  \nrn                   0.57     0.08  \ngendergirl          -0.30     0.20  \nrn:gendergirl       -0.14     0.08  \nrn:treatmentmeat     0.22     0.10  \nrn:treatmentmilk    -0.09     0.10  \nrn:treatmentcalorie  0.02     0.10  \n\nError terms:\n Groups      Name        Std.Dev. Corr  \n id:schoolid (Intercept) 1.42           \n             rn          0.44     -0.11 \n schoolid    (Intercept) 0.33           \n Residual                2.31           \n---\nnumber of obs: 2598, groups: id:schoolid, 546; schoolid, 12\nAIC = 12544.4, DIC = 12478\ndeviance = 12498.9 \n\nconfint( M1B )\n\n                      2.5 %   97.5 %\n.sig01               1.1657  1.65436\n.sig02              -0.3544  0.32878\n.sig03               0.3075  0.54180\n.sig04               0.0000  0.60033\n.sigma               2.2312  2.39580\n(Intercept)         17.0605 17.71696\nrn                   0.4091  0.72814\ngendergirl          -0.6870  0.09145\nrn:gendergirl       -0.2879  0.00772\nrn:treatmentmeat     0.0164  0.40811\nrn:treatmentmilk    -0.2876  0.09811\nrn:treatmentcalorie -0.1775  0.20453\n\n\nWe then have to puzzle out which confidence interval goes with what. The .sig01 is the variance of the kid (id:schoolid), which we can tell by the range it covers. Then the next must be correlation, and then the slope. This tells us we have no confidence the school random intercept is away from 0 (.sig04)."
  },
  {
    "objectID": "kenya_ex.html#some-quick-plots",
    "href": "kenya_ex.html#some-quick-plots",
    "title": "32  Example of a three-level longitudinal model",
    "section": "32.5 Some quick plots",
    "text": "32.5 Some quick plots\nWe can look at the empirical bayes intercepts:\n\nschools = data.frame( resid = ranef( M1 )$schoolid$`(Intercept)` )\nkids = data.frame( resid = ranef( M1 )$id$`(Intercept)` )\nresid = data.frame( resid = resid( M1 ) )\nresids = bind_rows( school=schools, child=kids, residual=resid, .id=\"type\" )\nresids$type = factor( resids$type, levels = c(\"school\",\"child\", \"residual\" ) )\n\nggplot( resids, aes( x = type, y = resid ) ) +\n  geom_boxplot() +\n  coord_flip()\n\n\n\n\nThis shows that the variation in occasion is much larger than kid, which is much larger than school.\nWe can calculate all the individual growth curves and plot those:\n\nkenya$predictions = predict( M1 )\nggplot( data=kenya, aes( x=rn, y=predictions, group=id ) ) +\n    facet_wrap( ~ schoolid, labeller = label_both) +\n    geom_line( alpha=0.3 )\n\n\n\n\nGenerally individual curves are estimated to have positive slopes. The schools visually look quite similar; any school variation is small compared to individual variation."
  },
  {
    "objectID": "derivations.html",
    "href": "derivations.html",
    "title": "Derivations and Technical Details",
    "section": "",
    "text": "This section contains derivations and other technical details."
  },
  {
    "objectID": "icc_derivation.html",
    "href": "icc_derivation.html",
    "title": "37  ICC Derivation",
    "section": "",
    "text": "Often, the ICC is described as the estimated correlation between pairs of points drawn from the same cluster. While you can look at the visualizer to get some intuition on what this means, here is a short proof adapted from S52 materials.\nConsider the variance components model:\n\\[\ny_{ij} = \\beta_0 + \\zeta_j + \\varepsilon_{ij}\n\\]\nThe correlation between an observation \\(y\\) and an observation from the same group \\(y'\\) is the standardized covariance:\n\\[\n\\rho(y, y') = \\frac{cov(y,y')}{\\sqrt{var(y)var(y')}}\n\\]\nWe can expand the numerator, the covariance between \\(y\\) and \\(y'\\) and substitute in the definition of \\(y\\) from our model:\n\\[\ncov(y,y') = cov(\\beta_0 + \\zeta_j + \\varepsilon_{ij}, \\beta_0 + \\zeta_j + \\varepsilon'_{ij})\n\\]\nBy definition, \\(\\beta_0\\) is the same for everyone (i.e., the “constant” term), and \\(\\zeta_j\\) will be the same for both observations because we are looking within a single cluster. The only difference between the two groups are the individual level error terms, \\(\\varepsilon_{ij}\\). The rules of covariance tell us that the constant drops out and the \\(\\varepsilon\\) too because it is independent of \\(\\zeta_j\\), we can simplify our equation:\n\\[\ncov(y,y') = cov(\\zeta_j, \\zeta_j)\n\\]\nThe covariance of a variable with itself is the variance:\n\\[\ncov(y,y') = cov(\\zeta_j, \\zeta_j) = var(\\zeta_j) = \\sigma^2_\\zeta\n\\]\nConceptually, the \\(\\zeta_j\\) represents the shared influences on \\(y\\) that would cause the similarity between observations in the same group.\nWe know from our variance decomposition in the ICC formula that \\(var(y)\\) is the sum of the between-group and within-group variance components:\n\\[\nvar(y) = var(y') = \\sigma^2_\\zeta + \\sigma^2_\\varepsilon\n\\]\nWe can substitute these quantities into the original formula:\n\\[\n\\rho(y, y') = \\frac{cov(y,y')}{\\sqrt{var(y)var(y')}} = \\frac{\\sigma^2_\\zeta}{\\sigma^2_\\zeta + \\sigma^2_\\varepsilon} = ICC\n\\] Thus, the ICC is both the proportion of total variance accounted for by group membership and the correlation between pairs of observations drawn from the same group. QED!"
  },
  {
    "objectID": "cov_matrix_derivation.html#calculating-the-delta_tt",
    "href": "cov_matrix_derivation.html#calculating-the-delta_tt",
    "title": "38  Covariance Derivation",
    "section": "Calculating the \\(\\delta_{tt'}\\)",
    "text": "Calculating the \\(\\delta_{tt'}\\)\nLet’s calculate \\(\\delta_{13} = cov( \\epsilon_{i1}, \\epsilon_{i2} )\\).\nFirst we need a math fact about random quantities \\(A\\), \\(B\\), and \\(C\\): \\[cov( A + B, C ) = cov( A, C ) + cov( B, C ) .\\] Also if you multiply something by a constant \\(k\\) you have \\[cov( k_1 A, k_2 B ) = k_1 k_2 cov( A, B ) .\\]\nAlso note that \\(a_1 = 0\\) and \\(a_3 = 2\\). Then we have: \\[\\begin{aligned}\n\\delta_{13} &= cov( \\epsilon_{i1}, \\epsilon_{i3} ) \\\\\n   &= cov(  u_{0i} + u_{1i} a_1 + \\tilde{\\epsilon}_{1i},  u_{0i} + u_{1i} a_3 + \\tilde{\\epsilon}_{3i} ) \\\\\n   &= cov(  u_{0i}  + \\tilde{\\epsilon}_{1i},  u_{0i} + 2 u_{1i} + \\tilde{\\epsilon}_{3i} ) \\\\\n   &= cov(  u_{0i}, u_{0i} ) + cov( u_{0i}, 2 u_{1i} ) + cov( u_{0i}, \\tilde{\\epsilon}_{3i} ) + cov( \\tilde{\\epsilon}_{1i}, u_{0i}) + cov( \\tilde{\\epsilon}_{1i}, 2 u_{1i} )  + cov( \\tilde{\\epsilon}_{1i}, \\tilde{\\epsilon}_{3i}) \\\\\n   &= \\tau_{00} + 2\\tau_{01} + 0 + 0 + 0 + 0 \\\\\n   &= \\tau_{00} + 2\\tau_{01}\n\\end{aligned}\\]\nNote how we multiple out the individual components, and this gives an expression for the overall covariance of our two residuals. If we did this for each \\(\\delta_{tt'}\\) we could fill in our \\(5 \\times 5\\) matrix. Fun!\nA core idea here is the independence of the different residual pieces makes a lot of the terms go to 0, giving short(er) expressions than we might have otherwise. The random slope model dictates the overall covariance of the residuals."
  },
  {
    "objectID": "cov_matrix_derivation.html#calculating-the-diagonal-terms.",
    "href": "cov_matrix_derivation.html#calculating-the-diagonal-terms.",
    "title": "38  Covariance Derivation",
    "section": "38.1 Calculating the diagonal terms.",
    "text": "38.1 Calculating the diagonal terms.\nFor the variances, you would just calculate covariance of a quantity with itself. Let’s do \\(\\delta_{11}\\), the variance of timepoint 1: \\[\\begin{aligned}\n\\delta_{11} &= var( \\epsilon_{i1} ) = cov( \\epsilon_{i1}, \\epsilon_{i1} ) \\\\\n   &= cov(  u_{0i} + u_{1i} a_1 + \\tilde{\\epsilon}_{1i},  u_{0i} + u_{1i} a_1 + \\tilde{\\epsilon}_{1i} ) \\\\\n   &= cov(  u_{0i}  + \\tilde{\\epsilon}_{1i},  u_{0i} + \\tilde{\\epsilon}_{1i} ) \\\\\n   &= cov(  u_{0i}, u_{0i} ) + cov( u_{0i},  \\tilde{\\epsilon}_{1i} ) + cov( \\tilde{\\epsilon}_{1i}, u_{0i}) + cov( \\tilde{\\epsilon}_{1i},\\tilde{\\epsilon}_{1i} )  \\\\\n   &= \\tau_{00} + 0 + 0 + \\sigma^2 =  \\tau_{00} + \\sigma^2\n\\end{aligned}\\]\nNow let’s do \\(\\delta_{55}\\), the variance of timepoint 5: \\[\\begin{aligned}\n\\delta_{55} &= var( \\epsilon_{i5} ) = cov( \\epsilon_{i5}, \\epsilon_{i5} ) \\\\\n   &= cov(  u_{0i} + u_{1i} a_5 + \\tilde{\\epsilon}_{5i},  u_{0i} + u_{5i} a_5 + \\tilde{\\epsilon}_{5i} ) \\\\\n   &= cov(  u_{0i} + 4 u_{1i} + \\tilde{\\epsilon}_{5i},  u_{0i} + 4 u_{1i} + \\tilde{\\epsilon}_{5i} ) \\\\\n   &= cov(  u_{0i}, u_{0i} ) + cov(  u_{0i}, 4 u_{1i} )  + cov( u_{0i},  \\tilde{\\epsilon}_{5i} ) + \\\\\n    &\\qquad cov( 4 u_{1i}, u_{0i} ) + cov( 4 u_{1i}, 4 u_{1i} )  + cov( 4 u_{1i}, \\tilde{\\epsilon}_{5i} ) \\\\\n    & \\qquad cov( \\tilde{\\epsilon}_{1i}, u_{0i}) + cov( \\tilde{\\epsilon}_{1i}, 4 u_{1i} )  + cov( \\tilde{\\epsilon}_{1i},\\tilde{\\epsilon}_{1i} )  \\\\\n   &= \\tau_{00} + 4 \\tau_{01} + 0 + 4 \\tau_{01} + 16 \\tau_{11} + 0 + 0 + 0 + \\sigma^2 \\\\\n   &= \\tau_{00} + 16 \\tau_{11} + 8 \\tau_{01} + \\sigma^2 .\n\\end{aligned}\\]\nNote how the variance around the intercept (at time 1 where \\(a_1 = 0\\)) looks like it would be smaller than far out. That being said, the covariance \\(\\tau_{01}\\) could be large and negative, causing the variance at the intercept to be less. But, \\(\\tau_{01}\\) is positive, the overall variance increases as we move away from the intercept point.\nOne interesting aspect of random slope models is the marginal (at each time point) has heteroskedasticity: the variances are each time point can be different because the lines can spread or gather."
  },
  {
    "objectID": "inflated_variance.html",
    "href": "inflated_variance.html",
    "title": "39  Inflated Variance Derivation",
    "section": "",
    "text": "Say we want to estimate the variability of mean math achievement across schools. I.e., each school has some average math achievement of its students, and we want to know how different schools are.\nThe naïve way of doing this is to estimate the mean math achievement for a sample of schools, and take the standard deviation (square root of variance) of this sample as a reasonable estimate. In math terms, we would calculate \\(\\bar{Y}_j\\) for each school \\(j\\) and then use as our estimate \\[\\widehat{\\tau^2} = var( \\bar{Y}_j ) = \\frac{1}{n-1} \\sum_{j=1}^J \\bar{Y}_j .\\] This will give a number that is too big. The following is a math derivation on a simple scenario that illustrates why.\nFirst, pretend our Data Generation Process (DGP) is Mother Nature making a bunch of schools, and then for each school making a bunch of kids. Our model is that the schools are represented by school-level true mean math achievement, and the kids are made by adding an individual kid effect to the mean math achievement of their schools.\nSo we have \\[\\alpha_j \\sim N( \\mu, \\tau^2 )\\] meaning each school is a random draw from a normal distribution with a mean \\(\\mu\\) and a standard deviation \\(\\tau\\). These are the true means of the schools. We wish we knew them, but we do not. Instead we see a sample of kids from the school and we hope the mean of the kids is close to this true mean \\(\\alpha_j\\).\nFor any kid \\(i\\) we have \\[Y_i  = \\alpha_{j[i]} + \\epsilon_i\\] with \\[\\epsilon_i \\sim N( 0, \\sigma^2 ).\\] These \\(\\epsilon_i\\) are the classic residuals we are used to.\nFor the moment, assume each school \\(j\\) has \\(n\\) kids. Then the average observed math achievement is \\[\\bar{Y}_j = \\frac{1}{n} \\sum_{i : j[i] = j} Y_i ,\\] the average of all kids in the school. Note the “\\(i : j[i] = j\\)” term, which reads as “\\(i\\) for those \\(i\\) where \\(j[i] = j\\)” meaning “sum over all students which go to school \\(j\\).”\nOk, so now we have math achievement for school \\(j\\). We then have \\[\\bar{Y}_j = \\frac{1}{n} \\sum_{i : j[i] = j} Y_i  = \\frac{1}{n} \\sum_{i : j[i] = j} \\alpha_{j[i]} + \\epsilon_i =   \\frac{1}{n} \\sum_{i : j[i] = j} \\alpha_j +  \\frac{1}{n} \\sum_{i : j[i] = j} \\epsilon_i = \\alpha_j +  \\frac{1}{n} \\sum_{i : j[i] = j}\\epsilon_i = \\alpha_j + \\bar{\\epsilon}_j .\\] Here we have \\(\\bar{\\epsilon}_j = \\bar{Y}_j - \\alpha_j\\), i.e., we have a school-level residual, the error in our estimate of \\(\\alpha_j\\) using \\(\\bar{Y}_j\\). This residual is the sum of a bunch of student residuals, which we assume are all independent of each other. When you average a bunch of independent, identically distributed (i.i.d.) residuals, each with variance \\(\\sigma^2\\), you get something which still has the same mean (of 0) but a smaller variance by a factor of \\(n\\): \\[\\var{ \\bar{\\epsilon_j} } = \\var{  \\frac{1}{n} \\sum_{i : j[i] = j}\\epsilon_i } = \\frac{1}{n^2} \\sum_{i : j[i] = j} \\var{ \\epsilon_i } =  \\frac{1}{n} \\sigma^2\\] This is the familiar result that the mean of a bunch of variables has a standard deviation \\(1/\\sqrt{n}\\) of the original standard deviation (part of the Central Limit Theorem).\nWe can think of \\(\\bar{Y}_j\\) as a random quantity, random for two reasons: school \\(j\\) is a randomly made school, and the students in school \\(j\\) are randomly made students. Under the assumption that the students’ error terms are independent of the school’s mean math achievement we can easily calculate the variance of our estimator: \\[\\var{ \\bar{Y}_j } = \\var{ \\alpha_j } + \\var{ \\bar{\\epsilon}_j } =   \\tau^2 +  \\frac{1}{n} \\sigma^2\\]\nThis is bigger than our target of \\(\\tau^2\\), the true variability in mean math achievement across schools. The uncertainty in estimating the \\(\\alpha_j\\) has entered into the variability.\nOur estimate \\(\\widehat{\\tau^2}\\) will be an unbiased estimate of \\(\\tau^2 + \\frac{1}{n} \\sigma^2\\). One way to fix is to estimate \\(\\sigma^2\\) and then adjust our estimate of the variance of \\(\\tau^2\\) by subtracting \\(\\frac{1}{n} \\hat{\\sigma}^2\\). Another is to use multilevel modeling, which does this for us, in effect."
  },
  {
    "objectID": "pooling.html#pooledunpooled-v.s.-fixedrandom-effects",
    "href": "pooling.html#pooledunpooled-v.s.-fixedrandom-effects",
    "title": "40  Pooling",
    "section": "40.1 Pooled/unpooled v.s. fixed/random effects",
    "text": "40.1 Pooled/unpooled v.s. fixed/random effects\nYou may have noticed that we use a couple of different terms interchangeably in this class when it comes to models. Sometimes we talk about coefficients as being completely pooled/partially pooled/unpooled, and sometimes we talk about coefficients as being random or fixed. Yikes, so confusing! Here’s a quick document explaining what these various terms mean and what sorts of models they represent. We’re only going to be talking about models where the pooling applied to the intercept and slope is the same; most models look like this, and these models are easier to talk about. You should be able to see how you might pool different coefficients differently, though the R code for that can be challenging. We’ll use the HSB data, and all of the models we’ll consider will look at regressions of math achievement on SES.\n\n40.1.1 Completely pooled\nA completely pooled model is a model where we assume that every second-level unit (school) has the same intercept and slope (slopes and intercepts are both completely pooled). This doesn’t really have an analog in the fixed/random effects world.\nA completely pooled model in this setting might look like\n\\[\\begin{aligned}\n    mathach_i &= \\beta_0 + \\beta_1SES_i + \\varepsilon_i \\\\\n    \\varepsilon_i &\\sim Normal(0, \\sigma^2)\n\\end{aligned}\\]\nIn a completely pooled model we’re basically assuming that every school has the same intercept and slope, so we just ignore school membership; notice that we don’t even include the \\(j\\) subscript because we’re ignoring schools completely. How rude!\nWe would fit this model with\nlm(mathach ~ ses).\n\n\n40.1.2 Partially pooled\nA partially pooled model allows for the possibility that different schools might have different slopes and intercepts, but assumes that these slopes and intercepts come from a Normal distribution, which has the effect of pulling them all in towards a grand mean (or partially pooling them). This model can also be called a model with random slopes and random intercepts, since we assume that school intercepts and residuals are random draws from a multivariate distribution with means equal to the grand means (and some possibly non-0 correlation). We don’t try to estimate these by themselves, only their variances and covariance.\nThis model can be represented as\n\\[\\begin{aligned}\nmathach_{ij} &= \\beta_{0j[i]} + \\beta_{1j[i]}SES_i + \\varepsilon_i, \\\\\n    \\beta_{0j} &= \\gamma_{00} + u_{0j},\\\\\n    \\beta_{1j} &= \\gamma_{10} + u_{1j},\\\\\n    \\varepsilon_i &\\sim Normal(0, \\sigma^2_\\varepsilon) \\\\\n    \\begin{pmatrix}\n        u_{0j}\\\\\n        u_{1j}\\\\\n    \\end{pmatrix} &\\sim  N\n    \\begin{bmatrix}\n        \\begin{pmatrix}\n            0\\\\\n            0\n        \\end{pmatrix}\\!\\!,&\n        \\begin{pmatrix}\n            \\sigma^2_0 & \\rho\\sigma_0\\sigma_1\\\\\n            \\rho\\sigma_0\\sigma_1 & \\sigma^2_1\n        \\end{pmatrix}\n    \\end{bmatrix}\n\\end{aligned}\\]\nWe would fit this model with\nlmer(mathach ~ ses + (ses|school))\n\n\n40.1.3 Unpooled\nIn an unpooled model, we don’t share any information across schools about the slopes and intercepts. Instead, we estimate each one separately in each higher-order unit. This is a fixed-effects model, because the model treats each school-level slope and intercept as a fixed quantity in the population to be estimated directly. In general parlance, to be a little more precise, a fixed-effects model is a model with unpooled intercepts and completely pooled slopes (although in theory the completely pooled model also has only fixed effects, it’s just that those effects are the same in every school; this is why the language of completely pooled, partially pooled, and unpooled coefficients is a little more precise, though it’s also less popular).\nWe could represent an unpooled model as\n\\[\\begin{aligned}\n    mathach_{ij} &= \\beta_{0j[i]} + \\beta_{1j[i]}SES_i + \\varepsilon_i \\\\\n    \\varepsilon_i &\\sim Normal(0, \\sigma^2)\n\\end{aligned}\\]\nWe would fit the model with\nlm(mathach ~ ses*school)\nalthough we might get our estimates in a more useful way by specifying an (identical) model which has no reference school, i.e.,\nlm(mathach ~ 0 + ses + ses:school)\nFor either of these models to fit you need to ensure that school is coded as a factor and not a number; this is not a concern for lmer."
  },
  {
    "objectID": "survey_weights.html#multilevel-modeling-and-survey-weights",
    "href": "survey_weights.html#multilevel-modeling-and-survey-weights",
    "title": "41  Survey Weights",
    "section": "41.1 Multilevel modeling and survey weights",
    "text": "41.1 Multilevel modeling and survey weights\nIn many circumstances you may be faced with a multilevel modeling project where you also have survey weights. Unfortunately R does not have good support for this hybrid of two worlds (although if you go the econometric direction you can incorporate weights into your robust standard errors).\nYou basically have two options at this point: you can ignore the weights (defendable, but often upsetting to reviewers and colleagues), or switch to Stata, which allows for both."
  },
  {
    "objectID": "survey_weights.html#topline-advice",
    "href": "survey_weights.html#topline-advice",
    "title": "41  Survey Weights",
    "section": "41.2 Topline advice",
    "text": "41.2 Topline advice\nIgnore the weights for your final project and worry about extending to your general population later on, depending on where your research takes you. More important than the weights is making sure you have random effects corresponding to all clustering involved in the way the data were collected. For example, if the program was a sample of states and then a sample of villages, and then households, you would have a 4-level model: states, villages, households, and individuals. You would want a random effect for each level. If you had few states, you could back off and have fixed effects for state and generalize only to the sampled states rather than the full country."
  },
  {
    "objectID": "survey_weights.html#what-are-survey-weights",
    "href": "survey_weights.html#what-are-survey-weights",
    "title": "41  Survey Weights",
    "section": "41.3 What are survey weights?",
    "text": "41.3 What are survey weights?\nThe easy way to think of sample weights (survey weights) is the answer to “how many people does this individual represent in the population?” (Although note that weights will generally be proportional to the answer to this question rather than literally that value.)\nFor example, if you had three people, the first with a weight of 1, the second with a weight of 0.5 and the third with a weight of 3, then we would think of our population as being \\(1 + 0.5 + 3 = 4.5\\) people, 3 of whom are people similar to our third sampled person, 0.5 of which our second sampled person, and 1 of whom is similar to our first person.\nIn other words, our first person is sampled proportional to their prevalence in the population. The 0.5 weight person is “oversampled”—we have too many people like this in our sample, as compared to the population so we “downweight” them. The third person is underrepresented, by contrast. We should have had three times as many of these types of people in our sample as we have.\nThus, sampling weights adjust for the probability of selecting an individual from the population when that probability is not constant (this could be due either by design or by chance). For nationally representative data surveys often select a sample where individuals have an unequal probability of being selected. This is done to increase the number of individuals and reduce sampling variability, particularly for certain areas or subgroups of the population. In some cases, corrections for non-response are also built into the weights. Sampling weights are then inversely proportional to the probability of selection.\nIn some complex surveys, there may be more than one sampling weight when different subsamples are selected. For example, the Demographic and Health Surveys (DHS)1 select a subsample of adults to be tested for HIV. If 1 in 5 households is selected for HIV testing, then no weighting is needed. But if 1 in 5 urban households and 1 in 2 rural households are selected, then sampling weights need to be applied to both descriptive statistics and model estimates to estimate at national level."
  },
  {
    "objectID": "survey_weights.html#what-happens-if-you-ignore-the-weights",
    "href": "survey_weights.html#what-happens-if-you-ignore-the-weights",
    "title": "41  Survey Weights",
    "section": "41.4 What happens if you ignore the weights?",
    "text": "41.4 What happens if you ignore the weights?\nIn this case (as long as you are modeling the clustering correctly) you are estimating relationships on your sample, rather than the target population. This can be a totally fine thing: if you are interested in how some variables interrelate you might reasonably believe that a found pattern of relationships in the sample is very indicative of how things may play out on a wider stage. It would be odd for (statistically significant) relationships in the sample to not be at least somewhat similar to the population the sample came from. The true magnitudes may shift, but the story should be the same.\nFor example, if, after ignoring weights, you find an impact of some treatment, then you know the treatment works, at least for those in your sample. Even if your sample is a nonrepresentative sample of your population, it is still some sort of representation, and thus you would believe that your treatment would work to some degree more generally. In this case, any differences between your sample and population would be due to treatment variation, i.e., some in your sample respond differently than some in the population, and so your results in the sample would be weighting some people more than we “should,” causing the discrepancy.\nSurvey weights are usually much more important when trying to estimate level, or prevalence, of an outcome. If, for example, you are attempting to measure the average literacy in a population, then survey weights will be very important: if the weights of those systematically more (or less) literate are different, then ignoring the weights can cause bias. In addition, if some types of groups or areas are oversampled, then your estimates will tend to be biased towards the levels and relationships in the oversampled group/area. But if you are interested in the relationship between literacy and some covariate, the weights will matter less: it is only if the relationship between these variables is different in your high weight and low weight individuals where you will get bias. This is arguably a less natural phenomenon."
  },
  {
    "objectID": "survey_weights.html#how-to-apply-weights",
    "href": "survey_weights.html#how-to-apply-weights",
    "title": "41  Survey Weights",
    "section": "41.5 How to apply weights?",
    "text": "41.5 How to apply weights?\nAs a rule of thumb, you want to first read any available documentation for the data you are using. You want to understand how the sample was obtained and how the weights were calculated. Publicly available data often comes with manuals on how to handle weights. Some manuals even come with R and Stata code! This is a very important step as sometimes you have to manipulate the weights before you can use them! For example, when using DHS data, you have to divide the weight by 1,000,000 before use. This is a function of how the weights are calculated. In addition, many complex surveys that use weights may also have stratified the sample, and that is also something to account for in your analysis.\nWhen using survey weights it is always advisable to compare the results that include weights with those without them. In general, one should not expect see substantive changes in the point estimates of regression coefficients to the point of dramatically changing one’s interpretation of one’s results. The model itself is supposed to capture structural relationships between covariates and outcomes, and under correct model specification the weights are superfluous with regards to these coefficients. Where weights could cause change is with descriptives such as the overall averages (e.g., the intercepts, in particular, could be different. We may also see changes in the variance parameters. Finally, with weights, one usually sees an increase in the the standard errors.\nA limitation with some packages is one might not have an easy way to obtain model fit statistics to help compare models. A clean way to avoid this is to go through the process of model selection using the data in the sample (ignoring the weights) and using the packages and approaches we have seen in class. The findings from such an exploration would be valid for the structure of the data in our sample. Then, as a second step, include the survey weights to move to inference to a larger population (for which the sample is supposed to be representative), taking the preferred model choices from step one and fitting them through a package that allows for survey weights."
  },
  {
    "objectID": "survey_weights.html#further-references",
    "href": "survey_weights.html#further-references",
    "title": "41  Survey Weights",
    "section": "41.6 Further references",
    "text": "41.6 Further references\nFor some good resources see (asparouhov2006?; carle2009?; rabe-hesketh2006?). Prior students previously also used (laukaityte2018importance?) and (lorah2020estimating?) for some guidance. They then worked with the BIFIEsurvey R package to fit multi-level models with survey weights to account for the complex sampling design in their data."
  },
  {
    "objectID": "survey_weights.html#footnotes",
    "href": "survey_weights.html#footnotes",
    "title": "41  Survey Weights",
    "section": "",
    "text": "These are nationally representative surveys conducted in low- and middle-income countries collecting data primarily on maternal and child health.↩︎"
  },
  {
    "objectID": "complex_error.html#national-youth-survey-running-example",
    "href": "complex_error.html#national-youth-survey-running-example",
    "title": "42  An overview of complex error structures",
    "section": "42.1 National Youth Survey running example",
    "text": "42.1 National Youth Survey running example\nOur running example is the data as described in Raudenbush and Bryk, and we follow the discussion on page 190. These data are the first cohort of the National Youth Survey (NYS). This data comes from a survey in which the same students were asked yearly about their acceptance of 9 “deviant” behaviors (such as smoking marijuana, stealing, etc.). The study began in 1976, and followed two cohorts of children, starting at ages 11 and 14 respectively. We will analyze the first 5 years of data.\nAt each time point, we have measures of:\n\nATTIT, the attitude towards deviance, with higher numbers implying higher tolerance for deviant behaviors.\nEXPO, the “exposure”, based on asking the children how many friends they had who had engaged in each of the behaviors. Both of these numbers have been transformed to a logarithmic scale to reduce skew.\n\nFor each student, we have:\n\nGender (binary)\nMinority status (binary)\nFamily income, in units of $10K.\n\nOne reasonable research question would to describe how the cohort evolved. For this question, the parameters of interest would be the average attitudes at each age. Standard deviations and intrasubject correlations are, as is often but not always the case, simply nuisance parameters. Still, the better we can do at realistically modeling these nuisance parameters, the more precision we will have for the measures of interest, and the power we will have to test relevant hypotheses.\n\n42.1.1 Getting the data ready\nWe’ll focus on the first cohort, from ages 11-15. First, let’s read the data.\nNote that this table is in “wide format”. That is, there is only one row for each student, with all the different observations for that student in different columns of that one row.\n\nnyswide = read.csv(\"data/nyswide.csv\")\nhead(nyswide)\n\n  ID ATTIT.11 EXPO.11 ATTIT.12 EXPO.12 ATTIT.13 EXPO.13 ATTIT.14 EXPO.14\n1  3     0.11   -0.37     0.20   -0.27     0.00   -0.37     0.00   -0.27\n2  8     0.29    0.42     0.29    0.20     0.11    0.42     0.51    0.20\n3  9     0.80    0.47     0.58    0.52     0.64    0.20     0.75    0.47\n4 15     0.44    0.07     0.44    0.32     0.89    0.47     0.75    0.26\n5 33     0.20   -0.27     0.64   -0.27     0.69   -0.27       NA      NA\n6 45     0.11    0.26     0.37   -0.17     0.37    0.14     0.37    0.14\n  ATTIT.15 EXPO.15 FEMALE MINORITY INCOME\n1     0.11   -0.17      1        0      3\n2     0.69    0.20      0        0      4\n3     0.98    0.47      0        0      3\n4     0.80    0.47      0        0      4\n5     0.11    0.07      1        0      4\n6     0.69    0.32      1        0      4\n\n\nFor our purposes, we want it in “long format”. The reshape() command does this for us (reshape allows us to reshape two variables at once, as compared to gather() from tidyverse:\n\nnys1.na = reshape(nyswide, direction=\"long\", #we want it in long format\n              varying=list(ATTIT=paste(\"ATTIT\",11:15,sep=\".\"), \n                          EXPO=paste(\"EXPO\",11:15,sep=\".\") ),\n              v.names=c(\"ATTIT\",\"EXPO\"), idvar=\"ID\", timevar=\"AGE\", times=11:15)\n\n## Drop missing ATTIT values\nnys1 = nys1.na[!is.na(nys1.na$ATTIT),] \n\nhead( nys1 )\n\n      ID FEMALE MINORITY INCOME AGE ATTIT  EXPO\n3.11   3      1        0      3  11  0.11 -0.37\n8.11   8      0        0      4  11  0.29  0.42\n9.11   9      0        0      3  11  0.80  0.47\n15.11 15      0        0      4  11  0.44  0.07\n33.11 33      1        0      4  11  0.20 -0.27\n45.11 45      1        0      4  11  0.11  0.26\n\n\nNote, the paste command makes the sequence c(\"ATTIT.12\", \"ATTIT.13\", ...) to autogenerate our variable names to reshape:\n\npaste(\"ATTIT\",11:15,sep=\".\")\n\n[1] \"ATTIT.11\" \"ATTIT.12\" \"ATTIT.13\" \"ATTIT.14\" \"ATTIT.15\"\n\n\nWe also need to make our age a factor so it is treated appropriately as an indicator of what wave the data was collected in.\n\nnys1$agefac = as.factor(nys1$AGE) \n\nJust to get a sense of the data, let’s plot each age as a boxplot\n\nggplot( nys1, aes( agefac, ATTIT ) ) +\n  geom_boxplot()\n\n\n\n\nNote some features of the data: First, we see that ATTIT goes up over time. Second, we see the variation of points also goes up over time. This is heteroskedasticity.\nIf we plot individual lines we have\n\nnys1$AGEjit = jitter(nys1$AGE)\nnys1$ATTITjit = jitter(nys1$ATTIT, amount=0.05)\nggplot( filter( nys1, complete.cases(nys1) ), aes( AGEjit, ATTITjit, group=ID ) ) +\n  geom_line( alpha=0.2 )\n\n\n\n\nNote how we have correlation of residuals, in that some students are systematically low and some are systematically higher (although there is a lot of bouncing around)."
  },
  {
    "objectID": "complex_error.html#representation-of-error-structure",
    "href": "complex_error.html#representation-of-error-structure",
    "title": "42  An overview of complex error structures",
    "section": "42.2 Representation of error structure",
    "text": "42.2 Representation of error structure\nIn our data, we have 5 observations \\(y_{it}\\) for each subject i at 5 fixed times \\(t=1\\) through \\(t=5\\). Within each person \\(i\\) (where person is our Level-2 group, and time is our Level-1),\n\\[\\begin{pmatrix}y_{i1}\\\\\ny_{i2}\\\\\ny_{i3}\\\\\ny_{i4}\\\\\ny_{i5}\n\\end{pmatrix} \\sim N\\left[\\left(\\begin{array}{c}\n\\mu_1\\\\\n\\mu_2\\\\\n\\mu_3\\\\\n\\mu_4\\\\\n\\mu_5\n\\end{array}\\right),\\left(\\begin{array}{ccccc}\n\\tau_{11} & \\tau_{12} & \\tau_{13} & \\tau_{14} & \\tau_{15}\\\\\n. & \\tau_{22} & \\tau_{23}& \\tau_{24} & \\tau_{25}\\\\\n. & . & \\tau_{33}& \\tau_{34} & \\tau_{35}\\\\\n. & . & . & \\tau_{44} & \\tau_{45}\\\\\n. & . & . & . & \\tau_{52}\n\\end{array}\\right)\\right] = N[ \\mu, \\Sigma ]\\]\nNote that the key parts here are the correlations between the residuals at different times. We call our entire covariance matrix \\(\\Sigma\\). This matrix describes how the residuals within a single individual (with 5 time points of observation) are correlated. The mean vector can easily be separated out:\n\\[\\begin{pmatrix}y_{i1}\\\\\ny_{i2}\\\\\ny_{i3}\\\\\ny_{i4}\\\\\ny_{i5}\n\\end{pmatrix} = \\left(\\begin{array}{c}\n\\mu_{1i}\\\\\n\\mu_{2i}\\\\\n\\mu_{3i}\\\\\n\\mu_{4i}\\\\\n\\mu_{5i}\n\\end{array}\\right) + \\left(\\begin{array}{c}\n\\epsilon_{1i}\\\\\n\\epsilon_{2i}\\\\\n\\epsilon_{3i}\\\\\n\\epsilon_{4i}\\\\\n\\epsilon_{5i}\n\\end{array}\\right)\\] \\[\\left(\\begin{array}{c}\n\\epsilon_{1i}\\\\\n\\epsilon_{2i}\\\\\n\\epsilon_{3i}\\\\\n\\epsilon_{4i}\\\\\n\\epsilon_{5i}\n\\end{array}\\right)\\sim N\\left[\\left(\\begin{array}{c}\n0\\\\\n0\\\\\n0\\\\\n0\\\\\n0\n\\end{array}\\right),\\left(\\begin{array}{ccccc}\n\\tau_{11} & \\tau_{12} & \\tau_{13} & \\tau_{14} & \\tau_{15}\\\\\n. & \\tau_{22} & \\tau_{23}& \\tau_{24} & \\tau_{25}\\\\\n. & . & \\tau_{33}& \\tau_{34} & \\tau_{35}\\\\\n. & . & . & \\tau_{44} & \\tau_{45}\\\\\n. & . & . & . & \\tau_{55}\n\\end{array}\\right)\\right]\\]\nOur regression model would give us the mean vector for any given student (e.g., it would be \\(X'\\beta\\) for some covariate matrix (design matrix) \\(X\\) and fixed effect parameter vector \\(\\beta\\). \\(X\\) would have one row per time point and time would be one of the columns, to give our predictions for our 5 time points.\nOur error structure model gives us the distribution of the \\((\\epsilon_{1i}, \\ldots, \\epsilon_{5i})\\) for that student. Different ideas about the data generating process lead to different correlation structures here. We saw a couple of those in class."
  },
  {
    "objectID": "complex_error.html#reproducing-chapter-6-examples",
    "href": "complex_error.html#reproducing-chapter-6-examples",
    "title": "42  An overview of complex error structures",
    "section": "42.3 Reproducing Chapter 6 examples",
    "text": "42.3 Reproducing Chapter 6 examples\nThe above provides a framework for thinking about grouped data: each group is a small world with a linear prediction line and a collection of residuals around that line. Under this view, we specify a specific structure on how the residuals relate to each other. (E.g., for classic OLS we would have i.i.d. normally distributed residuals, represented as our \\(\\Sigma\\) being a diagonal matrix with \\(\\sigma^2\\) along the diagonal and 0s everywhere else). In R, once we determine what structure we want, we can fit models based on parameterized correlation matrices using the “lme” command from the “nlme” package (You may need to first call install.packages(\"nlme\") to get this package). Let’s load the package now:\n\nlibrary(nlme)\n\nRecall that all of these models include just the single fixed effect (besides intercept) of a linear term on age.\n\n42.3.1 Compound symmetry (random intercept model)\nA “compound symmetry” residual covariance structure (all diagonal elements equal, all off-diagonal elements equal) is actually equivalent to a random intercepts model. Thus, there are 2 ways to get this same model:\n\nmodelRE = lme(ATTIT ~ AGE, \n              data=nys1,\n              random=~1|ID )\n\nand\n\nmodelCompSymm = gls(ATTIT ~ AGE,\n                    data=nys1,\n                    correlation=corCompSymm(form=~AGE|ID) )\n\nFor reference, using the lme4 package we again have (we use lme4:: in front of lmer to avoid loading the lme4 package fully):\n\nmodelRE.lme4 = lme4::lmer(ATTIT ~ AGE + (1|ID), data=nys1 )\n\nWe can get the correlation matrix for individuals #3:\n\nmyVarCovs = getVarCov(modelRE,type=\"marginal\", individual=3)\nmyVarCovs\n\nID 9 \nMarginal variance covariance matrix\n         1        2        3        4        5\n1 0.066450 0.034113 0.034113 0.034113 0.034113\n2 0.034113 0.066450 0.034113 0.034113 0.034113\n3 0.034113 0.034113 0.066450 0.034113 0.034113\n4 0.034113 0.034113 0.034113 0.066450 0.034113\n5 0.034113 0.034113 0.034113 0.034113 0.066450\n  Standard Deviations: 0.25778 0.25778 0.25778 0.25778 0.25778 \n\n\nIf we look at an individual #5, who only has 4 timepoints we get a \\(4 \\times 4\\) matrix:\n\ngetVarCov(modelRE,type=\"marginal\", individual=5)\n\nID 33 \nMarginal variance covariance matrix\n         1        2        3        4\n1 0.066450 0.034113 0.034113 0.034113\n2 0.034113 0.066450 0.034113 0.034113\n3 0.034113 0.034113 0.066450 0.034113\n4 0.034113 0.034113 0.034113 0.066450\n  Standard Deviations: 0.25778 0.25778 0.25778 0.25778 \n\n\n(Other individuals are the same, if they have the same number of time points, given our model.)\n\n42.3.1.1 Comparing the models\nThese are two very different ways of specifying the same thing, and the parameter estimates we get out are also not the same. Compare the two summary printouts:\n\nsummary(modelRE)\n\nLinear mixed-effects model fit by REML\n  Data: nys1 \n        AIC       BIC   logLik\n  -204.9696 -185.0418 106.4848\n\nRandom effects:\n Formula: ~1 | ID\n        (Intercept)  Residual\nStdDev:   0.1846979 0.1798237\n\nFixed effects:  ATTIT ~ AGE \n                 Value  Std.Error  DF   t-value p-value\n(Intercept) -0.5099954 0.05358498 839 -9.517505       0\nAGE          0.0644387 0.00398784 839 16.158810       0\n Correlation: \n    (Intr)\nAGE -0.969\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-2.90522949 -0.64353962 -0.01388485  0.60377631  3.26938845 \n\nNumber of Observations: 1079\nNumber of Groups: 239 \n\n\nand\n\nsummary(modelCompSymm)\n\nGeneralized least squares fit by REML\n  Model: ATTIT ~ AGE \n  Data: nys1 \n        AIC       BIC   logLik\n  -204.9696 -185.0418 106.4848\n\nCorrelation Structure: Compound symmetry\n Formula: ~AGE | ID \n Parameter estimate(s):\n      Rho \n0.5133692 \n\nCoefficients:\n                 Value  Std.Error   t-value p-value\n(Intercept) -0.5099954 0.05358498 -9.517505       0\nAGE          0.0644387 0.00398784 16.158810       0\n\n Correlation: \n    (Intr)\nAGE -0.969\n\nStandardized residuals:\n        Min          Q1         Med          Q3         Max \n-1.77123071 -0.77132300 -0.06434029  0.71151900  3.38387884 \n\nResidual standard error: 0.2577787 \nDegrees of freedom: 1079 total; 1077 residual\n\n\nThese do not look very similar, do they? But wait:\n\nlogLik(modelCompSymm)\n\n'log Lik.' 106.4848 (df=4)\n\nlogLik(modelRE)\n\n'log Lik.' 106.4848 (df=4)\n\nlogLik(modelRE.lme4)\n\n'log Lik.' 106.4848 (df=4)\n\nAIC( modelCompSymm )\n\n[1] -204.9696\n\nAIC( modelRE )\n\n[1] -204.9696\n\nAIC( modelRE.lme4 )\n\n[1] -204.9696\n\n\nIn fact, they have the same AIC, etc., because they are equivalent models.\nThe lesson is that it’s actually quite hard to see the correspondence between a familiar random-effects model and an equivalent model expressed in terms of a covariance matrix. Sure, we could do a bunch of math and see that in the end they are the same; but that math is already daunting here, and this is the simplest possible situation. The fitted parameters of a covariance-based model are just really hard to interpret in familiar terms.\n\n\n\n42.3.2 Autoregressive error structure (AR[1])\nOne typical structure used for longitudinal data is the “autoregressive” structure. The idea is threefold:\n\n\\(Var(u_{it}) = \\sigma^2\\) - that is, overall marginal variance is staying constant.\n\\(Cor(u_{it},u_{i(t-1)}) = \\rho\\) - that is, residuals are a little bit “sticky” over time so residuals from nearby time points tend to be similar.\n\\(E(u_{it}|u_{i(t-1)},u_{i(t-2)}) = E(u_{it}|u_{i(t-1)})\\) - that is, the only way the two-periods-ago measurement tells you anything about the current one is through the intermediate one, with no longer-term effects or “momentum”.\n\nIn this case, the unconditional two-step correlation \\(Cor(u_{it},u_{i(t-2)})\\) is also easy to calculate. Intuitively, we can say that a portion \\(\\rho\\) of the residual “is the same” after each step, so that after two steps the portion that “is the same” is \\(\\rho\\) of \\(\\rho\\), or \\(\\rho^2\\). Clearly, then, after three steps the correlation will be \\(\\rho^3\\), and so on. In other words, the part that “is the same” is decaying in an exponential pattern. Indeed, one could show that (3.), above, requires the correlated part to decay in a memoryless pattern, leaving the Exponential and Hypergeometric distributions (which both show exponential decay) among the few options.\nThus, the within-subject correlation structure implied by these postulates is:\n\\[\\left(\\begin{array}{c}\nu_{1i}\\\\\nu_{2i}\\\\\nu_{3i}\\\\\nu_{4i}\\\\\n...\\\\\nu_{ni}\n\\end{array}\\right)\\sim N\\left[\\left(\\begin{array}{c}\n0\\\\\n0\\\\\n0\\\\\n0\\\\\n...\\\\\n0\n\\end{array}\\right),\\sigma^2\\left(\\begin{array}{cccccc}\n1 & \\rho  & \\rho^2 & \\rho^3 & ... & \\rho^{n-1}\\\\\n. & 1 & \\rho & \\rho^2 & ... & \\rho^{n-2}\\\\\n. & . & 1& \\rho  & ... & \\rho^{n-3}\\\\\n. & . & . & 1 & ... & \\rho^{n-4} \\\\\n... & ... & ... & ... & ... & ... \\\\\n. & . & . & . & ... & 1\n\\end{array}\\right)\\right]\\\\\\]\nAs you can see, this structure takes advantage of the temporal nature of the data sequence to parameterize the covariance matrix with only two underlying parameters: \\(\\sigma\\) and \\(\\rho\\). By contrast, a random intercept model needs the overall \\(\\sigma\\) and variance of intercepts \\(\\tau\\)—also two parameters! Same complexity, different structure.\n\n42.3.2.1 Fitting the AR[1] covariance structure\nTo get a true AR[1] residual covariance structure, we need to leave the world of hierarchical models, and thus use the command gls. This is just what we’ve discussed in class. However, later on in this document, we’ll see how to add AR[1] structure on top of a hierarchical model, which is messier from a theoretical point of view, but often more useful and interpretable in practice.\n\nmodelAR1 = gls(ATTIT ~ AGE, \n                    data=nys1,\n                    correlation=corAR1(form=~AGE|ID) )\n\nsummary(modelAR1)\n\nGeneralized least squares fit by REML\n  Model: ATTIT ~ AGE \n  Data: nys1 \n        AIC       BIC   logLik\n  -250.4103 -230.4826 129.2051\n\nCorrelation Structure: ARMA(1,0)\n Formula: ~AGE | ID \n Parameter estimate(s):\n     Phi1 \n0.6159857 \n\nCoefficients:\n                 Value  Std.Error   t-value p-value\n(Intercept) -0.4534647 0.07515703 -6.033564       0\nAGE          0.0601205 0.00569797 10.551218       0\n\n Correlation: \n    (Intr)\nAGE -0.987\n\nStandardized residuals:\n        Min          Q1         Med          Q3         Max \n-1.75013168 -0.81139621 -0.03256558  0.74814629  3.40350724 \n\nResidual standard error: 0.2561765 \nDegrees of freedom: 1079 total; 1077 residual\n\n\nYou have to dig around in the large amount of output to find the parameter estimates, but they are there. Phi1 is the auto-correlation parameter. And the covariance of residuals:\n\ngetVarCov(modelAR1,type=\"marginal\")\n\nMarginal variance covariance matrix\n          [,1]     [,2]     [,3]     [,4]      [,5]\n[1,] 0.0656260 0.040425 0.024901 0.015339 0.0094485\n[2,] 0.0404250 0.065626 0.040425 0.024901 0.0153390\n[3,] 0.0249010 0.040425 0.065626 0.040425 0.0249010\n[4,] 0.0153390 0.024901 0.040425 0.065626 0.0404250\n[5,] 0.0094485 0.015339 0.024901 0.040425 0.0656260\n  Standard Deviations: 0.25618 0.25618 0.25618 0.25618 0.25618 \n\nsummary(modelAR1)$AIC\n\n[1] -250.4103\n\n\nNote that the AIC of our AR[1] model is lower by about 45 than the random intercept model; clearly far superior because it is getting nearby residuals being more correlated, while the random intercept model does not do this. Also see the banding structure of the residual correlation matrix.\n\n\n\n42.3.3 Random slopes\nIn theory, a random slopes model could be done with gls as well as with lme; in practice, it’s much more practical just to do it as a hierarchical model with lme:\n\nmodelRS = lme(ATTIT ~ 1 + AGE, \n              data=nys1,\n              random=~AGE|ID )\n\nWe have separated our fixed and random components with lme(). We first include a formula with only fixed effects, and then give a right-side-only formula with terms similar to what you’d put in parentheses with lmer() for the random effects.\nOur results:\n\nsummary(modelRS)\n\nLinear mixed-effects model fit by REML\n  Data: nys1 \n       AIC       BIC   logLik\n  -310.125 -280.2334 161.0625\n\nRandom effects:\n Formula: ~AGE | ID\n Structure: General positive-definite, Log-Cholesky parametrization\n            StdDev     Corr  \n(Intercept) 0.51024133 (Intr)\nAGE         0.05038614 -0.98 \nResidual    0.16265428       \n\nFixed effects:  ATTIT ~ 1 + AGE \n                 Value  Std.Error  DF   t-value p-value\n(Intercept) -0.5133250 0.05834087 839 -8.798719       0\nAGE          0.0646849 0.00492904 839 13.123228       0\n Correlation: \n    (Intr)\nAGE -0.981\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-2.87852426 -0.55971198 -0.07521191  0.57495076  3.45648137 \n\nNumber of Observations: 1079\nNumber of Groups: 239 \n\ngetVarCov(modelRS,type=\"marginal\", individual=3)\n\nID 9 \nMarginal variance covariance matrix\n         1        2        3        4        5\n1 0.039649 0.015922 0.018650 0.021379 0.024108\n2 0.015922 0.047646 0.026457 0.031725 0.036992\n3 0.018650 0.026457 0.060720 0.042070 0.049876\n4 0.021379 0.031725 0.042070 0.078872 0.062760\n5 0.024108 0.036992 0.049876 0.062760 0.102100\n  Standard Deviations: 0.19912 0.21828 0.24641 0.28084 0.31953 \n\nsummary(modelRS)$AIC\n\n[1] -310.125\n\n\nThe first thing to note is the residual covariance matrix comes from the structure of the random intercept and random slope. If you squint hard enough at it, you can begin to see the linear structures in its diagonal and off-diagonal elements. If you graphed it, those structures would jump out more clearly. But in practice, it’s much easier to think of things in terms of the hierarchical model, not in terms of linear structures in a covariance matrix.\nNote also that the AIC has dropped by another 60 points or so; we’re continuing to improve the model.\n\n\n42.3.4 Random slopes with heteroskedasticity\nRelaxing the homoskedasticity assumption in the random slopes model leaves us a bit in between worlds. We’re not fully into the world of GLS, because there are still random effects; but we’re not fully in the world of hierarchical models because there is structure in the residuals within groups. We’ll talk more about this compromise below; for now, let’s just do it.\n\nmodelRSH = lme(ATTIT ~ AGE, \n              data=nys1,\n              random=~AGE|ID,\n              weights=varIdent(form=~1|agefac) )\n\nThe key line is the varIdent line: we are saying each age factor level gets its own weight (rescaling) of the residuals—this is heteroskedasticity. In particular, the above says our residual variance will be weighted by a weight for each age factor, so each age level effectively gets its own variance. This is where these models start to get a bit exciting—we have random slopes, and then heteroskedastic residuals (homoskedastic for any given age level), all together. Our fit model:\n\nsummary(modelRSH)\n\nLinear mixed-effects model fit by REML\n  Data: nys1 \n        AIC       BIC   logLik\n  -312.5801 -262.7608 166.2901\n\nRandom effects:\n Formula: ~AGE | ID\n Structure: General positive-definite, Log-Cholesky parametrization\n            StdDev     Corr  \n(Intercept) 0.57693635 (Intr)\nAGE         0.05431371 -0.979\nResidual    0.14054173       \n\nVariance function:\n Structure: Different standard deviations per stratum\n Formula: ~1 | agefac \n Parameter estimates:\n       11        12        13        14        15 \n1.0000000 1.1956045 1.3095883 1.1255244 0.9802308 \nFixed effects:  ATTIT ~ AGE \n                 Value  Std.Error  DF   t-value p-value\n(Intercept) -0.4929012 0.05715888 839 -8.623352       0\nAGE          0.0631404 0.00483385 839 13.062119       0\n Correlation: \n    (Intr)\nAGE -0.981\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-2.91635759 -0.54982016 -0.07583344  0.54829414  3.23703121 \n\nNumber of Observations: 1079\nNumber of Groups: 239 \n\n\nNote how we have 5 parameter estimates for the residuals, listed under agefac. It appears as if we have more variation in age 13 than other ages. Age 11, the baseline, is 1.0; it is our reference scaling. These numbers are all scaling the overall residual variance parameter \\(\\sigma^2\\) of \\(0.1405^2\\).\nFor looking at the covariance structure of the residuals, at this point we have to warn you: there appears to be a bug in getVarCov which rears its head when you use the weights argument to either lme or gls. It has to do with the order of the rows of the data set, something which obviously should not matter; and it means that you get simply wrong numbers for marginal variances, though correlations should still be correct. Jameson Quinn wrote a fix for this function, which is in the source file getVarCov2.R. This fix is used below, using the source command to load the fixed version of the function. But beware: while we’ve tested that this fix gives the right answers for this data set, and that it gives the right answers when the weights argument is not used (that is, when the old version was already right), we have not done the extensive checking it would take to say we trust it in all cases. This bug was reported back in 2016, and hopefully it will be fixed in later versions of R; but for now, tread with care, and double-check that the numbers you’re getting make sense.\n\nsource('scripts/getVarCov2.R')\nmyVarCov = getVarCovFixedLme(modelRSH,type=\"marginal\", individual=3)\nmyVarCov\n\nID 9 \nMarginal variance covariance matrix\n         1        2        3        4        5\n1 0.034915 0.016947 0.018731 0.020515 0.022300\n2 0.016947 0.049916 0.026415 0.031150 0.035884\n3 0.018731 0.026415 0.067975 0.041784 0.049468\n4 0.020515 0.031150 0.041784 0.077440 0.063052\n5 0.022300 0.035884 0.049468 0.063052 0.095615\n  Standard Deviations: 0.18685 0.22342 0.26072 0.27828 0.30922 \n\n\nWe get lists of matrices back from our call. We can convert any one to a correlation matrix:\n\ncov2cor(myVarCov[[1]])\n\n          1         2         3         4         5\n1 1.0000000 0.4059443 0.3844926 0.3945439 0.3859519\n2 0.4059443 1.0000000 0.4534860 0.5010154 0.5194178\n3 0.3844926 0.4534860 1.0000000 0.5759074 0.6136043\n4 0.3945439 0.5010154 0.5759074 1.0000000 0.7327485\n5 0.3859519 0.5194178 0.6136043 0.7327485 1.0000000\n\n\nAnd our AIC:\n\nsummary(modelRSH)$AIC\n\n[1] -312.5801\n\n\nNo amount of squinting will show the structure in that covariance matrix. But when you convert to a correlation matrix, you can again squint and begin to see the linear structures in its diagonal and off-diagonal elements. The same comment as above still applies: in practice, it’s much easier to think of things in terms of the hierarchical model, and only read the diagonals of the covariance matrix.\nNote that the AIC has dropped by only another 2.5 points or so; that corresponds to the idea that if one of these two models were exactly true, the odds are about \\(e^{2.5/2}\\cong 3.5\\) in favor of the more complex model. Aside from the fact that that premise is silly – we are pretty sure that neither of these models is the exact truth; and in that case, something like BIC would probably be better than AIC – those odds are also pretty weak; the simpler model is probably better here.\nHere’s the reported BICs, by the way: -280.2334145 for the homoskedastic one, and -262.7607678 for the heteroskedastic. As we expected, the simpler model wins that fight. (Though what \\(N\\) to use for BIC is sometimes not obvious with hierarchical models, so you can’t trust those numbers too much. We will discuss AIC and BIC more later in the course.)\n\n\n42.3.5 Fully unrestricted model\nOK, let’s go whole hog, and fit the unrestricted model. Again, this means leaving the world of hierarchical models and using gls.\n\nmodelUnrestricted = gls(ATTIT ~ AGE, \n               data=nys1,\n               correlation=corSymm(form=~1|ID),\n               weights=varIdent(form=~1|agefac) )\n\n\nsummary(modelUnrestricted)\n\nGeneralized least squares fit by REML\n  Model: ATTIT ~ AGE \n  Data: nys1 \n       AIC       BIC  logLik\n  -319.262 -234.5691 176.631\n\nCorrelation Structure: General\n Formula: ~1 | ID \n Parameter estimate(s):\n Correlation: \n  1     2     3     4    \n2 0.458                  \n3 0.372 0.511            \n4 0.441 0.437 0.663      \n5 0.468 0.443 0.597 0.764\nVariance function:\n Structure: Different standard deviations per stratum\n Formula: ~1 | agefac \n Parameter estimates:\n      11       12       13       14       15 \n1.000000 1.118479 1.414270 1.522511 1.560072 \n\nCoefficients:\n                 Value  Std.Error   t-value p-value\n(Intercept) -0.4557089 0.05465560 -8.337826       0\nAGE          0.0597274 0.00458343 13.031153       0\n\n Correlation: \n    (Intr)\nAGE -0.979\n\nStandardized residuals:\n         Min           Q1          Med           Q3          Max \n-1.482607475 -0.809004625 -0.006791829  0.840804662  4.082257854 \n\nResidual standard error: 0.1903187 \nDegrees of freedom: 1079 total; 1077 residual\n\n\nAnd our residual structure:\n\nsource('scripts/getVarCov2.R')\nmyvc = getVarCovFixedGls(modelUnrestricted,type=\"marginal\", individual=3)\nmyvc\n\nMarginal variance covariance matrix\n         [,1]     [,2]     [,3]     [,4]     [,5]\n[1,] 0.036221 0.018541 0.019071 0.024335 0.026466\n[2,] 0.018541 0.045313 0.029251 0.026924 0.027989\n[3,] 0.019071 0.029251 0.072448 0.051704 0.047736\n[4,] 0.024335 0.026924 0.051704 0.083962 0.065764\n[5,] 0.026466 0.027989 0.047736 0.065764 0.088156\n  Standard Deviations: 0.19032 0.21287 0.26916 0.28976 0.29691 \n\n\nAnd AIC:\n\nAIC( modelUnrestricted )\n\n[1] -319.262\n\n\nThis unrestricted covariance and correlation matrices have the same structures discussed in the book and in class. The AIC has improved by another 6 or 7 points; that’s marginally “significant”, but in practice probably not substantial enough to make up for the loss of interpretability. The lesson we should take from that is that there’s not a whole lot of room for improvement just by tinkering with the residual covariance structure; if we want a much better model, we would have to add new fixed or random effects; perhaps other covariates or perhaps a quadratic term in time."
  },
  {
    "objectID": "complex_error.html#mixing-ar1-and-random-slopes",
    "href": "complex_error.html#mixing-ar1-and-random-slopes",
    "title": "42  An overview of complex error structures",
    "section": "42.4 Mixing AR[1] and Random Slopes",
    "text": "42.4 Mixing AR[1] and Random Slopes\nLet’s look at an AR1 residual structure along with some covariates in our main model. The following has AR[1] and also a random intercept and slope:\n\nmodel1 = lme(fixed=ATTIT ~ EXPO + FEMALE + MINORITY + log(INCOME + 1), \n              data=nys1,\n              random=~AGE|ID,\n              correlation=corAR1()  )\n\nCompare to same model without AR1 correlation\n\nmodel1simple = lme(fixed=ATTIT ~ EXPO + FEMALE + MINORITY + log(INCOME + 1), \n             data=nys1,\n             random=~AGE|ID )\nscreenreg( list( model1, model1simple ) )\n\n\n=========================================\n                 Model 1      Model 2    \n-----------------------------------------\n(Intercept)         0.33 ***     0.32 ***\n                   (0.04)       (0.04)   \nEXPO                0.36 ***     0.37 ***\n                   (0.03)       (0.03)   \nFEMALE             -0.01        -0.01    \n                   (0.02)       (0.02)   \nMINORITY           -0.06 *      -0.06 *  \n                   (0.03)       (0.03)   \nlog(INCOME + 1)    -0.01        -0.01    \n                   (0.02)       (0.02)   \n-----------------------------------------\nAIC              -359.46      -350.54    \nBIC              -309.67      -305.73    \nLog Likelihood    189.73       184.27    \nNum. obs.        1079         1079       \nNum. groups: ID   239          239       \n=========================================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n\n\nThe AR1 model has notably lower AIC and thus is better. (A difference of ~9 in AIC, which can be interpreted as a factor of \\(e^{4.5}\\) in odds.) Here are the log likelihoods with degrees of freedom:\n\nlogLik( model1 )\n\n'log Lik.' 189.7308 (df=10)\n\nlogLik( model1simple )\n\n'log Lik.' 184.269 (df=9)\n\n\nOur model is actually kind of mixed up, conceptually. We allowed a random slope on age, and also an autoregressive component by age. Thus, we effectively allowed the covariance matrix to vary in two different ways, at two different levels of our modeling.\nIn fact, as we’ve seen in class, any random effects, whether they be on slope or intercept, are actually equivalent to certain ways of varying the [variance-]covariance matrix of the residuals within each group. For instance, random intercepts are equivalent to compound symmetry. Thus, by including both random intercepts and AR1 correlation in the above model, we’ve effectively fit a model that allows any covariance matrix that can be expressed as a sum of a random slope covariance matrix (with 2 parameters plus a scaling factor) and an AR1 covariance matrix (with 1 parameter plus a scaling factor). That makes 5 degrees of freedom total for our covariance matrix.\nBut conceptually this model is nice: people have linear growth trends, but vary around those growth trends in an autoregressive way."
  },
  {
    "objectID": "complex_error.html#the-kitchen-sink-building-complex-models",
    "href": "complex_error.html#the-kitchen-sink-building-complex-models",
    "title": "42  An overview of complex error structures",
    "section": "42.5 The Kitchen sink: building complex models",
    "text": "42.5 The Kitchen sink: building complex models\nWhich brings us to the next point: how do you actually use this stuff in practice. Ideally, you’d like both the interpretability (and robustness against MAR missingness) of hierarchical models, along with the ability to add additional residual structure such as AR[1] and/or heteroskedastic residuals. The good news is, you can get both. The bad news is, there’s a bit of a potential for bias due to overfitting.\nFor instance, imagine you use both random effects and AR[1]. Say that for a given subject you have 5 time points, and all of them are above the values you would have predicted based on fixed effects alone. That might be explained by an above-average random effect, or by a set of correlated residuals that all came in high. Whichever one of these is the “true” explanation, the MLE will tend to parcel it out between the two. This can lead to downward bias in variance and/or correlation parameter estimates, especially with small numbers of observations per subject.\nStill, as long as your focus is on location parameters such as true means or slopes, this can be a good way to proceed. Let’s explore this by first fitting a “kitchen sink” model for this data, in which we use all available covariates; and seeing how adding heteroskedasticity, AR[1] structure, or both changes it (or doesn’t).\nWhat do we want in this “kitchen sink” model? Let’s first fit a very simple random intercept model with fixed effects for gender, minority status, “exposure”, and log(income), to see which of these covariates to focus on. We use the lmerTest package to get some early \\(p\\)-values for these fixed effects.\n\nmodelKS0 = lmerTest::lmer(ATTIT ~ FEMALE + MINORITY + log(INCOME + 1) + EXPO + (1|ID), data=nys1)\nsummary(modelKS0, correlation=FALSE)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ATTIT ~ FEMALE + MINORITY + log(INCOME + 1) + EXPO + (1 | ID)\n   Data: nys1\n\nREML criterion at convergence: -265.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.1840 -0.5922 -0.0797  0.6043  2.6319 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept) 0.01756  0.1325  \n Residual             0.03444  0.1856  \nNumber of obs: 1079, groups:  ID, 239\n\nFixed effects:\n                  Estimate Std. Error         df t value Pr(&gt;|t|)    \n(Intercept)      3.480e-01  4.195e-02  2.301e+02   8.297    9e-15 ***\nFEMALE          -1.835e-02  2.094e-02  2.327e+02  -0.876   0.3819    \nMINORITY        -5.698e-02  2.789e-02  2.279e+02  -2.043   0.0422 *  \nlog(INCOME + 1)  2.102e-03  2.449e-02  2.272e+02   0.086   0.9317    \nEXPO             4.516e-01  2.492e-02  1.041e+03  18.122   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n(The correlation=FALSE shortens the printout.)\nApparently, MINORITY and EXPO are the covariates with significant effects; minority status is correlated with a lower tolerance for deviance, while “deviant” friends are of course correlated positively with tolerance of deviance. Let’s build a few hierarchical models including these in various specifications (can you identify what models are what? Some of these models are not necessarily good choices). We first center our age so we have meaningful intercepts.\n\nnys1$age13 = nys1$AGE - 13\n\nmodelKS1 = lme(ATTIT ~ MINORITY,\n              data=nys1,\n              random=~age13 + EXPO|ID )\n\nmodelKS2 = lme(ATTIT ~ MINORITY + age13, \n              data=nys1,\n              random=~age13 + EXPO|ID )\n\nmodelKS3 = lme(ATTIT ~ MINORITY + age13, \n              data=nys1,\n              random=~EXPO|ID )\n\nmodelKS4 = lme(ATTIT ~ MINORITY + age13 + EXPO, \n              data=nys1,\n              random=~1|ID )\n\nAnd now we examine them:\n\nlibrary( texreg )\nscreenreg( list( modelKS1, modelKS2, modelKS3, modelKS4 ))\n\n\n===================================================================\n                 Model 1      Model 2      Model 3      Model 4    \n-------------------------------------------------------------------\n(Intercept)         0.24 ***     0.31 ***     0.29 ***     0.34 ***\n                   (0.01)       (0.01)       (0.01)       (0.01)   \nMINORITY           -0.05        -0.05 *      -0.04        -0.06 *  \n                   (0.02)       (0.02)       (0.03)       (0.03)   \nage13                            0.06 ***     0.05 ***     0.05 ***\n                                (0.00)       (0.00)       (0.00)   \nEXPO                                                       0.37 ***\n                                                          (0.02)   \n-------------------------------------------------------------------\nAIC              -305.22      -374.18      -312.13      -394.06    \nBIC              -260.38      -324.37      -277.27      -364.18    \nLog Likelihood    161.61       197.09       163.07       203.03    \nNum. obs.        1079         1079         1079         1079       \nNum. groups: ID   239          239          239          239       \n===================================================================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n\n\nOK, Number 4 seems like a pretty good model. Let’s see how much it improves when we add AR[1]:\n\nmodelKS5 = lme(ATTIT ~ MINORITY + age13 + EXPO, \n              data=nys1,\n              random=~1|ID,\n              correlation=corAR1(form=~AGE|ID) )\nAIC( modelKS5 )\n\n[1] -433.5943\n\nfixef( modelKS4 )\n\n(Intercept)    MINORITY       age13        EXPO \n 0.34054593 -0.05606947  0.04830698  0.36778951 \n\nfixef( modelKS5 )\n\n(Intercept)    MINORITY       age13        EXPO \n 0.34083507 -0.05704474  0.04733879  0.35207989 \n\n\nNote that the estimates for all the effects are essentially unchanged. However, the AIC is almost 40 points better. Also, because the model has done a better job explaining residual variance, the p-value for the coefficient on MINORITY has dropped from 0.032 to 0.029, as we can see on the summary display below. This is not a large drop, but a noticeable one:\n\nsummary( modelKS5 )\n\nLinear mixed-effects model fit by REML\n  Data: nys1 \n        AIC       BIC   logLik\n  -433.5943 -398.7338 223.7972\n\nRandom effects:\n Formula: ~1 | ID\n        (Intercept)  Residual\nStdDev:   0.1186009 0.1864592\n\nCorrelation Structure: ARMA(1,0)\n Formula: ~AGE | ID \n Parameter estimate(s):\n     Phi1 \n0.3212696 \nFixed effects:  ATTIT ~ MINORITY + age13 + EXPO \n                 Value   Std.Error  DF   t-value p-value\n(Intercept)  0.3408351 0.011858053 838 28.742920   0.000\nMINORITY    -0.0570447 0.025968587 237 -2.196683   0.029\nage13        0.0473388 0.004523745 838 10.464512   0.000\nEXPO         0.3520799 0.024451130 838 14.399330   0.000\n Correlation: \n         (Intr) MINORI age13 \nMINORITY -0.457              \nage13    -0.013  0.010       \nEXPO      0.006 -0.001 -0.224\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-2.82097989 -0.65136103 -0.08846076  0.61819746  2.77303796 \n\nNumber of Observations: 1079\nNumber of Groups: 239 \n\n\nIs any of this drop in the \\(p\\)-value due to overfitting? Given the size of the change in AIC, it seems doubtful that that’s a significant factor.\nLet’s try including heteroskedasticity, without AR[1]:\n\nmodelKS6 = lme(ATTIT ~ MINORITY + age13 + EXPO, \n              data=nys1,\n              random=~1|ID,\n              weights=varIdent(form=~1|agefac) )\nAIC( modelKS6 )\n\n[1] -389.5696\n\n\nThis did not improve AIC in this case, so we can avoid looking at this model further.\nFor completeness, let’s look at a model with both AR(1) and heteroskedasticity:\n\nmodelKS7 = lme(ATTIT ~ MINORITY + age13 + EXPO, \n              data=nys1,\n              random=~1|ID,\n              correlation=corAR1(form=~AGE|ID),\n              weights=varIdent(form=~1|agefac) )\nAIC( modelKS7 )\n\n[1] -431.1943\n\n\nAgain, no improvement. So we settle with our AR[1] model with a random intercept to get overall level of a student."
  },
  {
    "objectID": "lmer_optimization.html#convergence-and-optimization-algorithms",
    "href": "lmer_optimization.html#convergence-and-optimization-algorithms",
    "title": "43  Optimization Algorithms for MLMs",
    "section": "43.1 Convergence and optimization algorithms",
    "text": "43.1 Convergence and optimization algorithms\nUnlike OLS, which has a simple closed-form solution for parameter estimates, multi-level models are complex and often do not have closed-form solutions.1 As a result, programming languages use optimization algorithms to fit models. These optimization algorithms are typically iterative processes that repeatedly test potential values and eventually converge to the model estimates.\nTypically, optimization algorithms involve approximating the log-likelihood function as a multivariate quadratic function. Sometimes this approximation is easy to find and closely matches the true log-likelihood; in these cases, convergence occurs quickly. However, we’ve seen that convergence is trickier when the log-likelihood function is flat near the maximum; it’s also trickier with more complex and fragile likelihoods, like those created by the link functions from Generalized Least Squares (GLS) models."
  },
  {
    "objectID": "lmer_optimization.html#what-to-do-when-your-model-wont-converge",
    "href": "lmer_optimization.html#what-to-do-when-your-model-wont-converge",
    "title": "43  Optimization Algorithms for MLMs",
    "section": "43.2 What to do when your model won’t converge",
    "text": "43.2 What to do when your model won’t converge\nIf your error won’t converge, you might get a warning message like this:\nWarning message: In checkConv(attr(opt, “derivs”), opt\\(par, ctrl = control\\)checkConv, : Model failed to converge with max|grad| = 0.0463355 (tol = 0.001, component 1)\nThis warning message tells us two things. First, remember that we are trying to find the maximum of the likelihood function, or the place where the slope = 0. In the warning, the tol = 0.001 tells us that R will be happy if it finds estimates where the slope \\(\\leq\\) 0.001. It’s also saying that our slope when R stopped converging was 0.0463355.\nSteps that you can take to resolve:\nTo address items #2 and #3, you add a Control option into your lme, lmer, or glmer function. Each of those functions has its own option, but they all take the same arguments:\nBelow are some other optimizer options that you can try. For simplicitly, we’re specifying them all as “glmer” options, but you could easily adjust them to match whichever model you are trying (but failing) to fit:\n\n## Use a Nelder-Mead optimizer\nlog_mod &lt;- glmer(pass ~ (gender + frl_new + f3) + \n                   (gender + frl_new + f3|sch), \n                 data = wide_dat, family = binomial(),\n                 control = glmerControl(optimizer = 'Nelder_Mead'))\n\n## Use a BFGS optimizer \nlog_mod &lt;- glmer(pass ~ (gender + frl_new + f3) + \n                   (gender + frl_new + f3|sch), \n                 data = wide_dat, family = binomial(),\n                 control = glmerControl(optimizer=\"optim\", optimMethod = \"BFGS\"))\n\n#If these aren't working, you can downlaod a special package to use the optimx optimizer\n#install.packages(‘optimx’)\nlibrary(optimx)\nlog_mod &lt;- glmer(pass ~ (gender + frl_new + f3) + \n                   (gender + frl_new + f3|sch), \n                 data = wide_dat, family = binomial(),\n                 glmerControl(optimizer = 'optimx', calc.derivs = FALSE,\n                              optCtrl = list(method = \"L-BFGS-B\", \n                                             starttests = FALSE, \n                                             kkt = FALSE)))\n\nAside from these examples, there are many other ways to adjust your optimization commands, which can be found here: https://rdrr.io/cran/lme4/man/lmerControl.html"
  },
  {
    "objectID": "lmer_optimization.html#technical-appendix-understanding-the-types-of-optimization-algorithms",
    "href": "lmer_optimization.html#technical-appendix-understanding-the-types-of-optimization-algorithms",
    "title": "43  Optimization Algorithms for MLMs",
    "section": "43.3 Technical Appendix: Understanding the Types of Optimization Algorithms",
    "text": "43.3 Technical Appendix: Understanding the Types of Optimization Algorithms\nThere are generally four “types” of algorithms employed to find MLE/REML solutions:"
  },
  {
    "objectID": "lmer_optimization.html#newton-methods",
    "href": "lmer_optimization.html#newton-methods",
    "title": "43  Optimization Algorithms for MLMs",
    "section": "43.4 Newton Methods",
    "text": "43.4 Newton Methods\nNewton’s method is the most “pure” of these approaches; essentially Newton’s method uses a Taylor series approximation to approximate a quadratic function and find its maxima. It involves finding the Hessian (a matrix containing all the second and partial derivatives from your likelihood). An advantage of this approach is that it is theoretically the best of the three named approaches because it will often require fewer iterations to converge. However, there are two drawbacks:"
  },
  {
    "objectID": "lmer_optimization.html#quasi-newton-methods",
    "href": "lmer_optimization.html#quasi-newton-methods",
    "title": "43  Optimization Algorithms for MLMs",
    "section": "43.5 Quasi-Newton Methods",
    "text": "43.5 Quasi-Newton Methods\nQuasi-Newton methods start with a “guess” for the Hessian, apply the quadratic formula to attain a new point, update the guess of the Hessian, and repeat until convergence is attained. Importantly, the approximated Hessian will converge to the Hessian so long as the Wolfe conditions (a set of conditions on the likelihood) are satisfied. The easiest guess for the initial Hessian is the identity matrix, making the first step simply a gradient descent. When the identity matrix is used as an initial guess, the quasi-Newton methods converge “super-linearly”–that is it displays linear convergece initially, but approach quadratic convergence as the approximated Hessian updates itself. There are many quasi-Newton methods, but the most common is the “BFGS” updating method.\nIn terms of time to convergence, quasi-Newton is typically much faster than pure Newton methods. This addresses the first drawback listed for Newton’s method, but it is still susceptible to the second issue. The other potential challenge with Quasi-Newton methods occurs when the Wolfe conditions are not satisfied - the method will typically not converge to the Hessian within a reasonable number of iterations, and can often exceed the maximum iterations set by a program."
  },
  {
    "objectID": "lmer_optimization.html#em-expectation-maximiation-algorithm",
    "href": "lmer_optimization.html#em-expectation-maximiation-algorithm",
    "title": "43  Optimization Algorithms for MLMs",
    "section": "43.6 EM (Expectation-Maximiation) Algorithm",
    "text": "43.6 EM (Expectation-Maximiation) Algorithm\nThe EM algorithm is another way of approximating the likelihood function and maximizing that approximation. It does this in a repeating series of stseps: the E (Expectation) step and the M (Maximization) step. In random effect models, where normality is assumed, the E-step results in an a quadratic function to be maximized in the M-step. Importantly, each iteration of the EM algorithm is guaranteed to increase the likelihood function, a feature that that may be too difficult to attain with the Newton methods when a quadratic function is not yet a good approximation. Thus, even if the likelihood function not well approximated by a quadratic function, we are assured to be getting closer to a maximum with the EM algorithm. Thus the EM algorithm fixes the second issue from Newton’s method. However, it only displays linear convergence (as opposed to “super linear” or “quadratic”) and can therefore take a very long time to converge."
  },
  {
    "objectID": "lmer_optimization.html#implementation-in-different-programs",
    "href": "lmer_optimization.html#implementation-in-different-programs",
    "title": "43  Optimization Algorithms for MLMs",
    "section": "43.7 Implementation in Different Programs",
    "text": "43.7 Implementation in Different Programs\n\n43.7.1 Stata/MPlus/HLM\nStata, Mplus, and HLM, each use a combination of the EM and the quasi-Newton methods when estimating models with random effects. The algorithms start with the EM algorithm and proceed until there is sufficient concavity to switch a quasi-Newton method. Using a combination of the EM and quasi-Newton methods minimizes computational time while maximizing the opportunity that the algorithm will converge to a maximum. Mplus and HLM will even switch back to the EM algorithm if the Wolfe conditions are not attained in a set amount of time; thus, my experience has been that Mplus and HLM tend to converge the fastest and tend to minimize convergence issues.\nDisclaimer: sometimes you may need to manually increase the number of EM iterations allowed to acheive convergence.\n\n\n43.7.2 R\nIf I am interpreting the lmerControls documentation correctly, this method starts with the EM algorithm and then applies “unconstrained and box-constrained optimization using PORT routines” from the nlminb function. I’ll classify this algorithm as “other”, as opposed to the three named approaches above.\nIn my opinion, lme’s optimization algorithm is less than ideal for two reasons. First, the number of initial EM steps is fixed and who’s to say that the default number of EM iterations will bring us to a region where the log-likelihood function is sufficiently concave?\nSecond, HLM and Mplus have been estimating random effect models for a long time, and developers from both have come to the conclusion that the quasi-Newton method as the second method in a combination is the best for these models. I’ll assume this is a very informed decision on the end of these developers. Yet, it does not appear that this is what is occuring in R. Instead, R uses “unconstrained and box-constrained optimization using PORT routines,” whatever that is.\nEven the according to the “See Also” section in the nlminb help file, the optim function is listed as preferred over the nlminb function. As it turns out, the optim function applies the “BFGS” quasi-Newton method as the default, which is consistent with Stata’s approach."
  },
  {
    "objectID": "lmer_optimization.html#footnotes",
    "href": "lmer_optimization.html#footnotes",
    "title": "43  Optimization Algorithms for MLMs",
    "section": "",
    "text": "“Closed form” means that there is a formula you can use to simply and directly calculate your estimates. For example, in OLS your matrix equation for \\(\\hat{\\beta} = (X'X)^{-1}X'Y\\)↩︎"
  },
  {
    "objectID": "cluster_demo.html#robust-errors-no-clustering",
    "href": "cluster_demo.html#robust-errors-no-clustering",
    "title": "44  Walk-through of calculating robust standard errors",
    "section": "44.1 Robust errors (no clustering)",
    "text": "44.1 Robust errors (no clustering)\nThe (no clustering, ordinary) linear regression model assumes that\n\\[y = X\\beta + \\varepsilon\\]\nwith the \\(\\varepsilon\\)’s independently and identically normally distributed with variance \\(\\sigma^2\\). Here \\(\\beta\\) is a column vector of regression coefficients, \\((\\beta_0, \\beta_1)\\) in our example. \\(y\\) is a vector of the outcomes and \\(\\varepsilon\\) is a vector of the residuals. \\(X\\) is a \\(n\\) by \\(p\\) matrix referred to as the model matrix (p is the number of predictors, including the intercept). In this example, the first column of the matrix is all 1’s, for the intercept, and the second column is each person’s value for ses. The third is each person’s value for sector (which will be the same for all students in a single school).\n\ndat = read.spss( \"data/hsb1.sav\", to.data.frame=TRUE )\nsdat = read.spss( \"data/hsb2.sav\", to.data.frame=TRUE )\ndat = merge( dat, sdat, by=\"id\", all.x=TRUE )\ndat = dat[ c( \"id\", \"mathach\", \"ses\", \"sector\" ) ]\ndat$id &lt;- factor( dat$id ) ### make the school variable a factor\nhead( dat )\n\n    id mathach    ses sector\n1 1224   5.876 -1.528      0\n2 1224  19.708 -0.588      0\n3 1224  20.349 -0.528      0\n4 1224   8.781 -0.668      0\n5 1224  17.898 -0.158      0\n6 1224   4.583  0.022      0\n\n\nMaking a model matrix from a regression\n\nX &lt;- model.matrix( mathach ~ ses + sector, data = dat )\nhead( X )\n\n  (Intercept)    ses sector\n1           1 -1.528      0\n2           1 -0.588      0\n3           1 -0.528      0\n4           1 -0.668      0\n5           1 -0.158      0\n6           1  0.022      0\n\ny &lt;- dat$mathach\nhead( y )\n\n[1]  5.876 19.708 20.349  8.781 17.898  4.583\n\n\nWith these assumptions, our estimate for \\(\\beta\\) using the OLS criterion is \\(\\hat{\\beta} = (X^TX)^{-1}X^Ty\\). We can calculate this directly with R.\n\nsolve(t(X) %*% X) %*% t(X) %*% y ##(X'X)^{-1}X'y\n\n                 [,1]\n(Intercept) 11.793254\nses          2.948558\nsector       1.935013\n\n\nCompare with lm: they are the same!\n\nmod = lm(mathach ~ ses + sector, data = dat)\nmod \n\n\nCall:\nlm(formula = mathach ~ ses + sector, data = dat)\n\nCoefficients:\n(Intercept)          ses       sector  \n     11.793        2.949        1.935  \n\n\nWe can also estimate standard errors for the coefficients by taking \\(\\sqrt{\\hat{\\sigma}^2diag((X^TX)^{-1})}\\).\n\nbeta_hat &lt;- solve(t(X) %*% X) %*% t(X) %*% y\npreds &lt;- X %*% beta_hat\nresids &lt;- y - preds\nsigma_2_hat &lt;- sum(resids^2)/(nrow(X)-3) ### estimate of the residual variance\nsqrt(sigma_2_hat * diag(solve(t(X) %*% X))) ### using the matrix algebra\n\n(Intercept)         ses      sector \n 0.10610213  0.09783058  0.15249341 \n\n\nAgain, compare:\n\nlibrary( arm )\ndisplay( mod ) ### same results\n\nlm(formula = mathach ~ ses + sector, data = dat)\n            coef.est coef.se\n(Intercept) 11.79     0.11  \nses          2.95     0.10  \nsector       1.94     0.15  \n---\nn = 7185, k = 3\nresidual sd = 6.35, R-Squared = 0.15\n\n\nBut notice that this assumes that the residuals have a single variance, \\(\\sigma^2\\). Frequently this assumption is implausible, in which case the standard errors we derive may not be correct. It would be useful to have a way to derive standard errors which does not require us to assume that the residuals are homoscedastic. This is where heteroscedasticity-robust standard errors, or Huber-White standard errors, come in. Huber-White standard errors are asymptotically correct, even if the residual variance is not constant at all values of the predictor.\nThe basic idea behind Huber-White standard errors is that we let each individual residual serve as an estimate of the variance of the residuals at that value of the predictors. If we let \\(V = (X^TX)^{-1},\\) \\(N\\) be the number of observations, and \\(K\\) be the number of predictors, including the intercept, then the formula for the standard errors is\n\\[ SE^2 = \\frac{N}{N-K} \\cdot diag\\left( V \\cdot \\left( \\sum X_i X_i^T \\varepsilon_i^2 \\right) \\cdot V\\right) \\]\nThis is called a sandwich estimator, where \\(V\\) is the bread and \\(\\sum X_i X_i^T \\varepsilon_i^2\\) (which is a \\(K\\) by \\(K\\) matrix) is the meat. Below, we implement this in R.\n\nN &lt;- nrow(dat) ### number of observations\nK &lt;- 3 ### number of regression coefficients, including the intercept\nV &lt;- solve(t(X) %*% X) ### the bread\nV\n\n              (Intercept)           ses        sector\n(Intercept)  2.796108e-04  3.460078e-05 -0.0002847979\nses          3.460078e-05  2.377141e-04 -0.0000702375\nsector      -2.847979e-04 -7.023750e-05  0.0005775742\n\nmeat &lt;- matrix(0, nrow = K, ncol = K) ### we'll build the meat as we go, iterating over the \n                                      ### individual rows\nfor(i in 1:nrow(dat)){\n  this_point &lt;- X[i, ] %*% t(X[i, ]) * resids[i]^2 ### the contribution of this particular \n                                                   ### point\n  meat &lt;- meat + this_point ### take the current meat, and add this point's contribution\n}\nmeat\n\n     (Intercept)        ses     sector\n[1,]  289161.019  -3048.176 133136.299\n[2,]   -3048.176 159558.729   9732.201\n[3,]  133136.299   9732.201 133136.299\n\nSEs = sqrt(diag(N/(N-K) * V %*% meat %*% V)) ### standard errors\nSEs\n\n(Intercept)         ses      sector \n 0.11021454  0.09487279  0.15476724 \n\n\nNotice that the estimated standard errors haven’t changed much, so whatever heteroscedasticity is present in this association doesn’t seem to be affecting them.\nCombining the above steps in a tidy bit of code gives:\n\nmod &lt;- lm(mathach ~ ses + sector, data = dat)\nresids = resid( mod )\n\nX &lt;- model.matrix(mathach ~ ses + sector, data = dat)\n\nV &lt;- solve(t(X) %*% X) ### the bread\nvcov_hw = V %*% t(X) %*% diag(resids^2) %*% X %*% V\n\nvcov_hw\n\n             (Intercept)          ses       sector\n(Intercept)  0.012142174  0.001957716 -0.012535538\nses          0.001957716  0.008997088 -0.003992666\nsector      -0.012535538 -0.003992666  0.023942897\n\nsqrt(diag(vcov_hw)) ### standard errors\n\n(Intercept)         ses      sector \n 0.11019153  0.09485298  0.15473493 \n\nsqrt( diag( vcov( mod ) ) )\n\n(Intercept)         ses      sector \n 0.10610213  0.09783058  0.15249341 \n\n\n\n44.1.1 R Packages to do all this for you\nThere is an R package to do all of this for us. The following gives us the “Variance Covariance” matrix:\n\nlibrary(sandwich)\nvc &lt;- vcovHC( mod, type = \"HC0\")\nprint( vc, digits=3 )\n\n            (Intercept)      ses   sector\n(Intercept)     0.01214  0.00196 -0.01254\nses             0.00196  0.00900 -0.00399\nsector         -0.01254 -0.00399  0.02394\n\n\nThe square root of the diagonal are our standard errors\n\nsqrt( diag( vc ) )\n\n(Intercept)         ses      sector \n 0.11019153  0.09485298  0.15473493 \n\n\nThey are what we hand-calculated above (up to some rounding error). Observe how the differences are all very close to zero:\n\nsqrt( diag( vc ) ) - SEs\n\n  (Intercept)           ses        sector \n-2.301170e-05 -1.980850e-05 -3.231386e-05 \n\n\nWe can use them for testing as follows\n\nlibrary( lmtest )\ncoeftest( mod, vcov. = vc )\n\n\nt test of coefficients:\n\n             Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) 11.793254   0.110192 107.025 &lt; 2.2e-16 ***\nses          2.948558   0.094853  31.086 &lt; 2.2e-16 ***\nsector       1.935013   0.154735  12.505 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n(Note the weird “.”. I don’t know why it is part of the name.)\nIn fact, these packages play well together, so you can tell lmtest to use the vcovHC function as follows:\n\ncoeftest( mod, vcov. = vcovHC )\n\n\nt test of coefficients:\n\n             Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) 11.793254   0.110237 106.981 &lt; 2.2e-16 ***\nses          2.948558   0.094913  31.066 &lt; 2.2e-16 ***\nsector       1.935013   0.154801  12.500 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAll this is well and good, but everything we have done so far is WRONG because we have failed to account for the clustering of students within schools. Huber-White (Sandwich) corrections only deal with heteroskedasticity, not clustering. We extend these ideas to do clustering next."
  },
  {
    "objectID": "cluster_demo.html#cluster-robust-standard-errors",
    "href": "cluster_demo.html#cluster-robust-standard-errors",
    "title": "44  Walk-through of calculating robust standard errors",
    "section": "44.2 Cluster Robust Standard Errors",
    "text": "44.2 Cluster Robust Standard Errors\nThe next step is to get standard errors which allow the residuals to be correlated within clusters and to have non-0 means within clusters (which violates the assumption of independence of residuals). The math here is harder to explain. We start by calculating \\(X*\\varepsilon\\), multiplying each row in \\(X\\) by the associated residual. Then we take the column sum of \\(X\\) within each cluster. This is easiest to understand for the intercept column, where the sum is simply equal to the sum of the residuals in that cluster. If all of the residuals in a cluster are large and positive (or large and negative), then this sum will be very large; if the residuals are close to mean 0 in a cluster, the sum will be small. We then bind the results into a \\(M\\) by \\(K\\) matrix, where \\(M\\) is the number of clusters, each row corresponds to a cluster, and each column corresponds to a coefficient, which we’ll call \\(U\\). This is the meat which we sandwich with \\(V\\). Finally, we take\n\\[\\sqrt{ diag( \\frac{M}{M-1}\\frac{N-1}{N-K} VU^TUV)}\\]\nwhich gives us estimated standard errors for the regression coefficients.\nThe intuition isn’t so clear here, but notice that the more highly correlated residuals are within clusters (especially clusters with extreme values of the predictors), the larger \\(U^TU\\) will be, and the less precise our estimates.\nHere’s a “by hand” implementation in R.\n\ncluster &lt;- dat$id\nM &lt;- length(unique(cluster))\nweight_mat &lt;- as.vector(resids) * X ### start by calculating for each X predictor values \n                                    ### weighted by the residuals\nhead( weight_mat )\n\n  (Intercept)        ses sector\n1   -1.411858  2.1573194      0\n2    9.648498 -5.6733165      0\n3   10.112584 -5.3394444      0\n4   -1.042618  0.6964687      0\n5    6.570618 -1.0381576      0\n6   -7.275123 -0.1600527      0\n\nu_icept &lt;- tapply(weight_mat[, '(Intercept)'], cluster, sum) ### sum up the weighted \n                                                             ### intercepts in each cluster\nu_ses &lt;- tapply(weight_mat[, 'ses'], cluster, sum) ### sum up the weighted slopes in \n                                                       ### each cluster\nu_sector &lt;- tapply(weight_mat[, 'sector'], cluster, sum)\n\nu &lt;- cbind(u_icept, u_ses, u_sector)\n\n### cluster-robust standard errors\nSE.adj.hand = sqrt((M/(M-1))*((N-1)/(N-K)) * diag(V %*% t(u) %*% u %*% V)) \nSE.adj.hand\n\n(Intercept)         ses      sector \n  0.2031455   0.1279373   0.3171766 \n\n\nThese are a lot higher than before; there’s a lot of within-cluster correlation, and our OLS-based estimated standard errors are unrealistically small.\nYou can use these standard errors in general if you’re not interested in modeling what’s happening at the cluster level and just want to get the right standard errors for your fixed effects.\n\n44.2.1 Using R Packages\nThere is a package that gives you the cluster-robust estimate of the variance-covariance matrix. You can then use this matrix to get your adjusted standard errors:\n\nlibrary( multiwayvcov )\n\nm1 &lt;- lm( mathach ~ ses + sector, data=dat )\nvcov_id &lt;- cluster.vcov(m1, dat$id)\ncoeftest(m1, vcov_id)\n\n\nt test of coefficients:\n\n            Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) 11.79325    0.20315 58.0532 &lt; 2.2e-16 ***\nses          2.94856    0.12794 23.0469 &lt; 2.2e-16 ***\nsector       1.93501    0.31718  6.1007 1.111e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCompare to if we ignored clustering:\n\ncoeftest( m1 )  ## BAD!!\n\n\nt test of coefficients:\n\n             Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) 11.793254   0.106102 111.150 &lt; 2.2e-16 ***\nses          2.948558   0.097831  30.139 &lt; 2.2e-16 ***\nsector       1.935013   0.152493  12.689 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe can look at how much bigger they are:\n\nSE.adj = sqrt( diag( vcov_id ) )\nSE.bad = sqrt( diag( vcov( m1 ) ) )\nSE.adj / SE.bad\n\n(Intercept)         ses      sector \n   1.914623    1.307743    2.079937 \n\n\nMore than 100% bigger for our sector variable and intercept. The ses variable is less so, since it varies within cluster.\nFinally, we check to see that our hand-calculation is the same as the package:\n\nSE.adj.hand - SE.adj\n\n  (Intercept)           ses        sector \n 1.307288e-14 -1.831868e-15 -2.498002e-15 \n\n\nUp to rounding errors, we are the same!\n\n\n44.2.2 Aside: Making your own function\nThe following is code to generate the var-cor matrix more efficiently. For reference (or to ignore):\n\n cl &lt;- function(dat, fm, cluster){\n   attach(dat, warn.conflicts = F)\n   require(sandwich)\n   require(lmtest)\n   M &lt;- length(unique(cluster))\n   N &lt;- length(cluster)\n   K &lt;- fm$rank\n   dfc &lt;- (M/(M-1))*((N-1)/(N-K))\n   uj  &lt;- apply(estfun(fm), 2, function(x) \n                       tapply(x, cluster, sum));\n   vcovCL &lt;- dfc*sandwich(fm, meat=crossprod(uj)/N)\n   coeftest(fm, vcovCL)\n }\n \ncl(dat, mod, dat$id)\n\n\nt test of coefficients:\n\n            Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) 11.79325    0.20315 58.0532 &lt; 2.2e-16 ***\nses          2.94856    0.12794 23.0469 &lt; 2.2e-16 ***\nsector       1.93501    0.31718  6.1007 1.111e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Antonakis, John, Nicolas Bastardoz, and Mikko Rönkkö. 2019. “On\nIgnoring the Random Effects Assumption in Multilevel Models: Review,\nCritique, and Recommendations.” Organizational Research\nMethods 24 (2): 443–83. https://doi.org/10.1177/1094428119877457."
  }
]