[
  {
    "objectID": "fe_crve.html",
    "href": "fe_crve.html",
    "title": "19  A tour of fixed effects and cluster-robust SEs",
    "section": "",
    "text": "20 Aggregation\nOne way forward is to aggregate our HS&B data and merge it into our school-level data, and then analyze the result.\nWe aggregate as so:\ncol.dat = dat %&gt;% group_by( id ) %&gt;% \n    summarize( per.fem = mean(female),\n                 per.min = mean(minority),\n                 mean.ses = mean(ses),\n                 mean.ach = mean(mathach),\n                 n.stud = n() )\n\n# combine our school-level variables (ours and theirs) into one data.frame\nsdat = merge( sdat, col.dat, by=\"id\", all=TRUE )\nhead( sdat )\n\n    id size sector pracad disclim himinty meanses   per.fem    per.min\n1 1224  842      0   0.35   1.597       0  -0.428 0.5957447 0.08510638\n2 1288 1855      0   0.27   0.174       0   0.128 0.4400000 0.12000000\n3 1296 1719      0   0.32  -0.137       1  -0.420 0.6458333 0.97916667\n4 1308  716      1   0.96  -0.622       0   0.534 0.0000000 0.40000000\n5 1317  455      1   0.95  -1.694       1   0.351 1.0000000 0.72916667\n6 1358 1430      0   0.25   1.535       0  -0.014 0.3666667 0.10000000\n     mean.ses  mean.ach n.stud\n1 -0.43438298  9.715447     47\n2  0.12160000 13.510800     25\n3 -0.42550000  7.635958     48\n4  0.52800000 16.255500     20\n5  0.34533333 13.177687     48\n6 -0.01966667 11.206233     30\nWe can now answer our research question with a school-level regression:\nMagg = lm( mean.ach ~ 1 + sector + mean.ses, data=sdat )\n\nlibrary( sandwich )\nlibrary( lmtest )\ncoeftest( Magg, vcov. = vcovHC, type = \"HC0\" )\n\n\nt test of coefficients:\n\n            Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) 12.11950    0.18743 64.6605 &lt; 2.2e-16 ***\nsector       1.22194    0.31959  3.8235 0.0001894 ***\nmean.ses     5.38738    0.33769 15.9538 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nWe use heteroskedastic robust standard errors to take into account possible heteroskedasticity due to, for example, some school outcomes being based on smaller numbers of students (and thus having more variation) than other school outcomes.\nIn this regression we are controlling for school mean SES, not student SES. If anything is going on within school between SES and math achievement, and that is maybe different for different sectors, we might be missing it.\nInstead of using our aggregated data, we can merge our school-level variables into the studnet data and run a student level regression:\nThe merge brings in level 2 variables, repeating them for each student in a school:\ndat = merge( dat, sdat, by=\"id\" )\nhead( dat )\n\n    id minority female    ses mathach size sector pracad disclim himinty\n1 1224        0      1 -1.528   5.876  842      0   0.35   1.597       0\n2 1224        0      1 -0.588  19.708  842      0   0.35   1.597       0\n3 1224        0      0 -0.528  20.349  842      0   0.35   1.597       0\n4 1224        0      0 -0.668   8.781  842      0   0.35   1.597       0\n5 1224        0      0 -0.158  17.898  842      0   0.35   1.597       0\n6 1224        0      0  0.022   4.583  842      0   0.35   1.597       0\n  meanses   per.fem    per.min  mean.ses mean.ach n.stud\n1  -0.428 0.5957447 0.08510638 -0.434383 9.715447     47\n2  -0.428 0.5957447 0.08510638 -0.434383 9.715447     47\n3  -0.428 0.5957447 0.08510638 -0.434383 9.715447     47\n4  -0.428 0.5957447 0.08510638 -0.434383 9.715447     47\n5  -0.428 0.5957447 0.08510638 -0.434383 9.715447     47\n6  -0.428 0.5957447 0.08510638 -0.434383 9.715447     47\nAnd now our regression\nMstud = lm( mathach ~ 1 + sector + ses, data = dat )\nsummary( Mstud )\n\n\nCall:\nlm(formula = mathach ~ 1 + sector + ses, data = dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.1339  -4.6785   0.1504   4.8894  16.5847 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 11.79325    0.10610  111.15   &lt;2e-16 ***\nsector       1.93501    0.15249   12.69   &lt;2e-16 ***\nses          2.94856    0.09783   30.14   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.345 on 7182 degrees of freedom\nMultiple R-squared:  0.1492,    Adjusted R-squared:  0.149 \nF-statistic: 629.8 on 2 and 7182 DF,  p-value: &lt; 2.2e-16\nWe can combine fixed effects and cluster robust standard errors quite easily, but we cannot combine fixed effects and level 2 covariates at all. We next look at this latter problem, and then see what combining these options looks like when asking questions that do not rely on level 2 variables for main effects."
  },
  {
    "objectID": "fe_crve.html#getting-the-right-standard-errors",
    "href": "fe_crve.html#getting-the-right-standard-errors",
    "title": "19  A tour of fixed effects and cluster-robust SEs",
    "section": "21.1 Getting the right standard errors",
    "text": "21.1 Getting the right standard errors\nThe standard errors for the above regression, however, is wrong: we are not taking the clustering into account. We can fix this with cluster-robust standard errors:\n\ncoeftest( Mstud, vcov. = vcovCL, cluster = dat$id )\n\n\nt test of coefficients:\n\n            Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) 11.79325    0.20315 58.0532 &lt; 2.2e-16 ***\nsector       1.93501    0.31718  6.1007 1.111e-09 ***\nses          2.94856    0.12794 23.0469 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCatholic schools score 2 points higher than Public, on average, beyond individual level SES.\nWe can further control for school mean SES, like with aggregation:\n\nMstud2 = lm( mathach ~ 1 + sector + ses + meanses, data = dat )\ncoeftest( Mstud2, vcov. = vcovCL, cluster = dat$id )\n\n\nt test of coefficients:\n\n            Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) 12.09792    0.17024 71.0629 &lt; 2.2e-16 ***\nsector       1.28039    0.30008  4.2669 2.008e-05 ***\nses          2.19116    0.12981 16.8802 &lt; 2.2e-16 ***\nmeanses      2.97267    0.36491  8.1464 4.388e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe contextual value of school mean SES is explaining some of the difference between Catholic and public schools, here: note the reduction of the coefficient for sector. That being said, and still accounting for clustering, sector is still quite significant.\nConfidence intervals give:\n\ncoefci(Mstud2, vcov. = vcovCL, cluster = dat$id )\n\n                 2.5 %    97.5 %\n(Intercept) 11.7641926 12.431643\nsector       0.6921486  1.868627\nses          1.9367009  2.445618\nmeanses      2.2573428  3.687995\n\n\nRelative to the overall standard deviation of math achievement we have:\n\nsd_math = sd( dat$mathach )\nsd_math\n\n[1] 6.878246\n\ncoefci(Mstud2, vcov. = vcovCL, cluster = dat$id ) / sd_math\n\n                2.5 %    97.5 %\n(Intercept) 1.7103478 1.8073858\nsector      0.1006286 0.2716720\nses         0.2815690 0.3555583\nmeanses     0.3281858 0.5361825\n\n\nThe difference between Catholic and public schools is somewhere between 0.10 and 0.27 standard deviations, beyond what can be explained by ses."
  },
  {
    "objectID": "fe_crve.html#the-problem-of-fixed-effects-and-dummy-variables",
    "href": "fe_crve.html#the-problem-of-fixed-effects-and-dummy-variables",
    "title": "19  A tour of fixed effects and cluster-robust SEs",
    "section": "22.1 The problem of fixed effects and dummy variables",
    "text": "22.1 The problem of fixed effects and dummy variables\nFixed effects cannot be used to take into account school differences if we are interested in level 2 variables, because the fixed effects and level 2 variables are co-linear. Put another way, if we let each school have its own mean outcome (represented by the coefficient for a dummy variable for that school), then we can’t have a variable like sector to measure how Catholic schools are different from public schools, conditioned on all the school mean outcomes. There is nothing left to explain as, by construction, there are no differences in school mean outcomes once we “control for” the individual school mean outcomes via fixed effects!\nWhat R will do when you give colinear variables is drop the extra ones. Here is a mini-example fake dataset of 4 schools with 3 students in each school:\n\nfake = tibble( id = c( 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4),\n                   mathach = rnorm( 12 ),\n                   ses = rnorm( 12 ),\n                   sector = c( 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1 ) )\nfake$id = as.factor( fake$id )\nfake\n\n# A tibble: 12 × 4\n   id    mathach     ses sector\n   &lt;fct&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 1      1.31   -1.05        0\n 2 1      2.40    0.229       0\n 3 1      0.330  -0.695       0\n 4 2     -0.0249  0.280       1\n 5 2     -0.216  -0.526       1\n 6 2     -0.172  -0.435       1\n 7 3     -1.17   -1.30        0\n 8 3      1.02    0.197       0\n 9 3      0.354   1.57        0\n10 4      0.432  -0.263       1\n11 4     -0.565  -0.618       1\n12 4      2.01   -0.0752      1\n\n\nAnd our regression model with fixed effects for school plus our school-level ses gives this:\n\nlm( mathach ~ 0 + id + ses + sector, data = fake )\n\n\nCall:\nlm(formula = mathach ~ 0 + id + ses + sector, data = fake)\n\nCoefficients:\n     id1       id2       id3       id4       ses    sector  \n 1.71103   0.02661  -0.04497   0.85558   0.72354        NA  \n\n\nNote the NA for sector! We cannot estimate it due to colinearity, so it got dropped."
  },
  {
    "objectID": "fe_crve.html#using-fixed-effects-to-handle-clustering",
    "href": "fe_crve.html#using-fixed-effects-to-handle-clustering",
    "title": "19  A tour of fixed effects and cluster-robust SEs",
    "section": "22.2 Using fixed effects to handle clustering",
    "text": "22.2 Using fixed effects to handle clustering\nThat being said, fixed effects are an excellent way to control for school differences when looking at within-school relationships. For example, we can ask how math relates to SES within schools, controlling for systematic differences across schools.\nHere is the no fixed effect regression, and the fixed effect regression:\n\nMstud3_noFE = lm( mathach ~ 1 + ses, data=dat )\n\ndat$id = as.factor(dat$id)\nMstud3 = lm( mathach ~ 0 + ses + id, data=dat )\nhead( coef( Mstud3 ) )\n\n      ses    id1224    id1288    id1296    id1308    id1317 \n 2.191172 10.667255 13.244353  8.568302 15.098561 12.421003 \n\n\nFor our fixed effect model, we will have lots of coefficients because we have a fixed effect for each school; the head() command is just showing us the first few. We also had to explicitly make our id variable a factor (categorical variable), so R doesn’t think it is a continuous covariate.\nFor our standard errors, etc., we can further account for clustering of our residuals above and beyond what can be explained by our fixed effects (even if we subtract out the mean outcome, we might still have dependencies between students within a given school). So we use our cluster-robust standard errors as so:\n\ncoefs &lt;- coeftest( Mstud3, vcov. = vcovCL, cluster = dat$id )\nhead( coefs )\n\n        Estimate Std. Error   t value     Pr(&gt;|t|)\nses     2.191172 0.13124281  16.69556 2.130074e-61\nid1224 10.667255 0.05700964 187.11316 0.000000e+00\nid1288 13.244353 0.01595913 829.89215 0.000000e+00\nid1296  8.568302 0.05584382 153.43332 0.000000e+00\nid1308 15.098561 0.06929621 217.88439 0.000000e+00\nid1317 12.421003 0.04532252 274.05809 0.000000e+00\n\n\nWe have again used head() to just get the first lines. The whole printout would be one line per school, plus the ses coefficient!\nLet’s compare our three models (note the way we omit coefficients with id to drop our fixed effects from the table):\n\nlibrary( texreg )\nscreenreg( list( `No FE`=Mstud3_noFE, `FE` = Mstud3, `FE + CRVE`=coefs ), \n           omit.coef=\"id\" )\n\n\n================================================\n             No FE        FE           FE + CRVE\n------------------------------------------------\n(Intercept)    12.75 ***                        \n               (0.08)                           \nses             3.18 ***     2.19 ***   2.19 ***\n               (0.10)       (0.11)     (0.13)   \n------------------------------------------------\nR^2             0.13         0.83               \nAdj. R^2        0.13         0.82               \nNum. obs.    7185         7185                  \n================================================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n\n\nA few things to note:\n\nNot having fixed effects means we are getting an estimate of the math-ses relationship including school level context. Note the higher point estimate. Often we want to focus on within-school relationships. Fixed effects does this.\nThe standard errors are larger once we include fixed effects; the fixed effects are partially accounting for clustering.\nThe standard errors are even larger when we include CRVE. It is more fully accounting for the clustering, and the fact that the clusters themselves could vary. In general, one should typically use CRVE in addition to fixed effects, if one wants to view the clusters as representative of a larger population (in this case a larger population of schools)."
  },
  {
    "objectID": "fe_crve.html#bonus-interactions-with-level-2-variables-are-ok-even-with-fixed-effects",
    "href": "fe_crve.html#bonus-interactions-with-level-2-variables-are-ok-even-with-fixed-effects",
    "title": "19  A tour of fixed effects and cluster-robust SEs",
    "section": "22.3 Bonus: Interactions with level-2 variables are OK, even with fixed effects",
    "text": "22.3 Bonus: Interactions with level-2 variables are OK, even with fixed effects\nIf we want to see if the relationship of math and SES is different between schools, we can get tricky like so:\n\nMstud4 = lm( mathach ~ 0 + ses + ses:sector + id, data=dat )\nhead( coef( Mstud4 ) )\n\n      ses    id1224    id1288    id1296    id1308    id1317 \n 2.782105 10.923946 13.172496  8.819744 15.498595 12.682641 \n\ntail( coef( Mstud4 ) )\n\n    id9359     id9397     id9508     id9550     id9586 ses:sector \n 14.763044   9.982311  13.772485  10.941590  13.973252  -1.348572 \n\n\nNote interaction terms always get pushed to the end of the list of estimates by R. So we have to pull them out with tail().\nIn the following we compare SEs.\n\na &lt;- coeftest( Mstud4, vcov. = vcovCL, cluster = dat$id )\nlibrary( texreg )\nscreenreg( list( wrong=Mstud4, adjusted=a ), \n           omit.coef=\"id\", single.row = TRUE )\n\n\n================================================\n            wrong               adjusted        \n------------------------------------------------\nses            2.78 (0.14) ***   2.78 (0.16) ***\nses:sector    -1.35 (0.22) ***  -1.35 (0.24) ***\n------------------------------------------------\nR^2            0.83                             \nAdj. R^2       0.82                             \nNum. obs.   7185                                \n================================================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n\n\nIn our second column we are accounting for our clustering with our cluster robust SEs."
  },
  {
    "objectID": "robust_mlm.html",
    "href": "robust_mlm.html",
    "title": "21  MLM and Cluster-Robust Standard Errors",
    "section": "",
    "text": "22 Robust standard errors without multilevel modeling\nAs a reminder, here is how to get classic Huber-White / Sandwich / Heteroskedastic-Robust Standard Errors for vanilla OLS for a single school (school 8857):\nFirst we fit our regression, like we would normally:\none.sch = filter( dat, id == \"8857\" )\nnrow( one.sch )\n\n[1] 64\n\nM0 &lt;- lm( mathach ~ 1 + ses, dat) \narm::display( M0 )\n\nlm(formula = mathach ~ 1 + ses, data = dat)\n            coef.est coef.se\n(Intercept) 12.75     0.08  \nses          3.18     0.10  \n---\nn = 7185, k = 2\nresidual sd = 6.42, R-Squared = 0.13\nThen we use the sandwich and lmtest package:\nlibrary( sandwich )\nlibrary( lmtest )\nlmtest::coeftest(M0, type = \"HC1\")\n\n\nt test of coefficients:\n\n             Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) 12.747396   0.075686 168.424 &lt; 2.2e-16 ***\nses          3.183870   0.097121  32.782 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnd here is cluster robust standard errors when you have clustered data, but have not bothered with multilevel modeling:\nM1 = lm( mathach ~ 1 + ses + sector, data = dat )\nlmtest::coeftest( M1, type = \"CL\", cluster = dat$id )\n\n\nt test of coefficients:\n\n             Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept) 11.793254   0.106102 111.150 &lt; 2.2e-16 ***\nses          2.948558   0.097831  30.139 &lt; 2.2e-16 ***\nsector       1.935013   0.152493  12.689 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nOk, so we have seen how to get robust standard errors in the above; how do we combine them with multilevel modeling? First, let’s fit our multilevel model:\nM2 = lmer( mathach ~ 1 + ses + sector + (1|id), data=dat )\ndisplay( M2 )\n\nlmer(formula = mathach ~ 1 + ses + sector + (1 | id), data = dat)\n            coef.est coef.se\n(Intercept) 11.72     0.23  \nses          2.37     0.11  \nsector       2.10     0.34  \n\nError terms:\n Groups   Name        Std.Dev.\n id       (Intercept) 1.92    \n Residual             6.09    \n---\nnumber of obs: 7185, groups: id, 160\nAIC = 46621.2, DIC = 46601.7\ndeviance = 46606.4\nIf we believe all our MLM assumptions, we can get our vanilla standard errors as so:\nsummary( M2 )$coef\n\n             Estimate Std. Error   t value\n(Intercept) 11.718908  0.2280585 51.385536\nses          2.374711  0.1054911 22.511017\nsector       2.100837  0.3411243  6.158566\nIf we don’t believe them fully, we might want to make our inference more robust. Before we turn to this, first note that our point estimates can be different for MLM vs OLS:\ncoef( M1 )\n\n(Intercept)         ses      sector \n  11.793254    2.948558    1.935013 \n\nfixef( M2 )\n\n(Intercept)         ses      sector \n  11.718908    2.374711    2.100837\nThe assumption of the random effects means we are not weighting all our data the same way. For example, if we find the clusters vary in size a lot, we might weight the clusters more equally when estimating a cluster-level coefficient (e.g., sector) instead of counting on the big clusters more.\nRegardless, we might worry that complex dependencies within our clusters are messing up our standard errors in our MLM, however. Fixing that is easy:\nlibrary( clubSandwich )\nclub &lt;- coef_test( M2,\n           vcov = \"CR1S\",\n           test = \"Satterthwaite\")\nclub\n\n       Coef. Estimate    SE t-stat d.f. (Satt) p-val (Satt) Sig.\n (Intercept)    11.72 0.226  51.86        88.9       &lt;0.001  ***\n         ses     2.37 0.119  19.89       142.8       &lt;0.001  ***\n      sector     2.10 0.347   6.05       149.5       &lt;0.001  ***\nThe clubSandwich package works for multi-level models fit with either lme4::lmer() or nlme::lme(). Note that coef_test is not the same as coeftest. The vcov = \"CR1S\" replicates the Stata SEs (or so it has been speculated, and assuming they use the same correction as for panel data models).\nWe can compare the SEs as so:\nrbind( club$SE, se.fixef(M2) )\n\n     (Intercept)       ses    sector\n[1,]   0.2259713 0.1193704 0.3474632\n[2,]   0.2280585 0.1054911 0.3411243\nWe see that the SEs did not change much from the homoskedastic- and within-cluster-independence-assuming standard errors of the vanilla lmer call in this circumstance.\nSo what is this thing even doing? In the following I describe a rough approximation. The key idea is that a multilevel model specification is specifying a parameterized \\(n \\times n\\) variance-covariance matrix of the residuals of a generic linear model. Due to our assumption of independent clusters, this matrix is block diagonal with blocks \\(V_1, \\ldots, V_J\\), with block \\(V_j\\) corresponding to group \\(j\\). For a random intercept model, for example, block \\(j\\) would be a \\(n_j \\times n_j\\) matrix with \\(\\tau_{00} + \\sigma^2\\) for the diagonal and \\(\\tau_{00}\\) for the off-diagonal, with \\(\\tau_{00}\\) being the variance of the random intercepts and \\(\\sigma^2\\) being the within-block residual variance.\nIf we write our multilevel model in reduced form, we can write it as a mini-regression for each group \\(j\\) of: \\[\nY_j = X_j \\vec{\\beta} + Z_j \\vec{r}_j + e_j ,\n\\] where \\(Y_j\\) is the vector of outcomes, \\(X_j\\) and \\(Z_j\\) are mini design matrices of covariates (including a column of 1s for the intercept, normally), with \\(X_j\\) being all the covariates and \\(Z_j\\) being those covariates that have corresponding random effects (also with a column of 1s), \\(\\vec{\\beta}\\) the vector of coefficients (the fixed effects), \\(\\vec{r}_j\\) the vector of random effects for group \\(j\\), and \\(e_j\\) the vector of residuals.\nImportantly, the \\(u_j := Z_j \\vec{u}_j + e_j\\) is all residual, and \\(V_j = Var( u_j )\\): the variance-covariance matrix of the residuals is determined by this structure and our assumptions on \\(\\vec{u}_j\\) being multivariate normal and the \\(e_j\\) being a vector of independent residual draws (the \\(\\epsilon_{ij}\\)).\nNow, given this view of our multilevel model, we can estimated this with generalized least squares. Generalized least squares is a generic regression technique where, if you have a parameterized covariance matrix on the residuals, you can estimate your regression coefficients taking that correlation structure into account. Think of it as a three-step process: first fit the regression without taking the residuals into account, then use the fit model to estimate our big \\(n \\times n\\) variance-covariance matrix, and then use this estimated matrix as a set of weights that we plug back into a least squares estimation.\nIn particular, the estimator for \\(\\vec{\\beta}\\) is weighted least squares: \\[ \\hat{\\beta} = (X'WX)^{-1}X'W Y ,  \\] with \\(W\\) a weight matrix that is a \\(n \\times n\\) block-diagonal matrix formed from the inverses of the estimated \\(V_j\\). Now if the random effects structure (the assumed distribution of the \\(r_j\\)) is misspecified or the residual error structure (on the \\(\\epsilon_{ij}\\)) is wrong, then \\(V_j\\) will be wrong, but \\(\\hat{\\beta}\\) will still be asymptotically consistent (under some conditions).\nCluster-robust methods use the empirical residuals (the \\(\\hat{u}_{j}\\)) to assess the uncertainty in \\(\\hat{\\beta}\\) as an estimate of the \\(\\beta\\) as defined by the implied weights \\(W\\). Even if the random effects part of the model is wrong, the assumption of independent clusters means our inference on this estimand is still right. The key idea is cluster-robust methods take a weighted average of \\(J\\) very badly estimated variance-covariance matrices to get a decent estimate of overall population-level uncertainty.\nThe main advantage of the clubSandwich package is it will take our multilevel model and do this cluster-roboust standard error calculation. Even better, however, is it will (using “CR2” adjustment) try to improve the basic sandwich estimator by 1) adjusting the residuals (the \\(\\hat{u}_j\\)) a bit so that the variance estimator is exactly unbiased if the working model is exactly correct and b) using Satterthwaite degrees of freedom (or generalizations thereof) for tests/confidence intervals, also derived under the assumption that the working model is exactly correct.\nThanks to James Pustejovsky, the creator of the clubSandwich package, for the help in thinking this through. Much of these notes, in particular the reasons for misspecification and much of the technical notes, are liberally stolen from emails with this fine colleague."
  },
  {
    "objectID": "robust_mlm.html#misspecified-how",
    "href": "robust_mlm.html#misspecified-how",
    "title": "21  MLM and Cluster-Robust Standard Errors",
    "section": "23.1 Misspecified how?",
    "text": "23.1 Misspecified how?\nThe sorts of misspecification that we might be worried about are things such as the following:\n\nUsing a random intercept model when the real data-generating process has a random slope;\nUsing a model that assumes homogeneous random effects when the real data-generating process involves heteroskedasticity (e.g., different random effects variances for treatment schools than for control schools);\nUsing a model with a single level of random effects (e.g., school random effects) when the real data-generating process has multiple levels of structure (e.g., school and classroom random effects); or\nAssuming homoscedastic variance for the lowest-level errors when the real process is heteroskedastic or has some other structure.\n\nOf the above (3) could have “secret clustering” in your clusters, and give you radically incorrect standard errors. The other options are more violations of homoskedasticity, and are likely to not be as serious of concerns. You can also diagnose them with residual plots, and see if you are seeing more scatter in your data for some groups or individuals than others.\nRegardless, if you are worried about these things, then the above will give you improved standard errors."
  },
  {
    "objectID": "fixed_effects.html#the-language-of-fixed-effects",
    "href": "fixed_effects.html#the-language-of-fixed-effects",
    "title": "18  Clarification on Fixed Effects and Identification",
    "section": "18.1 The language of “Fixed Effects”",
    "text": "18.1 The language of “Fixed Effects”\nPeople will talk about “fixed effects” in (at least) two ways. The first is when you have a dummy variable for each of your clusters, and you are using OLS regression (not multilevel modeling). In this case you are estimating a parameter for each cluster, and we refer to that collection of estimates and parameters that go with these cluster level dummy variables as “fixed effects” and the model is a “fixed effects model.” The second is when you are using multilevel modeling, such as the following:\nM0 &lt;- lmer(Y ~ 1 + var1 + var2 + var3 + (var1|id), data)\nWhen we fit the above model, we will be estimating a grand intercept, and three coefficients for the three variables. Call these \\(\\beta_0\\), \\(\\beta_1\\), \\(\\beta_2\\), and \\(\\beta_3\\). We are also estimating a random intercept and random slope for var1, with each group defined by the id variable having its own random intercept and slope. These are described by a variance-covariance matrix that we have been describing with \\(\\tau_{00}, \\tau_{01}, \\tau_{11}\\).\nNow, the \\(\\beta\\) are the fixed part, or fixed effects, of the model. The \\(\\tau\\) describe the random part or random effects. This is why, in R, we say fixef(M0) to get the \\(\\beta\\). If we say ranef(M0) we get the Empirical Bayes estimates of the random parts for each cluster. If we say coef(M0) R adds all this together to give the sum of the fixed part and random part, for each cluster defined by id.\nRead Gelman and Hill 12.3 for more on this sticky language. G&H do not like “fixed effects” as a description because it is so vague."
  },
  {
    "objectID": "fixed_effects.html#underidentification",
    "href": "fixed_effects.html#underidentification",
    "title": "18  Clarification on Fixed Effects and Identification",
    "section": "18.2 Underidentification",
    "text": "18.2 Underidentification\nIf we fit a model with a dummy variable for each cluster, and a level to variable that does not vary within cluster, we say our model is “underidentified.” We say it is underidentified because no matter how much data we have, we will always have an infinite number of parameter values that can describe our model equally well. For example, say our level 2 variable is a dummy variable (e.g., sector). Then a model where we add five to the coefficient of the level 2 variable, and subtract five from all of the fixed effects for the clusters with sector=1 will fit our data just as well as one where we don’t. We can’t tell the difference! Hence we do not have enough to “identify” the parameter values."
  },
  {
    "objectID": "fixed_effects.html#further-reading",
    "href": "fixed_effects.html#further-reading",
    "title": "18  Clarification on Fixed Effects and Identification",
    "section": "18.4 Further Reading",
    "text": "18.4 Further Reading\n(Antonakis, Bastardoz, and Rönkkö 2019)\n\n\n\n\nAntonakis, John, Nicolas Bastardoz, and Mikko Rönkkö. 2019. “On Ignoring the Random Effects Assumption in Multilevel Models: Review, Critique, and Recommendations.” Organizational Research Methods 24 (2): 443–83. https://doi.org/10.1177/1094428119877457."
  },
  {
    "objectID": "fe_discussion.html",
    "href": "fe_discussion.html",
    "title": "20  Thinking about Fixed Effects Models",
    "section": "",
    "text": "21 Introduction\nThis document gives a reflection on the four concept questions from Packet 1.2 (the live session slides).\n\n21.0.1 Model syntax: removing the main ses term vs not\nWe talked about both these two models:\n\nM1 = lm( mathach ~ 0 + ses*id, data=dat.ten )\ncoef( M1 )\n\n       ses     id1288     id3533     id3881     id4530     id5761     id6074 \n 3.2554487 13.1149374 10.3671216 11.6441421 10.0390287 12.1419451 14.2022643 \n    id6170     id8800     id9225     id9347 ses:id3533 ses:id3881 ses:id4530 \n15.6332900  9.1573804 13.9360803 12.9702661 -3.5672188 -0.8647429 -1.6080227 \nses:id5761 ses:id6074 ses:id6170 ses:id8800 ses:id9225 ses:id9347 \n-0.1474381 -1.7263610  1.5563357 -0.6873233 -0.3695565 -0.5694548 \n\nM2 = lm( mathach ~ 0 + ses*id - ses, data=dat.ten )\ncoef( M2 )\n\n    id1288     id3533     id3881     id4530     id5761     id6074     id6170 \n13.1149374 10.3671216 11.6441421 10.0390287 12.1419451 14.2022643 15.6332900 \n    id8800     id9225     id9347 ses:id1288 ses:id3533 ses:id3881 ses:id4530 \n 9.1573804 13.9360803 12.9702661  3.2554487 -0.3117701  2.3907058  1.6474260 \nses:id5761 ses:id6074 ses:id6170 ses:id8800 ses:id9225 ses:id9347 \n 3.1080106  1.5290877  4.8117843  2.5681254  2.8858922  2.6859939 \n\n\nNote how when we remove ses via - ses we gain an extra interaction term of ses:id1288. In M1, our ses coefficient is our baseline slope of school 1288. The ses interaction terms are slope changes.\nNote how if we add ses to the changes we get back all the slopes in M2:\n\ncoef( M1 )[12:20] + coef(M1)[[1]]\n\nses:id3533 ses:id3881 ses:id4530 ses:id5761 ses:id6074 ses:id6170 ses:id8800 \n-0.3117701  2.3907058  1.6474260  3.1080106  1.5290877  4.8117843  2.5681254 \nses:id9225 ses:id9347 \n 2.8858922  2.6859939 \n\n\nBottom line: M1 and M2 are exactly the same in what they are describing, they are just parameterized differently. Anything we learn from one we could learn from the other.\n\n\n21.0.2 Plot our model\nTo plot our model we make a dataset of the intercepts and slopes of each school. Doing this with M2 is much easier than M1, since the coefficients are exactly what we want:\n\nlines = data.frame( id = names( coef(M2) )[1:10],\n                    inter = coef(M2)[1:10],\n                    slope = coef(M2)[11:20] )\n\n# we need to fix our IDs.  :-(\nlines$id = gsub( \"id\", \"\", lines$id)\nhead( lines )\n\n         id    inter      slope\nid1288 1288 13.11494  3.2554487\nid3533 3533 10.36712 -0.3117701\nid3881 3881 11.64414  2.3907058\nid4530 4530 10.03903  1.6474260\nid5761 5761 12.14195  3.1080106\nid6074 6074 14.20226  1.5290877\n\n\n(The gsub “substitutes” (replaces) the string “id” with “” in all of our ids so we get back to the actual school ids. Otherwise we will not be able to connect these data to our raw students as easily.)\nWe now plot!\n\nggplot( dat.ten, aes( ses, mathach ) ) +\n    facet_wrap( ~ id, nrow=2 ) +\n    geom_point( size=0.75, alpha=0.5 ) +\n    geom_abline( data=lines, aes( slope=slope, \n                                  intercept=inter ), \n                 col=\"red\", lwd=1 ) +\n    geom_vline( xintercept = 0 )\n\n\n\n\n\n\n21.0.3 What do the intercepts of any of the lines mean?\nThe intercepts predict what math achievement a studnet with ses = 0 going to a given school would have. For example, in school 8800, we predict a student with an ses of 0 would have a math achievement of 9.2.\nNotice that for some schools the intercept is extrapolating. E.g., most of school 8800’s students are below 0 for ses, and the intercept is thus describing what we expect for students at the higher end of their range. For school 9225, we are seeing a prediction for students a bit below the middle of their range.\n\n\n21.0.4 What differences, if any, are there between running a new linear model on each school vs. running the interacted model on the set of 10 schools?\nThe lines would be exactly the same. The standard errors are different. Here is the line on just school 1288:\n\ns1288 = filter( dat.ten, id == \"1288\" )\nM_1288 = lm( mathach ~ 1 + ses, data=s1288 )\nsummary( M_1288 )\n\n\nCall:\nlm(formula = mathach ~ 1 + ses, data = s1288)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.648  -5.700   1.048   4.420   9.415 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   13.115      1.387   9.456 2.17e-09 ***\nses            3.255      2.080   1.565    0.131    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.819 on 23 degrees of freedom\nMultiple R-squared:  0.09628,   Adjusted R-squared:  0.05699 \nF-statistic:  2.45 on 1 and 23 DF,  p-value: 0.1312\n\n\nThe SEs will be different, however. Compare:\n\nsum = summary( M2 )\nsum$coefficients[ c(1, 11 ), ]\n\n            Estimate Std. Error   t value     Pr(&gt;|t|)\nid1288     13.114937   1.291495 10.154850 9.121968e-22\nses:id1288  3.255449   1.936454  1.681139 9.349559e-02\n\n\nIn this case the SEs are close, but they could be a lot different if we have a lot of heteroskedasticity or the school has few data points so we do a bad job estimating uncertainty.\nThe key is in the single model we are using all the schools to estimate the residual variance, and this is the number that drives our SE estimates.\n\n\n21.0.5 Do we trust the red lines on the plot? Why or why not?\nWe trust them because they are driven just by the school data, so they are essentially unbiased. But these are small datasets, so they are unstable.\n\n\n21.0.6 What about the variability in the slopes and intercepts of the red lines?\nThe variation is not to be trusted. The slopes are varying because of measurement error. For example, it is unlikely school 3533 really has a negative slope. It is more likely we just got some low performing high ses kids by happenstance in our sample. Similarly, it is unlikely school 6170 has such a steep slope. It has few kids, and the kid with less than -2 ses and a very low math achievment is likely an influential point in that regression."
  },
  {
    "objectID": "fixed_effects.html#model-syntax-removing-the-main-ses-term-vs-not",
    "href": "fixed_effects.html#model-syntax-removing-the-main-ses-term-vs-not",
    "title": "18  Clarification on Fixed Effects and Identification",
    "section": "18.3 Model syntax: removing the main ses term vs not",
    "text": "18.3 Model syntax: removing the main ses term vs not\nWe talked about both these two models:\n\nM1 = lm( mathach ~ 0 + ses*id, data=dat.ten )\ncoef( M1 )\n\n       ses     id1288     id3533     id3881     id4530     id5761     id6074 \n 3.2554487 13.1149374 10.3671216 11.6441421 10.0390287 12.1419451 14.2022643 \n    id6170     id8800     id9225     id9347 ses:id3533 ses:id3881 ses:id4530 \n15.6332900  9.1573804 13.9360803 12.9702661 -3.5672188 -0.8647429 -1.6080227 \nses:id5761 ses:id6074 ses:id6170 ses:id8800 ses:id9225 ses:id9347 \n-0.1474381 -1.7263610  1.5563357 -0.6873233 -0.3695565 -0.5694548 \n\nM2 = lm( mathach ~ 0 + ses*id - ses, data=dat.ten )\ncoef( M2 )\n\n    id1288     id3533     id3881     id4530     id5761     id6074     id6170 \n13.1149374 10.3671216 11.6441421 10.0390287 12.1419451 14.2022643 15.6332900 \n    id8800     id9225     id9347 ses:id1288 ses:id3533 ses:id3881 ses:id4530 \n 9.1573804 13.9360803 12.9702661  3.2554487 -0.3117701  2.3907058  1.6474260 \nses:id5761 ses:id6074 ses:id6170 ses:id8800 ses:id9225 ses:id9347 \n 3.1080106  1.5290877  4.8117843  2.5681254  2.8858922  2.6859939 \n\n\nNote how when we remove ses via - ses we gain an extra interaction term of ses:id1288. In M1, our ses coefficient is our baseline slope of school 1288. The ses interaction terms are slope changes.\nNote how if we add ses to the changes we get back all the slopes in M2:\n\ncoef( M1 )[12:20] + coef(M1)[[1]]\n\nses:id3533 ses:id3881 ses:id4530 ses:id5761 ses:id6074 ses:id6170 ses:id8800 \n-0.3117701  2.3907058  1.6474260  3.1080106  1.5290877  4.8117843  2.5681254 \nses:id9225 ses:id9347 \n 2.8858922  2.6859939 \n\n\nBottom line: M1 and M2 are exactly the same in what they are describing, they are just parameterized differently. Anything we learn from one we could learn from the other.\n\n18.3.1 Plot our model\nTo plot our model we make a dataset of the intercepts and slopes of each school. Doing this with M2 is much easier than M1, since the coefficients are exactly what we want:\n\nlines = data.frame( id = names( coef(M2) )[1:10],\n                    inter = coef(M2)[1:10],\n                    slope = coef(M2)[11:20] )\n\n# we need to fix our IDs.  :-(\nlines$id = gsub( \"id\", \"\", lines$id)\nhead( lines )\n\n         id    inter      slope\nid1288 1288 13.11494  3.2554487\nid3533 3533 10.36712 -0.3117701\nid3881 3881 11.64414  2.3907058\nid4530 4530 10.03903  1.6474260\nid5761 5761 12.14195  3.1080106\nid6074 6074 14.20226  1.5290877\n\n\n(The gsub “substitutes” (replaces) the string “id” with “” in all of our ids so we get back to the actual school ids. Otherwise we will not be able to connect these data to our raw students as easily.)\nWe now plot!\n\nggplot( dat.ten, aes( ses, mathach ) ) +\n    facet_wrap( ~ id, nrow=2 ) +\n    geom_point( size=0.75, alpha=0.5 ) +\n    geom_abline( data=lines, aes( slope=slope, \n                                  intercept=inter ), \n                 col=\"red\", lwd=1 ) +\n    geom_vline( xintercept = 0 )\n\n\n\n\n\n\n18.3.2 What do the intercepts of any of the lines mean?\nThe intercepts predict what math achievement a studnet with ses = 0 going to a given school would have. For example, in school 8800, we predict a student with an ses of 0 would have a math achievement of 9.2.\nNotice that for some schools the intercept is extrapolating. E.g., most of school 8800’s students are below 0 for ses, and the intercept is thus describing what we expect for students at the higher end of their range. For school 9225, we are seeing a prediction for students a bit below the middle of their range.\n\n\n18.3.3 What differences, if any, are there between running a new linear model on each school vs. running the interacted model on the set of 10 schools?\nThe lines would be exactly the same. The standard errors are different. Here is the line on just school 1288:\n\ns1288 = filter( dat.ten, id == \"1288\" )\nM_1288 = lm( mathach ~ 1 + ses, data=s1288 )\nsummary( M_1288 )\n\n\nCall:\nlm(formula = mathach ~ 1 + ses, data = s1288)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.648  -5.700   1.048   4.420   9.415 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   13.115      1.387   9.456 2.17e-09 ***\nses            3.255      2.080   1.565    0.131    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.819 on 23 degrees of freedom\nMultiple R-squared:  0.09628,   Adjusted R-squared:  0.05699 \nF-statistic:  2.45 on 1 and 23 DF,  p-value: 0.1312\n\n\nThe SEs will be different, however. Compare:\n\nsum = summary( M2 )\nsum$coefficients[ c(1, 11 ), ]\n\n            Estimate Std. Error   t value     Pr(&gt;|t|)\nid1288     13.114937   1.291495 10.154850 9.121968e-22\nses:id1288  3.255449   1.936454  1.681139 9.349559e-02\n\n\nIn this case the SEs are close, but they could be a lot different if we have a lot of heteroskedasticity or the school has few data points so we do a bad job estimating uncertainty.\nThe key is in the single model we are using all the schools to estimate the residual variance, and this is the number that drives our SE estimates.\n\n\n18.3.4 Do we trust the red lines on the plot? Why or why not?\nWe trust them because they are driven just by the school data, so they are essentially unbiased. But these are small datasets, so they are unstable.\n\n\n18.3.5 What about the variability in the slopes and intercepts of the red lines?\nThe variation is not to be trusted. The slopes are varying because of measurement error. For example, it is unlikely school 3533 really has a negative slope. It is more likely we just got some low performing high ses kids by happenstance in our sample. Similarly, it is unlikely school 6170 has such a steep slope. It has few kids, and the kid with less than -2 ses and a very low math achievment is likely an influential point in that regression."
  }
]