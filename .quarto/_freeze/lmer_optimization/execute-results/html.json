{
  "hash": "142240a4d841a59ba3932bb19cbc1587",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Optimization Algorithms for MLMs\"\nauthor: \"Luke Miratrix\"\neditor: \n  markdown: \n    wrap: sentence\n---\n\n\n\n\n\n\nOne of the perils of fitting MLMs are the wide variety of obscure and upsetting errors you might face when fitting your model.\nMany of these relate to \"failure to converge.\"\nIn this chapter, we'll discuss what this means and how to address it.\n\nUnlike OLS, which has a simple closed-form solution for parameter estimates (meaning your computer can just calculate the estimates directly, using a formula), multi-level models are complex and often do not have closed-form solutions.[^lmer_optimization-1]\nAs a result, programming languages use optimization algorithms to fit models.\nThese optimization algorithms are typically iterative processes that repeatedly test potential values and eventually (we hope) converge to the model estimates.\nWe talked about one way of thinking about this when covering maximum likelihood estimation, with the gradient ascent (hill climbing) idea.\n\n[^lmer_optimization-1]: \"Closed form\" means that there is a formula you can use to simply and directly calculate your estimates.\n    For example, in OLS your matrix equation for $\\hat{\\beta} = (X'X)^{-1}X'Y$\n\nTypically, optimization algorithms involve approximating the log-likelihood function as a multivariate quadratic function.\nSometimes this approximation is easy to find and closely matches the true log-likelihood; in these cases, convergence occurs quickly.\nHowever, we've seen that convergence is trickier when the log-likelihood function is flat near the maximum; it is also trickier with more complex and fragile likelihoods, like those for problems that include link functions from Generalized Least Squares (GLS) models.\nWhen this happens, the algorithms can fail to find a good maximum in the number of iterations given, or can find a large area that is all about the same level, making determining which point is the maximum quite difficult.\nThis is when you get your warning messages.\n\n## What to do when your model won't converge\n\nIf your error won't converge, you might get a warning message like this:\n\n```\nWarning message: In checkConv(attr(opt, \"derivs\"), opt$par, \nctrl = control$checkConv, : Model failed to converge with \nmax\\|grad\\| = 0.0463355 (tol = 0.001, component 1)\n```\n\nThis warning message tells us two things.\nFirst, remember that we are trying to find the maximum of the likelihood function, or the place where the `slope = 0`, where the \"slope\" is how much the likelihood function is changing.\nIn the warning, the `tol = 0.001` tells us that R will be happy if it finds estimates where the `slope` $\\leq$ `0.001`.\nIt's also saying that our slope when R stopped converging was `0.0463355`.\n\nWhen convergence fails, there are a few steps you can take to try and get it to do better:\n\n 1. Try rescaling variables and refitting your model.\n 2. Try changing your optimizer settings.\n\nFor #1, calculate the standard deviation of all your variables and compare them to one another.  If you have one variable with a very large sd relative to the others, try rescaling it.\nFor example, if you had income as a covariate, you might divide it by 1000 to get \"income in thousands of dollars,\" which might make the scale more similar to other variables.\n\nFor #2, you add a `Control` option into your `lme`, `lmer`, or `glmer` function.\nEach of those functions has its own option, but they all take the same arguments:\n\n * lme: lmeControl()\n * lmer: lmerControl()\n * glmer: glmerControl()\n\nBelow are some other optimizer options that you can try.\nFor simplicity, we're specifying them all as \"glmer\" options, but you could easily adjust them to match whichever model you are trying (but failing) to fit:\n\nThe following is a good default that often works:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_mod <- glmer(pass ~ (gender + frl_new + f3) + \n                   (gender + frl_new + f3|sch), \n                 data = wide_dat, family = binomial(),\n                 control = glmerControl(optimizer = 'bobyqa'))\n```\n:::\n\n\n\n\n\n\nYou can also try `optimizer = 'Nelder_Mead'`.\nSometimes the syntax is a bit different:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Use a BFGS optimizer \nlog_mod <- glmer(pass ~ (gender + frl_new + f3) + \n                   (gender + frl_new + f3|sch), \n                 data = wide_dat, family = binomial(),\n                 control = glmerControl(optimizer=\"optim\", optimMethod = \"BFGS\"))\n```\n:::\n\n\n\n\n\n\nIf these aren't working, you can downlaod a special package to use the optimx optimizer:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#install.packages(‘optimx’)\nlibrary(optimx)\nlog_mod <- glmer(pass ~ (gender + frl_new + f3) + \n                   (gender + frl_new + f3|sch), \n                 data = wide_dat, family = binomial(),\n                 glmerControl(optimizer = 'optimx', calc.derivs = FALSE,\n                              optCtrl = list(method = \"L-BFGS-B\", \n                                             starttests = FALSE, \n                                             kkt = FALSE)))\n```\n:::\n\n\n\n\n\n\nThere are many other ways to adjust your optimization commands, which can be found here: https://rdrr.io/cran/lme4/man/lmerControl.html\n\n## Technical Appendix: Understanding the Types of Optimization Algorithms\n\nThere are generally four \"types\" of algorithms employed to find MLE/REML solutions:\n\n * Newton methods \n * Quasi-Newton methods\n * EM algorithm \n * Other\n\n### Newton Methods\n\nNewton's method is the most \"pure\" of these approaches; essentially Newton's method uses a Taylor series approximation to approximate a quadratic function and find its maxima.\nIt involves finding the Hessian (a matrix containing all the second and partial derivatives from your likelihood).\nAn advantage of this approach is that it is theoretically the best of the three named approaches because it will often require fewer iterations to converge.\nHowever, there are two drawbacks:\n\n 1. When there are a large number of parameters, it is time-consuming to analytically calculate or numerically approximate all second order and mixed derivatives needed for the Hessian matrix.\n 2. In regions where the log-likelihood function is not sufficiently concave down, there is a tendency to dramatically overshoot because the step size to the next point is proportional to the inverse of the second derivative, resulting in pathological oscillations that would amplify if allowed to continue. Thus, where the log-likelihood function is not well approximated by a second order Taylor expansion, the method tends to fail miserably. This would be the case, for example, if the log-likelihood function was a standard normal density and you started out 2 SD from the mean.\n\\end{enumerate}\n\n### Quasi-Newton Methods\n\nQuasi-Newton methods start with a \"guess\" for the Hessian, apply the quadratic formula to attain a new point, update the guess of the Hessian, and repeat until convergence is attained.\nImportantly, the approximated Hessian will converge to the Hessian so long as the Wolfe conditions (a set of conditions on the likelihood) are satisfied.\nThe easiest guess for the initial Hessian is the identity matrix, making the first step simply a gradient descent.\nWhen the identity matrix is used as an initial guess, the quasi-Newton methods converge \"super-linearly\"--that is it displays linear convergece initially, but approach quadratic convergence as the approximated Hessian updates itself.\nThere are many quasi-Newton methods, but the most common is the \"BFGS\" updating method.\n\nIn terms of time to convergence, quasi-Newton is typically much faster than pure Newton methods.\nThis addresses the first drawback listed for Newton's method, but it is still susceptible to the second issue.\nThe other potential challenge with Quasi-Newton methods occurs when the Wolfe conditions are not satisfied - the method will typically not converge to the Hessian within a reasonable number of iterations, and can often exceed the maximum iterations set by a program.\n\n### EM (Expectation-Maximiation) Algorithm\n\nThe EM algorithm is another way of approximating the likelihood function and maximizing that approximation.\nIt does this in a repeating series of stseps: the E (Expectation) step and the M (Maximization) step.\nIn random effect models, where normality is assumed, the E-step results in an a quadratic function to be maximized in the M-step.\nImportantly, each iteration of the EM algorithm is guaranteed to increase the likelihood function, a feature that that may be too difficult to attain with the Newton methods when a quadratic function is not yet a good approximation.\nThus, even if the likelihood function not well approximated by a quadratic function, we are assured to be getting closer to a maximum with the EM algorithm.\nThus the EM algorithm fixes the second issue from Newton's method.\nHowever, it only displays linear convergence (as opposed to \"super linear\" or \"quadratic\") and can therefore take a very long time to converge.\n\n## Optimizer Implementation in Different Programs\n\n### Stata/MPlus/HLM\n\nStata, Mplus, and HLM, each use a combination of the EM and the quasi-Newton methods when estimating models with random effects.\nThe algorithms start with the EM algorithm and proceed until there is sufficient concavity to switch a quasi-Newton method.\nUsing a combination of the EM and quasi-Newton methods minimizes computational time while maximizing the opportunity that the algorithm will converge to a maximum.\nMplus and HLM will even switch back to the EM algorithm if the Wolfe conditions are not attained in a set amount of time; thus, my experience has been that Mplus and HLM tend to converge the fastest and tend to minimize convergence issues.\n\nDisclaimer: sometimes you may need to manually increase the number of EM iterations allowed to acheive convergence.\n\n### R\n\nIf I am interpreting the lmerControls documentation correctly, this method starts with the EM algorithm and then applies \"unconstrained and box-constrained optimization using PORT routines\" from the nlminb function.\nI'll classify this algorithm as \"other\", as opposed to the three named approaches above.\n\nIn my opinion, lme's optimization algorithm is less than ideal for two reasons.\nFirst, the number of initial EM steps is fixed and who's to say that the default number of EM iterations will bring us to a region where the log-likelihood function is sufficiently concave?\n\nSecond, HLM and Mplus have been estimating random effect models for a long time, and developers from both have come to the conclusion that the quasi-Newton method as the second method in a combination is the best for these models.\nI'll assume this is a very informed decision on the end of these developers.\nYet, it does not appear that this is what is occuring in R.\nInstead, R uses \"unconstrained and box-constrained optimization using PORT routines,\" whatever that is.\n\nEven the according to the \"See Also\" section in the `nlminb` help file, the optim function is listed as preferred over the nlminb function.\nAs it turns out, the optim function applies the \"BFGS\" quasi-Newton method as the default, which is consistent with Stata's approach.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}