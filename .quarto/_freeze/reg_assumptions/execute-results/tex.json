{
  "hash": "6014c270588f915b75444d1f3446ea4a",
  "result": {
    "markdown": "---\ntitle: \"MLM Assumptions\"\nauthor: \"Luke Miratrix\"\neditor: \n  markdown: \n    wrap: sentence\n---\n\n\n\n\n\n## Oh, assumptions\n\nThere are generally two kinds of assumptions we should worry about the most: ommitted variable bias, and independence assumptions.\nThe latter of these is one we should always think about.\n\nDo read Chapter 9 of R&B, paying attention to their examples and not so much to the mathematical formalism.\nIt has some dense prose, but then moves to specific diagnostics that make what they are talking about much more clear (and it also provides things you can do to check assumptions).\nThe *MLM in Plain Language* text has some simpler explanations.\nAlso see below for some further notes.\n\n## Ommitted variable bias\n\nConsider the following numerical example:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nN = 100\ndat = data.frame( X1 = rnorm( N ) )\ndat = mutate( dat, \n              X2 = X1 + rnorm( N ),\n              Y = 3 + 0.5 * X1 + 1.5 * X2 + rnorm( N ) )\n```\n:::\n\n\n\nThe above makes an `X2` that is correlated with `X1`, and a `Y` that is a function of both.\nThe true model here is $$ Y_i = \\beta_0 + \\beta_1 X_{1i} + \\beta_2 X_{2i} + \\epsilon_{i} $$ with coefficents $\\beta = (3, 0.5, 1.5)$.\n\nWe fit two models, one with both covariates, and one with only one:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nM0 = lm( Y ~ 1 + X1 + X2 , data = dat )\nM1 = lm( Y ~ 1 + X1, data = dat )\n```\n:::\n\n\n\nOur results:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntab_model(M0, M1, p.style = \"stars\",\n          show.ci = FALSE, show.se = TRUE)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table style=\"border-collapse:collapse; border:none;\">\n<tr>\n<th style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm;  text-align:left; \">&nbsp;</th>\n<th colspan=\"2\" style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; \">Y</th>\n<th colspan=\"2\" style=\"border-top: double; text-align:center; font-style:normal; font-weight:bold; padding:0.2cm; \">Y</th>\n</tr>\n<tr>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  text-align:left; \">Predictors</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">Estimates</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">std. Error</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">Estimates</td>\n<td style=\" text-align:center; border-bottom:1px solid; font-style:italic; font-weight:normal;  \">std. Error</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">(Intercept)</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">3.10 <sup>***</sup></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.11</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">2.95 <sup>***</sup></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.18</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">X1</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.44 <sup>**</sup></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.15</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">1.92 <sup>***</sup></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.16</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; \">X2</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">1.51 <sup>***</sup></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \">0.11</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:center;  \"></td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm; border-top:1px solid;\">Observations</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"2\">100</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left; border-top:1px solid;\" colspan=\"2\">100</td>\n</tr>\n<tr>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; text-align:left; padding-top:0.1cm; padding-bottom:0.1cm;\">R<sup>2</sup> / R<sup>2</sup> adjusted</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"2\">0.856 / 0.853</td>\n<td style=\" padding:0.2cm; text-align:left; vertical-align:top; padding-top:0.1cm; padding-bottom:0.1cm; text-align:left;\" colspan=\"2\">0.584 / 0.580</td>\n</tr>\n<tr>\n<td colspan=\"5\" style=\"font-style:italic; border-top:double black; text-align:right;\">* p&lt;0.05&nbsp;&nbsp;&nbsp;** p&lt;0.01&nbsp;&nbsp;&nbsp;*** p&lt;0.001</td>\n</tr>\n\n</table>\n\n`````\n:::\n:::\n\n\n\nNote our coefficient is completely wrong when we omit a correlated variable.\nThis is omitted variable bias, and in terms of our assumptions we are in a circumstance where the true residuals in our model are not centered around 0 for all values of `X1`, since they include the `X2` effect which is correlated with `X1`.\nWe can see this as follows:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat = mutate( dat, e = Y - 3 - 0.5 * X1 )\nggplot( dat, aes( X1, e ) ) +\n    geom_point() +\n    geom_hline( yintercept = 0 )\n```\n\n::: {.cell-output-display}\n![](reg_assumptions_files/figure-pdf/unnamed-chunk-4-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nNote how our residuals (which includes `X2`) are positive for bigger `X1`, due to the correlation of `X1` and `X2`.\n\n*Conclusion:* On one hand, we have the wrong estimate for $\\beta_1$.\nOn the other, the estimate we do get is fine if we view it as the best description of the data.\nWe just need to remember that the interpretation of our coefficient includes the confounding effect of `X2` on `X1`.\n\nIn this vein:\n\nQ: You mentioned during class that we don't care too much about assumptions when looking at trends in the data.\nHowever, if we are trying to draw causal inferences, do these assumptions become more important?\n\nA: Even with the assumptions causal inference would depend on the model.\nModern causal inference has a hard time with this, so you need other strategies such as quasiexperimental design.\nHence my focus on descriptive aspects of data analysis.\n\n## Independence assumptions\n\nThe independence assumptions are key.\nWhen we do not take violations of independence into account, we can be overly confident of our estimates.\n\nGenerally with MLM we should think of these assumptions in terms of how we sampled our data.\nIf we sampled our data by sampling a collection of schools, and then individuals within those schools, then we have two levels.\nWe then need to ask two questions:\n\n(1) Were the schools sampled independently?\n\n(2) Were the students sampled independently within the schools?\n\nIf yes to both, we have met both our independence assumptions!\nWe have met them even if the students are clustered in classes within their schools.\nAs long as we did not sample using those classes (or other clusters), we are ok as our sample of students will be representative of the school they are in.\n\nOne might then ask, more generally, if there is a problem with clustering if it's not part of the sampling plan?\nE.g. if you sampled at the school level and surveyed all students, there is still natural clustering in classrooms: is that a problem?\nWhat about unobserved clustering like families, neighborhoods, etc. which are not part of sampling, but do exist naturally in populations?\n\nE.g., see this [document](https://blogs.worldbank.org/impactevaluations/when-should-you-cluster-standard-errors-new-wisdom-econometrics-oracle) which says clustered SEs are *not* necessary (in OLS) unless sampling was conducted at the cluster-level and that econometricians often overuse them.\n\nThis is indeed correct.\nThat being said, we might want to make clusters to investigate how things vary across those clusters.\n\nQ: More generally, is there a problem with clustering if it's not part of the sampling plan?\nE.g. if you sampled at the school level and surveyed all students, there is still natural clustering in classrooms: is that a problem?\nWhat about unobserved clustering like families, neighborhoods, etc. which are not part of sampling, but do exist naturally in populations?\nQ: I'll let Luke respond to how this affects the assumptions later on Piazza.\nMy intuition is that we want to cluster/add levels at these different levels if we believe the outcomes or predictors are correlated Q: Thanks.\nI remember reading this https://blogs.worldbank.org/impactevaluations/when-should-you-cluster-standard-errors-new-wisdom-econometrics-oracle a few years ago, which says clustered SEs are *not* necessary (in OLS) unless sampling was conducted at the cluster-level and that econometricians often overuse them, but I am wondering to what degree this holds in MLMs.\n\nA: This is correct.\nThat being said, we might want to make clusters to investigate how things vary across those clusters.\n\n## Number of clusters needed?\n\nOk, this isn't an assumption per se, but onwards!\n\nQ: Why should you worry if the number of group is small?\n(referencing the recap slide)\n\nA: With few clusters, estimation is hard just like having a small dataset with OLS.\nThe variance parameters in particular are difficult.\n\nQ: When you say \"at least 20\" you mean for the number of j's, right?\n\nA: Yes, number of clusters.\nMostly Harmless Econometrics readers might recall a discussion of 42 clusters (8.2.3), which contributes to this debate of the appropriate level of j-units\n\n## Testing assumptions\n\nQ: How do we test these assumptions?\n\nA: Often with plots, like with classic OLS.\nfor example we can pot a histogram of the residuals and see if they are normally distributed.\n",
    "supporting": [
      "reg_assumptions_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}