{
  "hash": "00312d52601e60b36ee19ea6331db256",
  "result": {
    "markdown": "---\ntitle: \"An overview of complex error structures\"\nauthor: \"Luke Miratrix\"\neditor: \n  markdown: \n    wrap: sentence\n---\n\n::: {.cell}\n\n:::\n\n\n\nIn Unit 6, Lecture 3 we talked about how we can model residuals around an overall population model using different specified structures on the correlation matrices for the students.\nThis handout extends those topics, using the Raudenbush and Bryk Chapter 6 example on National Youth Survey data on deviant attitudes.\nWe're going to do a few things:\n\n1.  Reproduce the models in the book, showing you how to get them in R, using the commands `lme` and `gls`.\n\n2.  Discuss the relationship between `lme` and `gls`, and what it actually means when you include a gls-like \"correlation\" argument when calling `lme`.\n    To make a long story short: `gls` is cleaner and more principled from a mathematical point of view, but in practice you will probably prefer hybrid calls using `lme`.\n\n3.  Give two ways this stuff can actually be useful -- heteroskedasticity and AR\\[1\\] -- and show how to fit realistic models with either and both.\n    We'll interpret and check significance of parameters as appropriate.\n\n## National Youth Survey running example\n\nOur running example is the data as described in Raudenbush and Bryk, and we follow the discussion on page 190.\nThese data are the first cohort of the National Youth Survey (NYS).\nThis data comes from a survey in which the same students were asked yearly about their acceptance of 9 \"deviant\" behaviors (such as smoking marijuana, stealing, etc.).\nThe study began in 1976, and followed two cohorts of children, starting at ages 11 and 14 respectively.\nWe will analyze the first 5 years of data.\n\nAt each time point, we have measures of:\n\n-   ATTIT, the attitude towards deviance, with higher numbers implying higher tolerance for deviant behaviors.\n-   EXPO, the \"exposure\", based on asking the children how many friends they had who had engaged in each of the behaviors. Both of these numbers have been transformed to a logarithmic scale to reduce skew.\n\nFor each student, we have:\n\n-   Gender (binary)\n-   Minority status (binary)\n-   Family income, in units of \\$10K.\n\nOne reasonable research question would to describe how the cohort evolved.\nFor this question, the parameters of interest would be the average attitudes at each age.\nStandard deviations and intrasubject correlations are, as is often but not always the case, simply nuisance parameters.\nStill, the better we can do at realistically modeling these nuisance parameters, the more precision we will have for the measures of interest, and the power we will have to test relevant hypotheses.\n\n### Getting the data ready\n\nWe'll focus on the first cohort, from ages 11-15.\nFirst, let's read the data.\n\nNote that this table is in \"wide format\".\nThat is, there is only one row for each student, with all the different observations for that student in different columns of that one row.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnyswide = read.csv(\"data/nyswide.csv\")\nhead(nyswide)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  ID ATTIT.11 EXPO.11 ATTIT.12 EXPO.12 ATTIT.13 EXPO.13 ATTIT.14 EXPO.14\n1  3     0.11   -0.37     0.20   -0.27     0.00   -0.37     0.00   -0.27\n2  8     0.29    0.42     0.29    0.20     0.11    0.42     0.51    0.20\n3  9     0.80    0.47     0.58    0.52     0.64    0.20     0.75    0.47\n4 15     0.44    0.07     0.44    0.32     0.89    0.47     0.75    0.26\n5 33     0.20   -0.27     0.64   -0.27     0.69   -0.27       NA      NA\n6 45     0.11    0.26     0.37   -0.17     0.37    0.14     0.37    0.14\n  ATTIT.15 EXPO.15 FEMALE MINORITY INCOME\n1     0.11   -0.17      1        0      3\n2     0.69    0.20      0        0      4\n3     0.98    0.47      0        0      3\n4     0.80    0.47      0        0      4\n5     0.11    0.07      1        0      4\n6     0.69    0.32      1        0      4\n```\n:::\n:::\n\n\n\nFor our purposes, we want it in \"long format\".\nThe `reshape()` command does this for us (reshape allows us to reshape two variables at once, as compared to `gather()` from tidyverse:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnys1.na = reshape(nyswide, direction=\"long\", #we want it in long format\n              varying=list(ATTIT=paste(\"ATTIT\",11:15,sep=\".\"), \n                          EXPO=paste(\"EXPO\",11:15,sep=\".\") ),\n              v.names=c(\"ATTIT\",\"EXPO\"), idvar=\"ID\", timevar=\"AGE\", times=11:15)\n\n## Drop missing ATTIT values\nnys1 = nys1.na[!is.na(nys1.na$ATTIT),] \n\nhead( nys1 )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      ID FEMALE MINORITY INCOME AGE ATTIT  EXPO\n3.11   3      1        0      3  11  0.11 -0.37\n8.11   8      0        0      4  11  0.29  0.42\n9.11   9      0        0      3  11  0.80  0.47\n15.11 15      0        0      4  11  0.44  0.07\n33.11 33      1        0      4  11  0.20 -0.27\n45.11 45      1        0      4  11  0.11  0.26\n```\n:::\n:::\n\n\n\nNote, the `paste` command makes the sequence `c(\"ATTIT.12\", \"ATTIT.13\", ...)` to autogenerate our variable names to reshape:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npaste(\"ATTIT\",11:15,sep=\".\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"ATTIT.11\" \"ATTIT.12\" \"ATTIT.13\" \"ATTIT.14\" \"ATTIT.15\"\n```\n:::\n:::\n\n\n\nWe also need to make our age a factor so it is treated appropriately as an indicator of what wave the data was collected in.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnys1$agefac = as.factor(nys1$AGE) \n```\n:::\n\n\n\nJust to get a sense of the data, let's plot each age as a boxplot\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot( nys1, aes( agefac, ATTIT ) ) +\n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](complex_error_files/figure-pdf/unnamed-chunk-5-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nNote some features of the data: First, we see that ATTIT goes up over time.\nSecond, we see the variation of points also goes up over time.\nThis is heteroskedasticity.\n\nIf we plot individual lines we have\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnys1$AGEjit = jitter(nys1$AGE)\nnys1$ATTITjit = jitter(nys1$ATTIT, amount=0.05)\nggplot( filter( nys1, complete.cases(nys1) ), aes( AGEjit, ATTITjit, group=ID ) ) +\n  geom_line( alpha=0.2 )\n```\n\n::: {.cell-output-display}\n![](complex_error_files/figure-pdf/unnamed-chunk-6-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\nNote how we have correlation of residuals, in that some students are systematically low and some are systematically higher (although there is a lot of bouncing around).\n\n## Representation of error structure\n\nIn our data, we have 5 observations $y_{it}$ for each subject i at 5 fixed times $t=1$ through $t=5$.\nWithin each person $i$ (where person is our Level-2 group, and time is our Level-1),\n\n\n\n$$\\begin{pmatrix}y_{i1}\\\\\ny_{i2}\\\\\ny_{i3}\\\\\ny_{i4}\\\\\ny_{i5}\n\\end{pmatrix} \\sim N\\left[\\left(\\begin{array}{c}\n\\mu_1\\\\\n\\mu_2\\\\\n\\mu_3\\\\\n\\mu_4\\\\\n\\mu_5\n\\end{array}\\right),\\left(\\begin{array}{ccccc}\n\\tau_{11} & \\tau_{12} & \\tau_{13} & \\tau_{14} & \\tau_{15}\\\\\n. & \\tau_{22} & \\tau_{23}& \\tau_{24} & \\tau_{25}\\\\\n. & . & \\tau_{33}& \\tau_{34} & \\tau_{35}\\\\\n. & . & . & \\tau_{44} & \\tau_{45}\\\\\n. & . & . & . & \\tau_{52}\n\\end{array}\\right)\\right] = N[ \\mu, \\Sigma ]$$\n\nNote that the key parts here are the correlations between the residuals at different times.\nWe call our entire covariance matrix $\\Sigma$.\nThis matrix describes how the residuals within a single individual (with 5 time points of observation) are correlated.\nThe mean vector can easily be separated out:\n\n$$\\begin{pmatrix}y_{i1}\\\\\n\n\ny_{i2}\\\\\ny_{i3}\\\\\ny_{i4}\\\\\ny_{i5}\n\\end{pmatrix} = \\left(\\begin{array}{c}\n\\mu_{1i}\\\\\n\\mu_{2i}\\\\\n\\mu_{3i}\\\\\n\\mu_{4i}\\\\\n\\mu_{5i}\n\\end{array}\\right) + \\left(\\begin{array}{c}\n\\epsilon_{1i}\\\\\n\\epsilon_{2i}\\\\\n\\epsilon_{3i}\\\\\n\\epsilon_{4i}\\\\\n\\epsilon_{5i}\n\\end{array}\\right)$$ $$\\left(\\begin{array}{c}\n\\epsilon_{1i}\\\\\n\\epsilon_{2i}\\\\\n\\epsilon_{3i}\\\\\n\\epsilon_{4i}\\\\\n\\epsilon_{5i}\n\\end{array}\\right)\\sim N\\left[\\left(\\begin{array}{c}\n0\\\\\n0\\\\\n0\\\\\n0\\\\\n0\n\\end{array}\\right),\\left(\\begin{array}{ccccc}\n\\tau_{11} & \\tau_{12} & \\tau_{13} & \\tau_{14} & \\tau_{15}\\\\\n. & \\tau_{22} & \\tau_{23}& \\tau_{24} & \\tau_{25}\\\\\n. & . & \\tau_{33}& \\tau_{34} & \\tau_{35}\\\\\n. & . & . & \\tau_{44} & \\tau_{45}\\\\\n. & . & . & . & \\tau_{55}\n\\end{array}\\right)\\right]$$\n\nOur regression model would give us the mean vector for any given student (e.g., it would be $X'\\beta$ for some covariate matrix (design matrix) $X$ and fixed effect parameter vector $\\beta$.\n$X$ would have one row per time point and time would be one of the columns, to give our predictions for our 5 time points.\n\nOur error structure model gives us the distribution of the $(\\epsilon_{1i}, \\ldots, \\epsilon_{5i})$ for that student.\nDifferent ideas about the data generating process lead to different correlation structures here.\nWe saw a couple of those in class.\n\n## Reproducing Chapter 6 examples\n\nThe above provides a framework for thinking about grouped data: each group is a small world with a linear prediction line and a collection of residuals around that line.\nUnder this view, we specify a specific structure on how the residuals relate to each other.\n(E.g., for classic OLS we would have i.i.d. normally distributed residuals, represented as our $\\Sigma$ being a diagonal matrix with $\\sigma^2$ along the diagonal and 0s everywhere else).\nIn R, once we determine what structure we want, we can fit models based on parameterized correlation matrices using the \"lme\" command from the \"nlme\" package (You may need to first call `install.packages(\"nlme\")` to get this package).\nLet's load the package now:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(nlme)\n```\n:::\n\n\n\nRecall that all of these models include just the single fixed effect (besides intercept) of a linear term on age.\n\n### Compound symmetry (random intercept model)\n\nA \"compound symmetry\" residual covariance structure (all diagonal elements equal, all off-diagonal elements equal) is actually equivalent to a random intercepts model.\nThus, there are 2 ways to get this same model:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelRE = lme(ATTIT ~ AGE, \n              data=nys1,\n              random=~1|ID )\n```\n:::\n\n\n\nand\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelCompSymm = gls(ATTIT ~ AGE,\n                    data=nys1,\n                    correlation=corCompSymm(form=~AGE|ID) )\n```\n:::\n\n\n\nFor reference, using the `lme4` package we again have (we use `lme4::` in front of `lmer` to avoid loading the `lme4` package fully):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelRE.lme4 = lme4::lmer(ATTIT ~ AGE + (1|ID), data=nys1 )\n```\n:::\n\n\n\nWe can get the correlation matrix for individuals #3:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmyVarCovs = getVarCov(modelRE,type=\"marginal\", individual=3)\nmyVarCovs\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nID 9 \nMarginal variance covariance matrix\n         1        2        3        4        5\n1 0.066450 0.034113 0.034113 0.034113 0.034113\n2 0.034113 0.066450 0.034113 0.034113 0.034113\n3 0.034113 0.034113 0.066450 0.034113 0.034113\n4 0.034113 0.034113 0.034113 0.066450 0.034113\n5 0.034113 0.034113 0.034113 0.034113 0.066450\n  Standard Deviations: 0.25778 0.25778 0.25778 0.25778 0.25778 \n```\n:::\n:::\n\n\n\nIf we look at an individual #5, who only has 4 timepoints we get a $4 \\times 4$ matrix:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngetVarCov(modelRE,type=\"marginal\", individual=5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nID 33 \nMarginal variance covariance matrix\n         1        2        3        4\n1 0.066450 0.034113 0.034113 0.034113\n2 0.034113 0.066450 0.034113 0.034113\n3 0.034113 0.034113 0.066450 0.034113\n4 0.034113 0.034113 0.034113 0.066450\n  Standard Deviations: 0.25778 0.25778 0.25778 0.25778 \n```\n:::\n:::\n\n\n\n(Other individuals are the same, if they have the same number of time points, given our model.)\n\n#### Comparing the models\n\nThese are two very different ways of specifying the same thing, and the parameter estimates we get out are also not the same.\nCompare the two summary printouts:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(modelRE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed-effects model fit by REML\n  Data: nys1 \n        AIC       BIC   logLik\n  -204.9696 -185.0418 106.4848\n\nRandom effects:\n Formula: ~1 | ID\n        (Intercept)  Residual\nStdDev:   0.1846979 0.1798237\n\nFixed effects:  ATTIT ~ AGE \n                 Value  Std.Error  DF   t-value p-value\n(Intercept) -0.5099954 0.05358498 839 -9.517505       0\nAGE          0.0644387 0.00398784 839 16.158810       0\n Correlation: \n    (Intr)\nAGE -0.969\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-2.90522949 -0.64353962 -0.01388485  0.60377631  3.26938845 \n\nNumber of Observations: 1079\nNumber of Groups: 239 \n```\n:::\n:::\n\n\n\nand\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(modelCompSymm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGeneralized least squares fit by REML\n  Model: ATTIT ~ AGE \n  Data: nys1 \n        AIC       BIC   logLik\n  -204.9696 -185.0418 106.4848\n\nCorrelation Structure: Compound symmetry\n Formula: ~AGE | ID \n Parameter estimate(s):\n      Rho \n0.5133692 \n\nCoefficients:\n                 Value  Std.Error   t-value p-value\n(Intercept) -0.5099954 0.05358498 -9.517505       0\nAGE          0.0644387 0.00398784 16.158810       0\n\n Correlation: \n    (Intr)\nAGE -0.969\n\nStandardized residuals:\n        Min          Q1         Med          Q3         Max \n-1.77123071 -0.77132300 -0.06434029  0.71151900  3.38387884 \n\nResidual standard error: 0.2577787 \nDegrees of freedom: 1079 total; 1077 residual\n```\n:::\n:::\n\n\n\nThese do not look very similar, do they?\nBut wait:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogLik(modelCompSymm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'log Lik.' 106.4848 (df=4)\n```\n:::\n\n```{.r .cell-code}\nlogLik(modelRE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'log Lik.' 106.4848 (df=4)\n```\n:::\n\n```{.r .cell-code}\nlogLik(modelRE.lme4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'log Lik.' 106.4848 (df=4)\n```\n:::\n\n```{.r .cell-code}\nAIC( modelCompSymm )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -204.9696\n```\n:::\n\n```{.r .cell-code}\nAIC( modelRE )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -204.9696\n```\n:::\n\n```{.r .cell-code}\nAIC( modelRE.lme4 )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -204.9696\n```\n:::\n:::\n\n\n\nIn fact, they have the same AIC, etc., because they are equivalent models.\n\nThe lesson is that it's actually quite hard to see the correspondence between a familiar random-effects model and an equivalent model expressed in terms of a covariance matrix.\nSure, we could do a bunch of math and see that in the end they are the same; but that math is already daunting here, and this is the simplest possible situation.\nThe fitted parameters of a covariance-based model are just really hard to interpret in familiar terms.\n\n### Autoregressive error structure (AR\\[1\\])\n\nOne typical structure used for longitudinal data is the \"autoregressive\" structure.\nThe idea is threefold:\n\n1.  $Var(u_{it}) = \\sigma^2$ - that is, overall marginal variance is staying constant.\n2.  $Cor(u_{it},u_{i(t-1)}) = \\rho$ - that is, residuals are a little bit \"sticky\" over time so residuals from nearby time points tend to be similar.\n3.  $E(u_{it}|u_{i(t-1)},u_{i(t-2)}) = E(u_{it}|u_{i(t-1)})$ - that is, the only way the two-periods-ago measurement tells you anything about the current one is through the intermediate one, with no longer-term effects or \"momentum\".\n\nIn this case, the unconditional two-step correlation $Cor(u_{it},u_{i(t-2)})$ is also easy to calculate.\nIntuitively, we can say that a portion $\\rho$ of the residual \"is the same\" after each step, so that after two steps the portion that \"is the same\" is $\\rho$ of $\\rho$, or $\\rho^2$.\nClearly, then, after three steps the correlation will be $\\rho^3$, and so on.\nIn other words, the part that \"is the same\" is decaying in an exponential pattern.\nIndeed, one could show that (3.), above, requires the correlated part to decay in a memoryless pattern, leaving the Exponential and Hypergeometric distributions (which both show exponential decay) among the few options.\n\nThus, the within-subject correlation structure implied by these postulates is:\n\n\n\n$$\\left(\\begin{array}{c}\nu_{1i}\\\\\nu_{2i}\\\\\nu_{3i}\\\\\nu_{4i}\\\\\n...\\\\\nu_{ni}\n\\end{array}\\right)\\sim N\\left[\\left(\\begin{array}{c}\n0\\\\\n0\\\\\n0\\\\\n0\\\\\n...\\\\\n0\n\\end{array}\\right),\\sigma^2\\left(\\begin{array}{cccccc}\n1 & \\rho  & \\rho^2 & \\rho^3 & ... & \\rho^{n-1}\\\\\n. & 1 & \\rho & \\rho^2 & ... & \\rho^{n-2}\\\\\n. & . & 1& \\rho  & ... & \\rho^{n-3}\\\\\n. & . & . & 1 & ... & \\rho^{n-4} \\\\\n... & ... & ... & ... & ... & ... \\\\\n. & . & . & . & ... & 1\n\\end{array}\\right)\\right]\\\\$$\n\nAs you can see, this structure takes advantage of the temporal nature of the data sequence to parameterize the covariance matrix with only two underlying parameters: $\\sigma$ and $\\rho$.\nBy contrast, a random intercept model needs the overall $\\sigma$ and variance of intercepts $\\tau$---also two parameters!\nSame complexity, different structure.\n\n#### Fitting the AR\\[1\\] covariance structure\n\nTo get a true AR\\[1\\] residual covariance structure, we need to leave the world of hierarchical models, and thus use the command `gls`.\nThis is just what we've discussed in class.\nHowever, later on in this document, we'll see how to add AR\\[1\\] structure on top of a hierarchical model, which is messier from a theoretical point of view, but often more useful and interpretable in practice.\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelAR1 = gls(ATTIT ~ AGE, \n                    data=nys1,\n                    correlation=corAR1(form=~AGE|ID) )\n\nsummary(modelAR1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGeneralized least squares fit by REML\n  Model: ATTIT ~ AGE \n  Data: nys1 \n        AIC       BIC   logLik\n  -250.4103 -230.4826 129.2051\n\nCorrelation Structure: ARMA(1,0)\n Formula: ~AGE | ID \n Parameter estimate(s):\n     Phi1 \n0.6159857 \n\nCoefficients:\n                 Value  Std.Error   t-value p-value\n(Intercept) -0.4534647 0.07515703 -6.033564       0\nAGE          0.0601205 0.00569797 10.551218       0\n\n Correlation: \n    (Intr)\nAGE -0.987\n\nStandardized residuals:\n        Min          Q1         Med          Q3         Max \n-1.75013168 -0.81139621 -0.03256558  0.74814629  3.40350724 \n\nResidual standard error: 0.2561765 \nDegrees of freedom: 1079 total; 1077 residual\n```\n:::\n:::\n\nYou have to dig around in the large amount of output to find the parameter estimates, but they are there.\n`Phi1` is the auto-correlation parameter.\nAnd the covariance of residuals:\n\n::: {.cell}\n\n```{.r .cell-code}\ngetVarCov(modelAR1,type=\"marginal\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMarginal variance covariance matrix\n          [,1]     [,2]     [,3]     [,4]      [,5]\n[1,] 0.0656260 0.040425 0.024901 0.015339 0.0094485\n[2,] 0.0404250 0.065626 0.040425 0.024901 0.0153390\n[3,] 0.0249010 0.040425 0.065626 0.040425 0.0249010\n[4,] 0.0153390 0.024901 0.040425 0.065626 0.0404250\n[5,] 0.0094485 0.015339 0.024901 0.040425 0.0656260\n  Standard Deviations: 0.25618 0.25618 0.25618 0.25618 0.25618 \n```\n:::\n\n```{.r .cell-code}\nsummary(modelAR1)$AIC\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -250.4103\n```\n:::\n:::\n\nNote that the AIC of our AR\\[1\\] model is lower by about 45 than the random intercept model; clearly far superior because it is getting nearby residuals being more correlated, while the random intercept model does not do this.\nAlso see the banding structure of the residual correlation matrix.\n\n### Random slopes\n\nIn theory, a random slopes model could be done with `gls` as well as with `lme`; in practice, it's much more practical just to do it as a hierarchical model with `lme`:\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelRS = lme(ATTIT ~ 1 + AGE, \n              data=nys1,\n              random=~AGE|ID )\n```\n:::\n\nWe have separated our fixed and random components with `lme()`.\nWe first include a formula with only fixed effects, and then give a right-side-only formula with terms similar to what you'd put in parentheses with `lmer()` for the random effects.\n\nOur results:\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(modelRS)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed-effects model fit by REML\n  Data: nys1 \n       AIC       BIC   logLik\n  -310.125 -280.2334 161.0625\n\nRandom effects:\n Formula: ~AGE | ID\n Structure: General positive-definite, Log-Cholesky parametrization\n            StdDev     Corr  \n(Intercept) 0.51024132 (Intr)\nAGE         0.05038614 -0.98 \nResidual    0.16265429       \n\nFixed effects:  ATTIT ~ 1 + AGE \n                 Value  Std.Error  DF  t-value p-value\n(Intercept) -0.5133250 0.05834087 839 -8.79872       0\nAGE          0.0646849 0.00492904 839 13.12323       0\n Correlation: \n    (Intr)\nAGE -0.981\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-2.87852414 -0.55971196 -0.07521191  0.57495075  3.45648134 \n\nNumber of Observations: 1079\nNumber of Groups: 239 \n```\n:::\n\n```{.r .cell-code}\ngetVarCov(modelRS,type=\"marginal\", individual=3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nID 9 \nMarginal variance covariance matrix\n         1        2        3        4        5\n1 0.039649 0.015922 0.018650 0.021379 0.024108\n2 0.015922 0.047646 0.026457 0.031725 0.036992\n3 0.018650 0.026457 0.060720 0.042070 0.049876\n4 0.021379 0.031725 0.042070 0.078872 0.062760\n5 0.024108 0.036992 0.049876 0.062760 0.102100\n  Standard Deviations: 0.19912 0.21828 0.24641 0.28084 0.31953 \n```\n:::\n\n```{.r .cell-code}\nsummary(modelRS)$AIC\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -310.125\n```\n:::\n:::\n\nThe first thing to note is the residual covariance matrix comes from the structure of the random intercept and random slope.\nIf you squint hard enough at it, you can begin to see the linear structures in its diagonal and off-diagonal elements.\nIf you graphed it, those structures would jump out more clearly.\nBut in practice, it's much easier to think of things in terms of the hierarchical model, not in terms of linear structures in a covariance matrix.\n\nNote also that the AIC has dropped by another 60 points or so; we're continuing to improve the model.\n\n### Random slopes with heteroskedasticity\n\nRelaxing the homoskedasticity assumption in the random slopes model leaves us a bit in between worlds.\nWe're not fully into the world of GLS, because there are still random effects; but we're not fully in the world of hierarchical models because there is structure in the residuals within groups.\nWe'll talk more about this compromise below; for now, let's just do it.\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelRSH = lme(ATTIT ~ AGE, \n              data=nys1,\n              random=~AGE|ID,\n              weights=varIdent(form=~1|agefac) )\n```\n:::\n\nThe key line is the `varIdent` line: we are saying each age factor level gets its own weight (rescaling) of the residuals---this is heteroskedasticity.\nIn particular, the above says our residual variance will be weighted by a weight for each age factor, so each age level effectively gets its own variance.\nThis is where these models start to get a bit exciting---we have random slopes, and then heteroskedastic residuals (homoskedastic for any given age level), all together.\nOur fit model:\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(modelRSH)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed-effects model fit by REML\n  Data: nys1 \n        AIC       BIC   logLik\n  -312.5801 -262.7608 166.2901\n\nRandom effects:\n Formula: ~AGE | ID\n Structure: General positive-definite, Log-Cholesky parametrization\n            StdDev     Corr  \n(Intercept) 0.57693602 (Intr)\nAGE         0.05431367 -0.979\nResidual    0.14054184       \n\nVariance function:\n Structure: Different standard deviations per stratum\n Formula: ~1 | agefac \n Parameter estimates:\n       11        12        13        14        15 \n1.0000000 1.1956071 1.3095864 1.1255177 0.9802311 \nFixed effects:  ATTIT ~ AGE \n                 Value  Std.Error  DF   t-value p-value\n(Intercept) -0.4929012 0.05715889 839 -8.623351       0\nAGE          0.0631404 0.00483385 839 13.062122       0\n Correlation: \n    (Intr)\nAGE -0.981\n\nStandardized Within-Group Residuals:\n       Min         Q1        Med         Q3        Max \n-2.9163540 -0.5498217 -0.0758348  0.5482942  3.2370312 \n\nNumber of Observations: 1079\nNumber of Groups: 239 \n```\n:::\n:::\n\nNote how we have 5 parameter estimates for the residuals, listed under `agefac`.\nIt appears as if we have more variation in age 13 than other ages.\nAge 11, the baseline, is 1.0; it is our reference scaling.\nThese numbers are all scaling the overall residual variance parameter $\\sigma^2$ of $0.1405^2$.\n\nFor looking at the covariance structure of the residuals, at this point we have to warn you: there appears to be a bug in `getVarCov` which rears its head when you use the `weights` argument to either lme or gls.\nIt has to do with the order of the rows of the data set, something which obviously should not matter; and it means that you get simply wrong numbers for marginal variances, though correlations should still be correct.\nJameson Quinn wrote a fix for this function, which is in the source file `getVarCov2.R`.\nThis fix is used below, using the `source` command to load the fixed version of the function.\nBut beware: while we've tested that this fix gives the right answers for this data set, and that it gives the right answers when the `weights` argument is not used (that is, when the old version was already right), we have not done the extensive checking it would take to say we trust it in all cases.\nThis bug was reported back in 2016, and hopefully it will be fixed in later versions of R; but for now, tread with care, and double-check that the numbers you're getting make sense.\n\n::: {.cell}\n\n```{.r .cell-code}\nsource('scripts/getVarCov2.R')\nmyVarCov = getVarCovFixedLme(modelRSH,type=\"marginal\", individual=3)\nmyVarCov\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nID 9 \nMarginal variance covariance matrix\n         1        2        3        4        5\n1 0.034915 0.016947 0.018731 0.020516 0.022300\n2 0.016947 0.049916 0.026415 0.031150 0.035884\n3 0.018731 0.026415 0.067975 0.041784 0.049468\n4 0.020516 0.031150 0.041784 0.077440 0.063052\n5 0.022300 0.035884 0.049468 0.063052 0.095615\n  Standard Deviations: 0.18685 0.22342 0.26072 0.27828 0.30922 \n```\n:::\n:::\n\nWe get lists of matrices back from our call.\nWe can convert any one to a correlation matrix:\n\n::: {.cell}\n\n```{.r .cell-code}\ncov2cor(myVarCov[[1]])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          1         2         3         4         5\n1 1.0000000 0.4059446 0.3844935 0.3945453 0.3859525\n2 0.4059446 1.0000000 0.4534860 0.5010159 0.5194173\n3 0.3844935 0.4534860 1.0000000 0.5759089 0.6136046\n4 0.3945453 0.5010159 0.5759089 1.0000000 0.7327497\n5 0.3859525 0.5194173 0.6136046 0.7327497 1.0000000\n```\n:::\n:::\n\nAnd our AIC:\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(modelRSH)$AIC\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -312.5801\n```\n:::\n:::\n\nNo amount of squinting will show the structure in that covariance matrix.\nBut when you convert to a correlation matrix, you can again squint and begin to see the linear structures in its diagonal and off-diagonal elements.\nThe same comment as above still applies: in practice, it's much easier to think of things in terms of the hierarchical model, and only read the diagonals of the covariance matrix.\n\nNote that the AIC has dropped by only another 2.5 points or so; that corresponds to the idea that if one of these two models were exactly true, the odds are about $e^{2.5/2}\\cong 3.5$ in favor of the more complex model.\nAside from the fact that that premise is silly -- we are pretty sure that neither of these models is the exact truth; and in that case, something like BIC would probably be better than AIC -- those odds are also pretty weak; the simpler model is probably better here.\n\nHere's the reported BICs, by the way: -280.2334145 for the homoskedastic one, and -262.7607678 for the heteroskedastic.\nAs we expected, the simpler model wins that fight.\n(Though what $N$ to use for BIC is sometimes not obvious with hierarchical models, so you can't trust those numbers too much. We will discuss AIC and BIC more later in the course.)\n\n### Fully unrestricted model\n\nOK, let's go whole hog, and fit the unrestricted model.\nAgain, this means leaving the world of hierarchical models and using gls.\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelUnrestricted = gls(ATTIT ~ AGE, \n               data=nys1,\n               correlation=corSymm(form=~1|ID),\n               weights=varIdent(form=~1|agefac) )\n\n\nsummary(modelUnrestricted)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGeneralized least squares fit by REML\n  Model: ATTIT ~ AGE \n  Data: nys1 \n       AIC       BIC  logLik\n  -319.262 -234.5691 176.631\n\nCorrelation Structure: General\n Formula: ~1 | ID \n Parameter estimate(s):\n Correlation: \n  1     2     3     4    \n2 0.458                  \n3 0.372 0.511            \n4 0.441 0.437 0.663      \n5 0.468 0.443 0.597 0.764\nVariance function:\n Structure: Different standard deviations per stratum\n Formula: ~1 | agefac \n Parameter estimates:\n      11       12       13       14       15 \n1.000000 1.118479 1.414269 1.522510 1.560074 \n\nCoefficients:\n                 Value  Std.Error   t-value p-value\n(Intercept) -0.4557090 0.05465564 -8.337822       0\nAGE          0.0597274 0.00458344 13.031145       0\n\n Correlation: \n    (Intr)\nAGE -0.979\n\nStandardized residuals:\n         Min           Q1          Med           Q3          Max \n-1.482606297 -0.809004080 -0.006791942  0.840804584  4.082258054 \n\nResidual standard error: 0.1903187 \nDegrees of freedom: 1079 total; 1077 residual\n```\n:::\n:::\n\nAnd our residual structure:\n\n::: {.cell}\n\n```{.r .cell-code}\nsource('scripts/getVarCov2.R')\nmyvc = getVarCovFixedGls(modelUnrestricted,type=\"marginal\", individual=3)\nmyvc\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMarginal variance covariance matrix\n         [,1]     [,2]     [,3]     [,4]     [,5]\n[1,] 0.036221 0.018541 0.019071 0.024335 0.026466\n[2,] 0.018541 0.045313 0.029251 0.026924 0.027989\n[3,] 0.019071 0.029251 0.072448 0.051703 0.047736\n[4,] 0.024335 0.026924 0.051703 0.083962 0.065764\n[5,] 0.026466 0.027989 0.047736 0.065764 0.088156\n  Standard Deviations: 0.19032 0.21287 0.26916 0.28976 0.29691 \n```\n:::\n:::\n\nAnd AIC:\n\n::: {.cell}\n\n```{.r .cell-code}\nAIC( modelUnrestricted )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -319.262\n```\n:::\n:::\n\nThis unrestricted covariance and correlation matrices have the same structures discussed in the book and in class.\nThe AIC has improved by another 6 or 7 points; that's marginally \"significant\", but in practice probably not substantial enough to make up for the loss of interpretability.\nThe lesson we should take from that is that there's not a whole lot of room for improvement just by tinkering with the residual covariance structure; if we want a much better model, we would have to add new fixed or random effects; perhaps other covariates or perhaps a quadratic term in time.\n\n## Mixing AR\\[1\\] and Random Slopes\n\nLet's look at an AR1 residual structure along with some covariates in our main model.\nThe following has AR\\[1\\] and *also* a random intercept and slope:\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel1 = lme(fixed=ATTIT ~ EXPO + FEMALE + MINORITY + log(INCOME + 1), \n              data=nys1,\n              random=~AGE|ID,\n              correlation=corAR1()  )\n```\n:::\n\nCompare to same model without AR1 correlation\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel1simple = lme(fixed=ATTIT ~ EXPO + FEMALE + MINORITY + log(INCOME + 1), \n             data=nys1,\n             random=~AGE|ID )\nscreenreg( list( model1, model1simple ) )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n=========================================\n                 Model 1      Model 2    \n-----------------------------------------\n(Intercept)         0.33 ***     0.32 ***\n                   (0.04)       (0.04)   \nEXPO                0.36 ***     0.37 ***\n                   (0.03)       (0.03)   \nFEMALE             -0.01        -0.01    \n                   (0.02)       (0.02)   \nMINORITY           -0.06 *      -0.06 *  \n                   (0.03)       (0.03)   \nlog(INCOME + 1)    -0.01        -0.01    \n                   (0.02)       (0.02)   \n-----------------------------------------\nAIC              -359.46      -350.54    \nBIC              -309.67      -305.73    \nLog Likelihood    189.73       184.27    \nNum. obs.        1079         1079       \nNum. groups: ID   239          239       \n=========================================\n*** p < 0.001; ** p < 0.01; * p < 0.05\n```\n:::\n:::\n\nThe AR1 model has notably lower AIC and thus is better.\n(A difference of \\~9 in AIC, which can be interpreted as a factor of $e^{4.5}$ in odds.) Here are the log likelihoods with degrees of freedom:\n\n::: {.cell}\n\n```{.r .cell-code}\nlogLik( model1 )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'log Lik.' 189.7308 (df=10)\n```\n:::\n\n```{.r .cell-code}\nlogLik( model1simple )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n'log Lik.' 184.269 (df=9)\n```\n:::\n:::\n\nOur model is actually kind of mixed up, conceptually.\nWe allowed a random slope on age, and also an autoregressive component by age.\nThus, we effectively allowed the covariance matrix to vary in two different ways, at two different levels of our modeling.\n\nIn fact, as we've seen in class, any random effects, whether they be on slope or intercept, are actually equivalent to certain ways of varying the \\[variance-\\]covariance matrix of the residuals within each group.\nFor instance, random intercepts are equivalent to compound symmetry.\nThus, by including both random intercepts and AR1 correlation in the above model, we've effectively fit a model that allows any covariance matrix that can be expressed as a sum of a random slope covariance matrix (with 2 parameters plus a scaling factor) and an AR1 covariance matrix (with 1 parameter plus a scaling factor).\nThat makes 5 degrees of freedom total for our covariance matrix.\n\nBut conceptually this model is nice: people have linear growth trends, but vary around those growth trends in an autoregressive way.\n\n## The Kitchen sink: building complex models\n\nWhich brings us to the next point: how do you actually use this stuff in practice.\nIdeally, you'd like both the interpretability (and robustness against MAR missingness) of hierarchical models, along with the ability to add additional residual structure such as AR\\[1\\] and/or heteroskedastic residuals.\nThe good news is, you can get both.\nThe bad news is, there's a bit of a potential for bias due to overfitting.\n\nFor instance, imagine you use both random effects and AR\\[1\\].\nSay that for a given subject you have 5 time points, and all of them are above the values you would have predicted based on fixed effects alone.\nThat might be explained by an above-average random effect, or by a set of correlated residuals that all came in high.\nWhichever one of these is the \"true\" explanation, the MLE will tend to parcel it out between the two.\nThis can lead to downward bias in variance and/or correlation parameter estimates, especially with small numbers of observations per subject.\n\nStill, as long as your focus is on location parameters such as true means or slopes, this can be a good way to proceed.\nLet's explore this by first fitting a \"kitchen sink\" model for this data, in which we use all available covariates; and seeing how adding heteroskedasticity, AR\\[1\\] structure, or both changes it (or doesn't).\n\nWhat do we want in this \"kitchen sink\" model?\nLet's first fit a very simple random intercept model with fixed effects for gender, minority status, \"exposure\", and log(income), to see which of these covariates to focus on.\nWe use the `lmerTest` package to get some early $p$-values for these fixed effects.\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelKS0 = lmerTest::lmer(ATTIT ~ FEMALE + MINORITY + log(INCOME + 1) + EXPO + (1|ID), data=nys1)\nsummary(modelKS0, correlation=FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ATTIT ~ FEMALE + MINORITY + log(INCOME + 1) + EXPO + (1 | ID)\n   Data: nys1\n\nREML criterion at convergence: -265.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.1840 -0.5922 -0.0797  0.6043  2.6319 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n ID       (Intercept) 0.01756  0.1325  \n Residual             0.03444  0.1856  \nNumber of obs: 1079, groups:  ID, 239\n\nFixed effects:\n                  Estimate Std. Error         df t value Pr(>|t|)    \n(Intercept)      3.480e-01  4.195e-02  2.301e+02   8.297    9e-15 ***\nFEMALE          -1.835e-02  2.094e-02  2.327e+02  -0.876   0.3819    \nMINORITY        -5.698e-02  2.789e-02  2.279e+02  -2.043   0.0422 *  \nlog(INCOME + 1)  2.102e-03  2.449e-02  2.272e+02   0.086   0.9317    \nEXPO             4.516e-01  2.492e-02  1.041e+03  18.122   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n(The `correlation=FALSE` shortens the printout.)\n\nApparently, MINORITY and EXPO are the covariates with significant effects; minority status is correlated with a lower tolerance for deviance, while \"deviant\" friends are of course correlated positively with tolerance of deviance.\nLet's build a few hierarchical models including these in various specifications (can you identify what models are what? Some of these models are not necessarily good choices).\nWe first center our age so we have meaningful intercepts.\n\n::: {.cell}\n\n```{.r .cell-code}\nnys1$age13 = nys1$AGE - 13\n\nmodelKS1 = lme(ATTIT ~ MINORITY,\n              data=nys1,\n              random=~age13 + EXPO|ID )\n\nmodelKS2 = lme(ATTIT ~ MINORITY + age13, \n              data=nys1,\n              random=~age13 + EXPO|ID )\n\nmodelKS3 = lme(ATTIT ~ MINORITY + age13, \n              data=nys1,\n              random=~EXPO|ID )\n\nmodelKS4 = lme(ATTIT ~ MINORITY + age13 + EXPO, \n              data=nys1,\n              random=~1|ID )\n```\n:::\n\nAnd now we examine them:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary( texreg )\nscreenreg( list( modelKS1, modelKS2, modelKS3, modelKS4 ))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n===================================================================\n                 Model 1      Model 2      Model 3      Model 4    \n-------------------------------------------------------------------\n(Intercept)         0.24 ***     0.31 ***     0.29 ***     0.34 ***\n                   (0.01)       (0.01)       (0.01)       (0.01)   \nMINORITY           -0.05        -0.05 *      -0.04        -0.06 *  \n                   (0.02)       (0.02)       (0.03)       (0.03)   \nage13                            0.06 ***     0.05 ***     0.05 ***\n                                (0.00)       (0.00)       (0.00)   \nEXPO                                                       0.37 ***\n                                                          (0.02)   \n-------------------------------------------------------------------\nAIC              -305.22      -374.18      -312.13      -394.06    \nBIC              -260.38      -324.37      -277.27      -364.18    \nLog Likelihood    161.61       197.09       163.07       203.03    \nNum. obs.        1079         1079         1079         1079       \nNum. groups: ID   239          239          239          239       \n===================================================================\n*** p < 0.001; ** p < 0.01; * p < 0.05\n```\n:::\n:::\n\nOK, Number 4 seems like a pretty good model.\nLet's see how much it improves when we add AR\\[1\\]:\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelKS5 = lme(ATTIT ~ MINORITY + age13 + EXPO, \n              data=nys1,\n              random=~1|ID,\n              correlation=corAR1(form=~AGE|ID) )\nAIC( modelKS5 )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -433.5943\n```\n:::\n\n```{.r .cell-code}\nfixef( modelKS4 )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)    MINORITY       age13        EXPO \n 0.34054593 -0.05606947  0.04830698  0.36778951 \n```\n:::\n\n```{.r .cell-code}\nfixef( modelKS5 )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)    MINORITY       age13        EXPO \n 0.34083507 -0.05704474  0.04733879  0.35207989 \n```\n:::\n:::\n\nNote that the estimates for all the effects are essentially unchanged.\nHowever, the AIC is almost 40 points better.\nAlso, because the model has done a better job explaining residual variance, the p-value for the coefficient on MINORITY has dropped from 0.032 to 0.029, as we can see on the summary display below.\nThis is not a large drop, but a noticeable one:\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary( modelKS5 )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed-effects model fit by REML\n  Data: nys1 \n        AIC       BIC   logLik\n  -433.5943 -398.7338 223.7972\n\nRandom effects:\n Formula: ~1 | ID\n        (Intercept)  Residual\nStdDev:   0.1186009 0.1864592\n\nCorrelation Structure: ARMA(1,0)\n Formula: ~AGE | ID \n Parameter estimate(s):\n     Phi1 \n0.3212696 \nFixed effects:  ATTIT ~ MINORITY + age13 + EXPO \n                 Value   Std.Error  DF   t-value p-value\n(Intercept)  0.3408351 0.011858053 838 28.742920   0.000\nMINORITY    -0.0570447 0.025968587 237 -2.196683   0.029\nage13        0.0473388 0.004523745 838 10.464512   0.000\nEXPO         0.3520799 0.024451130 838 14.399330   0.000\n Correlation: \n         (Intr) MINORI age13 \nMINORITY -0.457              \nage13    -0.013  0.010       \nEXPO      0.006 -0.001 -0.224\n\nStandardized Within-Group Residuals:\n        Min          Q1         Med          Q3         Max \n-2.82097989 -0.65136103 -0.08846076  0.61819746  2.77303796 \n\nNumber of Observations: 1079\nNumber of Groups: 239 \n```\n:::\n:::\n\nIs any of this drop in the $p$-value due to overfitting?\nGiven the size of the change in AIC, it seems doubtful that that's a significant factor.\n\nLet's try including heteroskedasticity, without AR\\[1\\]:\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelKS6 = lme(ATTIT ~ MINORITY + age13 + EXPO, \n              data=nys1,\n              random=~1|ID,\n              weights=varIdent(form=~1|agefac) )\nAIC( modelKS6 )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -389.5696\n```\n:::\n:::\n\nThis did not improve AIC in this case, so we can avoid looking at this model further.\n\nFor completeness, let's look at a model with both AR(1) and heteroskedasticity:\n\n::: {.cell}\n\n```{.r .cell-code}\nmodelKS7 = lme(ATTIT ~ MINORITY + age13 + EXPO, \n              data=nys1,\n              random=~1|ID,\n              correlation=corAR1(form=~AGE|ID),\n              weights=varIdent(form=~1|agefac) )\nAIC( modelKS7 )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -431.1943\n```\n:::\n:::\n\nAgain, no improvement.\nSo we settle with our AR\\[1\\] model with a random intercept to get overall level of a student.\n",
    "supporting": [
      "complex_error_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}