{
  "hash": "d9477e1fd08c06e5fe290accdfef17cb",
  "result": {
    "markdown": "---\ntitle: \"Intro to Regression\"\nauthor: \"Luke Miratrix\"\neditor: \n  markdown: \n    wrap: sentence\n---\n\n\n\n\nThis walkthrough shows how to fit simple linear regression models in R.\nLinear regression is the main way researchers tend to examine the relationships between multiple variables.\nThis document runs through some code without too much discussion, with the assumption that you are already familiar with interpretation of such models.\n\n## Simple Regression\n\nWe are going to use an example dataset, `RestaurantTips`, that records tip amounts for a series of bills.\nLet's first regress `Tip` on `Bill`.\nBefore doing regression, we should plot the data to make sure using simple linear regression is reasonable.\nFor kicks, we add in an automatic regression line as well by taking advantage of ggplot's `geom_smooth()` method:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# load the data into memory\ndata(RestaurantTips)\n\n# plot Tip on Bill\nggplot( RestaurantTips, aes(x = Bill, y = Tip) ) +\n    geom_point() +\n    geom_smooth( method=\"lm\", se=FALSE ) +\n    geom_smooth( method=\"loess\", se=FALSE, col=\"orange\" ) +\n    labs(title = \"Tip given Bill\")\n```\n\n::: {.cell-output-display}\n![](intro_linear_regression_files/figure-html/RegressionCheck-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\nThat looks pretty darn linear!\nThere are a few unusually large tips, but no extreme outliers, and variability appears to be constant at all levels of `Bill` , so we proceed:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# fit the linear model\nmod <- lm(Tip ~ Bill, data = RestaurantTips)\nsummary(mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Tip ~ Bill, data = RestaurantTips)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-2.391 -0.489 -0.111  0.284  5.974 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.29227    0.16616   -1.76    0.081 .  \nBill         0.18221    0.00645   28.25   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.98 on 155 degrees of freedom\nMultiple R-squared:  0.837,\tAdjusted R-squared:  0.836 \nF-statistic:  798 on 1 and 155 DF,  p-value: <2e-16\n```\n:::\n:::\n\n\nThe first line tells R to fit the regression.\nThe thing on the left of the `~` is our outcome, the things on the right are our covariates or predictors.\nR then saves the results of all that work under the name `mod` (short for model - you can call it anything you want).\nOnce we fit the model, we used `summary()` command to print the output to the screen.\n\nResults relevant to the intercept are in the `(Intercept)` row and results relevant to the slope are in the `Bill` row (`Bill` is the explanatory variable).\nThe `Estimate` column gives the estimated coefficients, the `Std. Error` column gives the standard error for these estimates, the `t value` is simply estimate/SE, and the p-value is the result of a hypothesis test testing whether that coefficient is significantly different from 0.\n\nWe also see the RMSE as `Residual standard error` and $R^2$ as `Multiple R-squared`.\nThe last line of the regression output gives details relevant to an ANOVA table for testing our model against no model.\nIt has the F-statistic, degrees of freedom, and p-value.\n\nYou can pull the coefficients of your model out with the `coef()` command:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncoef(mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)        Bill \n     -0.292       0.182 \n```\n:::\n\n```{.r .cell-code}\ncoef(mod)[1] # intercept\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept) \n     -0.292 \n```\n:::\n\n```{.r .cell-code}\ncoef(mod)[2] # slope\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Bill \n0.182 \n```\n:::\n\n```{.r .cell-code}\ncoef(mod)[\"Bill\"] # alternate way.\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Bill \n0.182 \n```\n:::\n:::\n\n\nAlternatively, you can use the `tidy()` function from `broom` to turn the regression results into a tidy data frame, which makes it easier to work with:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntidy(mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 Ã— 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)   -0.292   0.166       -1.76 8.06e- 2\n2 Bill           0.182   0.00645     28.2  5.24e-63\n```\n:::\n\n```{.r .cell-code}\ntidy(mod)[[2,2]] # slope\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.182\n```\n:::\n:::\n\n\nWe can plot our regression line on top of the scatterplot manually using the `geom_abline()` layer in ggplot:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot( RestaurantTips, aes( Bill, Tip ) ) +\n  geom_point() +\n  geom_abline( intercept = -0.292, slope =  0.182, col=\"red\" )\n```\n\n::: {.cell-output-display}\n![](intro_linear_regression_files/figure-html/LinearRegressionPlot-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\n## Multiple Regression\n\nWe now include the additional explanatory variables of number in party (`Guests`) and whether or not they pay with a credit card (`Credit`):\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntip.mod <- lm(Tip ~ Bill + Guests + Credit, data=RestaurantTips )\nsummary(tip.mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Tip ~ Bill + Guests + Credit, data = RestaurantTips)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-2.384 -0.478 -0.108  0.272  5.984 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.25468    0.20273   -1.26     0.21    \nBill         0.18302    0.00846   21.64   <2e-16 ***\nGuests      -0.03319    0.10282   -0.32     0.75    \nCredity      0.04217    0.18282    0.23     0.82    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.985 on 153 degrees of freedom\nMultiple R-squared:  0.838,\tAdjusted R-squared:  0.834 \nF-statistic:  263 on 3 and 153 DF,  p-value: <2e-16\n```\n:::\n:::\n\n\nThis output should look very similar to the output for one variable, except now there is a row corresponding to each explanatory variable.\nOur two-category (y, n) `Credit` variable was automatically converted to a 0-1 dummy variable (with \"y\" being 1 and \"n\" our baseline).\n\nYou can make plots and tables of your fit models.\nFor one easy kind of regression graph, try `ggeffects`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# graph model 2, with Bill on X, Credit as color, and Guests held constant at the mean\nggeffect(tip.mod, terms = c(\"Bill\", \"Credit\")) |> \n  plot(add.data = TRUE, ci = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRaw data not available.\n```\n:::\n\n::: {.cell-output-display}\n![](intro_linear_regression_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\nFor making tables, @sec-make-regression-tables.\n\n## Categorical Variables (and Factors)\n\nYou can include any explanatory categorical variable in a multiple regression model, and R will automatically create corresponding 0/1 variables.\nFor example, if you were to include gender coded as male/female, R would create a variable GenderMale that is 1 for males and 0 for females.\n\n### Numbers Coding Categories.\n\nIf you have multiple levels of a category, but your levels are coded with numbers you have to be a bit careful because R can treat this as a quantitative (continuous) variable by mistake in some cases.\nYou will know it did this if you only see the single variable on one line of your output.\nFor categorical variables with $k$ categories, you should see $k-1$ lines.\n\nTo make a variable categorical, even if the levels are numbers, convert the variable to a factor with `as.factor` or `factor`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# load the US states data\ndata( USStates )\n\n# convert Region to a factor\nUSStates <- USStates |> \n  mutate(Region = factor(Region))\n```\n:::\n\n\n### Setting new baselines.\n\nWe can reorder the levels if desired (the first is our baseline).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlevels( USStates$Region )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"MW\" \"NE\" \"S\"  \"W\" \n```\n:::\n\n```{.r .cell-code}\nUSStates$Region = relevel(USStates$Region, \"S\" )\nlevels( USStates$Region )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"S\"  \"MW\" \"NE\" \"W\" \n```\n:::\n:::\n\n\nNow any regression will use the south as baseline.\n\n### Testing for significance of a categorical variable.\n\nWhen deciding whether to keep a categorical variable, we need to test how important all the dummy variables for that category are to the model all at once.\nWe do this with ANOVA.\nHere we examine whether region is useful for predicting the percent vote for Clinton in 2016:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmlm = lm( ClintonVote ~ Region, data=USStates)\nanova( mlm )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nResponse: ClintonVote\n          Df Sum Sq Mean Sq F value  Pr(>F)    \nRegion     3   1643     548    6.99 0.00057 ***\nResiduals 46   3603      78                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nIt is quite important.\n\nWe can also compare for region beyond some other variable:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmlm2 = lm( ClintonVote ~ HouseholdIncome + HouseholdIncome + HighSchool + \n               EighthGradeMath, data=USStates)\n\nmlm3 = lm( ClintonVote ~ HouseholdIncome + HouseholdIncome + HighSchool + \n               EighthGradeMath + Region, data=USStates)\nanova( mlm2, mlm3 )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Variance Table\n\nModel 1: ClintonVote ~ HouseholdIncome + HouseholdIncome + HighSchool + \n    EighthGradeMath\nModel 2: ClintonVote ~ HouseholdIncome + HouseholdIncome + HighSchool + \n    EighthGradeMath + Region\n  Res.Df  RSS Df Sum of Sq    F Pr(>F)  \n1     46 3287                           \n2     43 2649  3       638 3.45  0.025 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nRegion is still important, beyond including some further controls.\nInterpreting this mess of a regression is not part of this document; this document shows you how to run regressions but it doesn't discuss whether you should or not.\n\n### Missing levels in a factor\n\nR often treats categorical variables as factors.\nThis is often useful, but sometimes annoying.\nA factor has different **levels** which are the different values it can be.\nFor example:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata(FishGills3)\nlevels(FishGills3$Calcium)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"\"       \"High\"   \"Low\"    \"Medium\"\n```\n:::\n\n```{.r .cell-code}\ntable(FishGills3$Calcium)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n         High    Low Medium \n     0     30     30     30 \n```\n:::\n:::\n\n\nNote the weird nameless level; it also has no actual observations in it.\nNevertheless, if you make a boxplot, you will get an empty plot in addition to the other three.\nThis error was likely due to some past data entry issue.\nYou can drop the unused level:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nFishGills3$Calcium = droplevels(FishGills3$Calcium)\n```\n:::\n\n\nYou can also turn a categorical variable into a numeric one like so:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nsummary( FishGills3$Calcium )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  High    Low Medium \n    30     30     30 \n```\n:::\n\n```{.r .cell-code}\nasnum = as.numeric( FishGills3$Calcium )\nasnum\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3\n[39] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n[77] 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n```\n:::\n:::\n\n\nRegression on only a categorical variable is fine:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmylm = lm( GillRate ~ Calcium, data=FishGills3 )\nmylm\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = GillRate ~ Calcium, data = FishGills3)\n\nCoefficients:\n  (Intercept)     CalciumLow  CalciumMedium  \n         58.2           10.3            0.5  \n```\n:::\n:::\n\n\nR has made you a bunch of dummy variables automatically.\nHere \"high\" is the baseline, selected automatically.\nWe can also force it so there is no baseline by removing the intercept, in which case the coefficients are the means of each group.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmymm = lm( GillRate ~ 0 + Calcium, data=FishGills3 )\nmymm\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = GillRate ~ 0 + Calcium, data = FishGills3)\n\nCoefficients:\n  CalciumHigh     CalciumLow  CalciumMedium  \n         58.2           68.5           58.7  \n```\n:::\n:::\n\n\n## Some extensions (optional)\n\n### Confidence Intervals\n\nTo get confidence intervals around each parameter in your model, try this:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nconfint(tip.mod)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             2.5 % 97.5 %\n(Intercept) -0.655  0.146\nBill         0.166  0.200\nGuests      -0.236  0.170\nCredity     -0.319  0.403\n```\n:::\n:::\n\n\nYou can also create them easily using `tidy` and `mutate`:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntip.mod |> \n  tidy() |> \n  mutate(upper = estimate + 1.96*std.error,\n         lower = estimate - 1.96*std.error)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4 Ã— 7\n  term        estimate std.error statistic  p.value upper  lower\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl> <dbl>  <dbl>\n1 (Intercept)  -0.255    0.203      -1.26  2.11e- 1 0.143 -0.652\n2 Bill          0.183    0.00846    21.6   2.07e-48 0.200  0.166\n3 Guests       -0.0332   0.103      -0.323 7.47e- 1 0.168 -0.235\n4 Credity       0.0422   0.183       0.231 8.18e- 1 0.400 -0.316\n```\n:::\n:::\n\n\n### Prediction\n\nSuppose a server at this bistro is about to deliver a \\$20 bill, and wants to predict their tip.\nThey can get a predicted value and 95% (this is the default level, change with level) prediction interval with\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnew.dat = data.frame( Bill = c(20) )\npredict(mod,new.dat,interval = \"prediction\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   fit  lwr  upr\n1 3.35 1.41 5.29\n```\n:::\n:::\n\n\nThey should expect a tip somewhere between \\$1.41 and \\$5.30.\n\nIf we know a bit more we can use our more complex model called `tip.mod` from above:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnew.dat = data.frame( Bill = c(20), Guests=c(1), Credit=c(\"n\") )\npredict(tip.mod,new.dat,interval = \"prediction\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   fit  lwr  upr\n1 3.37 1.41 5.34\n```\n:::\n:::\n\n\nThis is the predicted tip for one guest paying with cash for a \\$20 tip.\nIt is wider than our original interval because our model is a bit more unstable (it turns out guest number and credit card aren't that relevant or helpful).\n\nCompare the prediction interval to the confidence interval\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnew.dat = data.frame( Bill = c(20), Guests=c(1), Credit=c(\"n\") )\npredict(tip.mod, new.dat, interval = \"confidence\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   fit  lwr  upr\n1 3.37 3.09 3.65\n```\n:::\n:::\n\n\nThis predicts the mean tip for all single guests who pay a \\$20 bill with cash.\nOur interval is smaller because we are generating a confidence interval for where the mean is, and are ignoring that individuals will vary around that mean.\nConfidence intervals are different from prediction intervals.\n\n### Removing Outliers\n\nIf you can identify which rows the outliers are on, you can do this by hand (say the rows are 5, 10, 12).\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnew.data = old.data[ -c(5,10,12), ]\nlm( Y ~ X, data=new.data )\n```\n:::\n\n\nSome technical details: The `c(5,10,12)` is a list of 3 numbers.\nThe `c()` is the concatenation function that takes things makes lists out of them.\nThe \"-list\" notation means give me my old data, but without rows 5, 10, and 12.\nNote the comma after the list.\nThis is because we identify elements in a dataframe with row, column notation.\nSo `old.data[1,3]` would be row 1, column 3.\n\nIf you notice your points all have X bigger than some value, say 20.5, you could use filtering to keep everything less than some value:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nnew.data = filter( old.data, X <= 20.5 )\n```\n:::\n\n\n### Missing data\n\nIf you have missing data, `lm` will automatically drop those cases because it doesn't know what else to do.\nIt will tell you this, however, with the `summary` command.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ndata(AllCountries)\ndev.lm = lm( BirthRate ~ Rural + Health + ElderlyPop, data=AllCountries )\nsummary( dev.lm  )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = BirthRate ~ Rural + Health + ElderlyPop, data = AllCountries)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-16.592  -3.728  -0.791   3.909  16.218 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  26.5763     1.6795   15.82  < 2e-16 ***\nRural         0.0985     0.0224    4.40  1.9e-05 ***\nHealth       -0.0995     0.0930   -1.07     0.29    \nElderlyPop   -1.0249     0.0881  -11.64  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.83 on 174 degrees of freedom\n  (39 observations deleted due to missingness)\nMultiple R-squared:  0.663,\tAdjusted R-squared:  0.657 \nF-statistic:  114 on 3 and 174 DF,  p-value: <2e-16\n```\n:::\n:::\n\n\n### Residual plots and model fit\n\nIf we throw out model into the `plot` function, we get some nice regression diagnostics.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot(tip.mod)\n```\n\n::: {.cell-output-display}\n![](intro_linear_regression_files/figure-html/unnamed-chunk-8-1.png){fig-align='center' width=70%}\n:::\n\n::: {.cell-output-display}\n![](intro_linear_regression_files/figure-html/unnamed-chunk-8-2.png){fig-align='center' width=70%}\n:::\n\n::: {.cell-output-display}\n![](intro_linear_regression_files/figure-html/unnamed-chunk-8-3.png){fig-align='center' width=70%}\n:::\n\n::: {.cell-output-display}\n![](intro_linear_regression_files/figure-html/unnamed-chunk-8-4.png){fig-align='center' width=70%}\n:::\n:::\n\n\nTo generate classic model fit diagnostics with more control, we need to calculate residuals, make a residual versus fitted values plot, and make a histogram of the residuals.\nWe can make some quick and dirty plots with `qplot` (standing for \"quick plot\") like so:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nqplot(tip.mod$fit, tip.mod$residuals )\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: `qplot()` was deprecated in ggplot2 3.4.0.\n```\n:::\n\n::: {.cell-output-display}\n![](intro_linear_regression_files/figure-html/ConditionsForRegression-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\nand\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nqplot(tip.mod$residuals, bins=30)\n```\n\n::: {.cell-output-display}\n![](intro_linear_regression_files/figure-html/unnamed-chunk-9-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\nWe see no real pattern other than some extreme outliers.\nThe residual histogram suggests we are not really normally distributed, so we should treat our SEs and $p$-values with caution.\nThese plots are the canonical \"model-checking'' plots you might use.\n\nAnother is the \"fitted outcomes vs. actual outcomes'' plot of:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npredicted = predict( dev.lm )\nactual = dev.lm$model$BirthRate\nqplot( actual, predicted, main=\"Fit vs. actual Birth Rate\" )\n```\n\n::: {.cell-output-display}\n![](intro_linear_regression_files/figure-html/ConditionsForRegression2-1.png){fig-align='center' width=70%}\n:::\n:::\n\n\nNote the `dev.lm` variable has a `model` variable inside it.\nThis is a data frame of the **used** data for the model (i.e., if cases were dropped due to missingness, they will not be in the model).\nWe then grab the birth rates from this, and make a scatterplot.\nIf we tried to skip this, and use the original data, we would get an error because our original data set has some observations that were dropped.\n\nNote we can't just add our predictions to `AllCountries` since we would get an error due to this dropped data issue:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nAllCountries$predicted = predict( dev.lm )\n```\n:::\n\n\n```         \nError in `$<-.data.frame`(`*tmp*`, predicted, value = c(`1` = 31.630301617421,  : \n  replacement has 179 rows, data has 217\n```\n\nWe can, however, predict like this:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nAllCountries$predicted = predict( dev.lm, newdata=AllCountries )\n```\n:::\n\n\nThe `newdata` tells predict to generate a prediction for each row in AllCountries rather than each row in the left over data after `lm` dropped cases with missing values.\n",
    "supporting": [
      "intro_linear_regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}