{
  "hash": "5e04efbdafec44485359baaab4ee49bb",
  "result": {
    "markdown": "---\ntitle: \"Walk-through of calculating robust standard errors\"\nauthor: \"Luke Miratrix\"\neditor: \n  markdown: \n    wrap: sentence\n---\n\n\n\n\n\nIn this document, we'll discuss approaches to dealing with clustered data which focus on getting the standard errors for the coefficients right, without bothering with modeling the second level.\nWe'll start by discussing an approach for correcting for heteroscedasticity (unequal variance in the residuals at different levels of the predictors), and then show how to use a similar technique to correct for residuals which may be correlated within clusters.\n\nThe goal is to show you how to use *cluster-robust standard errors* to correct for biased standard errors introduced by working with clustered data.\nWe'll also show you how you can implement some model-fitting techniques using the matrix operations in R.\n\nWe'll be working with data we've seen before (The High School and Beyond dataset.)\n\nWhile this document shows how to calculate things by hand, it also shows the relevant R packages to automate it so you don't have to bother.\nThe \"by-hand\" stuff is for interest, and to see what is happening under the hood.\n\n## Robust errors (no clustering)\n\nThe (no clustering, ordinary) linear regression model assumes that\n\n$$y = X\\beta + \\varepsilon$$\n\nwith the $\\varepsilon$'s independently and identically normally distributed with variance $\\sigma^2$.\nHere $\\beta$ is a column vector of regression coefficients, $(\\beta_0, \\beta_1)$ in our example.\n$y$ is a vector of the outcomes and $\\varepsilon$ is a vector of the residuals.\n$X$ is a $n$ by $p$ matrix referred to as the *model matrix* (p is the number of predictors, including the intercept).\nIn this example, the first column of the matrix is all 1's, for the intercept, and the second column is each person's value for ses.\nThe third is each person's value for sector (which will be the same for all students in a single school).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat = read.spss( \"data/hsb1.sav\", to.data.frame=TRUE )\nsdat = read.spss( \"data/hsb2.sav\", to.data.frame=TRUE )\ndat = merge( dat, sdat, by=\"id\", all.x=TRUE )\ndat = dat[ c( \"id\", \"mathach\", \"ses\", \"sector\" ) ]\ndat$id <- factor( dat$id ) ### make the school variable a factor\nhead( dat )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    id mathach    ses sector\n1 1224   5.876 -1.528      0\n2 1224  19.708 -0.588      0\n3 1224  20.349 -0.528      0\n4 1224   8.781 -0.668      0\n5 1224  17.898 -0.158      0\n6 1224   4.583  0.022      0\n```\n:::\n:::\n\n\n\nMaking a model matrix from a regression\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX <- model.matrix( mathach ~ ses + sector, data = dat )\nhead( X )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  (Intercept)    ses sector\n1           1 -1.528      0\n2           1 -0.588      0\n3           1 -0.528      0\n4           1 -0.668      0\n5           1 -0.158      0\n6           1  0.022      0\n```\n:::\n\n```{.r .cell-code}\ny <- dat$mathach\nhead( y )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  5.876 19.708 20.349  8.781 17.898  4.583\n```\n:::\n:::\n\n\n\nWith these assumptions, our estimate for $\\beta$ using the OLS criterion is $\\hat{\\beta} = (X^TX)^{-1}X^Ty$.\nWe can calculate this directly with R.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsolve(t(X) %*% X) %*% t(X) %*% y ##(X'X)^{-1}X'y\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 [,1]\n(Intercept) 11.793254\nses          2.948558\nsector       1.935013\n```\n:::\n:::\n\n\n\nCompare with lm: they are the same!\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod = lm(mathach ~ ses + sector, data = dat)\nmod \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = mathach ~ ses + sector, data = dat)\n\nCoefficients:\n(Intercept)          ses       sector  \n     11.793        2.949        1.935  \n```\n:::\n:::\n\n\n\nWe can also estimate standard errors for the coefficients by taking $\\sqrt{\\hat{\\sigma}^2diag((X^TX)^{-1})}$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbeta_hat <- solve(t(X) %*% X) %*% t(X) %*% y\npreds <- X %*% beta_hat\nresids <- y - preds\nsigma_2_hat <- sum(resids^2)/(nrow(X)-3) ### estimate of the residual variance\nsqrt(sigma_2_hat * diag(solve(t(X) %*% X))) ### using the matrix algebra\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)         ses      sector \n 0.10610213  0.09783058  0.15249341 \n```\n:::\n:::\n\n\n\nAgain, compare:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary( arm )\ndisplay( mod ) ### same results\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nlm(formula = mathach ~ ses + sector, data = dat)\n            coef.est coef.se\n(Intercept) 11.79     0.11  \nses          2.95     0.10  \nsector       1.94     0.15  \n---\nn = 7185, k = 3\nresidual sd = 6.35, R-Squared = 0.15\n```\n:::\n:::\n\n\n\nBut notice that this assumes that the residuals have a single variance, $\\sigma^2$.\nFrequently this assumption is implausible, in which case the standard errors we derive may not be correct.\nIt would be useful to have a way to derive standard errors which does not require us to assume that the residuals are homoscedastic.\nThis is where *heteroscedasticity-robust standard errors*, or Huber-White standard errors, come in.\nHuber-White standard errors are asymptotically correct, even if the residual variance is not constant at all values of the predictor.\n\nThe basic idea behind Huber-White standard errors is that we let each individual residual serve as an estimate of the variance of the residuals at that value of the predictors.\nIf we let $V = (X^TX)^{-1},$ $N$ be the number of observations, and $K$ be the number of predictors, including the intercept, then the formula for the standard errors is\n\n$$ SE^2 = \\frac{N}{N-K} \\cdot diag\\left( V \\cdot \\left( \\sum X_i X_i^T \\varepsilon_i^2 \\right) \\cdot V\\right) $$\n\nThis is called a sandwich estimator, where $V$ is the bread and $\\sum X_i X_i^T \\varepsilon_i^2$ (which is a $K$ by $K$ matrix) is the meat.\nBelow, we implement this in R.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nN <- nrow(dat) ### number of observations\nK <- 3 ### number of regression coefficients, including the intercept\nV <- solve(t(X) %*% X) ### the bread\nV\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              (Intercept)           ses        sector\n(Intercept)  2.796108e-04  3.460078e-05 -0.0002847979\nses          3.460078e-05  2.377141e-04 -0.0000702375\nsector      -2.847979e-04 -7.023750e-05  0.0005775742\n```\n:::\n\n```{.r .cell-code}\nmeat <- matrix(0, nrow = K, ncol = K) ### we'll build the meat as we go, iterating over the \n                                      ### individual rows\nfor(i in 1:nrow(dat)){\n  this_point <- X[i, ] %*% t(X[i, ]) * resids[i]^2 ### the contribution of this particular \n                                                   ### point\n  meat <- meat + this_point ### take the current meat, and add this point's contribution\n}\nmeat\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     (Intercept)        ses     sector\n[1,]  289161.019  -3048.176 133136.299\n[2,]   -3048.176 159558.729   9732.201\n[3,]  133136.299   9732.201 133136.299\n```\n:::\n\n```{.r .cell-code}\nSEs = sqrt(diag(N/(N-K) * V %*% meat %*% V)) ### standard errors\nSEs\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)         ses      sector \n 0.11021454  0.09487279  0.15476724 \n```\n:::\n:::\n\n\n\nNotice that the estimated standard errors haven't changed much, so whatever heteroscedasticity is present in this association doesn't seem to be affecting them.\n\nCombining the above steps in a tidy bit of code gives:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod <- lm(mathach ~ ses + sector, data = dat)\nresids = resid( mod )\n\nX <- model.matrix(mathach ~ ses + sector, data = dat)\n\nV <- solve(t(X) %*% X) ### the bread\nvcov_hw = V %*% t(X) %*% diag(resids^2) %*% X %*% V\n\nvcov_hw\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             (Intercept)          ses       sector\n(Intercept)  0.012142174  0.001957716 -0.012535538\nses          0.001957716  0.008997088 -0.003992666\nsector      -0.012535538 -0.003992666  0.023942897\n```\n:::\n\n```{.r .cell-code}\nsqrt(diag(vcov_hw)) ### standard errors\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)         ses      sector \n 0.11019153  0.09485298  0.15473493 \n```\n:::\n\n```{.r .cell-code}\nsqrt( diag( vcov( mod ) ) )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)         ses      sector \n 0.10610213  0.09783058  0.15249341 \n```\n:::\n:::\n\n\n\n### R Packages to do all this for you\n\nThere is an R package to do all of this for us.\nThe following gives us the \"Variance Covariance\" matrix:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sandwich)\nvc <- vcovHC( mod, type = \"HC0\")\nprint( vc, digits=3 )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            (Intercept)      ses   sector\n(Intercept)     0.01214  0.00196 -0.01254\nses             0.00196  0.00900 -0.00399\nsector         -0.01254 -0.00399  0.02394\n```\n:::\n:::\n\n\n\nThe square root of the diagonal are our standard errors\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsqrt( diag( vc ) )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)         ses      sector \n 0.11019153  0.09485298  0.15473493 \n```\n:::\n:::\n\n\n\nThey are what we hand-calculated above (up to some rounding error).\nObserve how the differences are all very close to zero:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsqrt( diag( vc ) ) - SEs\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  (Intercept)           ses        sector \n-2.301170e-05 -1.980850e-05 -3.231386e-05 \n```\n:::\n:::\n\n\n\nWe can use them for testing as follows\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary( lmtest )\ncoeftest( mod, vcov. = vc )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nt test of coefficients:\n\n             Estimate Std. Error t value  Pr(>|t|)    \n(Intercept) 11.793254   0.110192 107.025 < 2.2e-16 ***\nses          2.948558   0.094853  31.086 < 2.2e-16 ***\nsector       1.935013   0.154735  12.505 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n\n(Note the weird \".\". I don't know why it is part of the name.)\n\nIn fact, these packages play well together, so you can tell `lmtest` to use the `vcovHC` function as follows:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoeftest( mod, vcov. = vcovHC )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nt test of coefficients:\n\n             Estimate Std. Error t value  Pr(>|t|)    \n(Intercept) 11.793254   0.110237 106.981 < 2.2e-16 ***\nses          2.948558   0.094913  31.066 < 2.2e-16 ***\nsector       1.935013   0.154801  12.500 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n\nAll this is well and good, but everything we have done so far is **WRONG** because we have failed to account for the clustering of students within schools.\nHuber-White (Sandwich) corrections only deal with heteroskedasticity, not clustering.\nWe extend these ideas to do clustering next.\n\n## Cluster Robust Standard Errors\n\nThe next step is to get standard errors which allow the residuals to be correlated within clusters and to have non-0 means within clusters (which violates the assumption of independence of residuals).\nThe math here is harder to explain.\nWe start by calculating $X*\\varepsilon$, multiplying each row in $X$ by the associated residual.\nThen we take the column sum of $X$ within each cluster.\nThis is easiest to understand for the intercept column, where the sum is simply equal to the sum of the residuals in that cluster.\nIf all of the residuals in a cluster are large and positive (or large and negative), then this sum will be very large; if the residuals are close to mean 0 in a cluster, the sum will be small.\nWe then bind the results into a $M$ by $K$ matrix, where $M$ is the number of clusters, each row corresponds to a cluster, and each column corresponds to a coefficient, which we'll call $U$.\nThis is the meat which we sandwich with $V$.\nFinally, we take\n\n$$\\sqrt{ diag( \\frac{M}{M-1}\\frac{N-1}{N-K} VU^TUV)}$$\n\nwhich gives us estimated standard errors for the regression coefficients.\n\nThe intuition isn't so clear here, but notice that the more highly correlated residuals are within clusters (especially clusters with extreme values of the predictors), the larger $U^TU$ will be, and the less precise our estimates.\n\nHere's a \"by hand\" implementation in R.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncluster <- dat$id\nM <- length(unique(cluster))\nweight_mat <- as.vector(resids) * X ### start by calculating for each X predictor values \n                                    ### weighted by the residuals\nhead( weight_mat )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  (Intercept)        ses sector\n1   -1.411858  2.1573194      0\n2    9.648498 -5.6733165      0\n3   10.112584 -5.3394444      0\n4   -1.042618  0.6964687      0\n5    6.570618 -1.0381576      0\n6   -7.275123 -0.1600527      0\n```\n:::\n\n```{.r .cell-code}\nu_icept <- tapply(weight_mat[, '(Intercept)'], cluster, sum) ### sum up the weighted \n                                                             ### intercepts in each cluster\nu_ses <- tapply(weight_mat[, 'ses'], cluster, sum) ### sum up the weighted slopes in \n                                                       ### each cluster\nu_sector <- tapply(weight_mat[, 'sector'], cluster, sum)\n\nu <- cbind(u_icept, u_ses, u_sector)\n\n### cluster-robust standard errors\nSE.adj.hand = sqrt((M/(M-1))*((N-1)/(N-K)) * diag(V %*% t(u) %*% u %*% V)) \nSE.adj.hand\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)         ses      sector \n  0.2031455   0.1279373   0.3171766 \n```\n:::\n:::\n\n\n\nThese are a lot higher than before; there's a lot of within-cluster correlation, and our OLS-based estimated standard errors are unrealistically small.\n\nYou can use these standard errors in general if you're not interested in modeling what's happening at the cluster level and just want to get the right standard errors for your fixed effects.\n\n### Using R Packages\n\nThere is a package that gives you the cluster-robust estimate of the variance-covariance matrix.\nYou can then use this matrix to get your adjusted standard errors:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary( multiwayvcov )\n\nm1 <- lm( mathach ~ ses + sector, data=dat )\nvcov_id <- cluster.vcov(m1, dat$id)\ncoeftest(m1, vcov_id)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nt test of coefficients:\n\n            Estimate Std. Error t value  Pr(>|t|)    \n(Intercept) 11.79325    0.20315 58.0532 < 2.2e-16 ***\nses          2.94856    0.12794 23.0469 < 2.2e-16 ***\nsector       1.93501    0.31718  6.1007 1.111e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n\nCompare to if we ignored clustering:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoeftest( m1 )  ## BAD!!\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nt test of coefficients:\n\n             Estimate Std. Error t value  Pr(>|t|)    \n(Intercept) 11.793254   0.106102 111.150 < 2.2e-16 ***\nses          2.948558   0.097831  30.139 < 2.2e-16 ***\nsector       1.935013   0.152493  12.689 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n\nWe can look at how much bigger they are:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSE.adj = sqrt( diag( vcov_id ) )\nSE.bad = sqrt( diag( vcov( m1 ) ) )\nSE.adj / SE.bad\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)         ses      sector \n   1.914623    1.307743    2.079937 \n```\n:::\n:::\n\n\n\nMore than 100% bigger for our sector variable and intercept.\nThe ses variable is less so, since it varies within cluster.\n\nFinally, we check to see that our hand-calculation is the same as the package:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSE.adj.hand - SE.adj\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  (Intercept)           ses        sector \n 1.296185e-14 -3.025358e-15 -2.997602e-15 \n```\n:::\n:::\n\n\n\nUp to rounding errors, we are the same!\n\n### Aside: Making your own function\n\nThe following is code to generate the var-cor matrix more efficiently.\nFor reference (or to ignore):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n cl <- function(dat, fm, cluster){\n   attach(dat, warn.conflicts = F)\n   require(sandwich)\n   require(lmtest)\n   M <- length(unique(cluster))\n   N <- length(cluster)\n   K <- fm$rank\n   dfc <- (M/(M-1))*((N-1)/(N-K))\n   uj  <- apply(estfun(fm), 2, function(x) \n                       tapply(x, cluster, sum));\n   vcovCL <- dfc*sandwich(fm, meat=crossprod(uj)/N)\n   coeftest(fm, vcovCL)\n }\n \ncl(dat, mod, dat$id)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nt test of coefficients:\n\n            Estimate Std. Error t value  Pr(>|t|)    \n(Intercept) 11.79325    0.20315 58.0532 < 2.2e-16 ***\nses          2.94856    0.12794 23.0469 < 2.2e-16 ***\nsector       1.93501    0.31718  6.1007 1.111e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n",
    "supporting": [
      "cluster_demo_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}