{
  "hash": "1b7d7d189a04b5925d92b41b596cee57",
  "result": {
    "markdown": "---\ntitle: \"How to extract information from fitted `lmer` models\"\nauthor: \"Luke Miratrix\"\neditor: \n  markdown: \n    wrap: sentence\n---\n\n::: {.cell}\n\n:::\n\n\nCheck out the newer chapter on `broom` for a simpler approach to extracting information from `lmer` models.\n\n## Introduction\n\nThis document walks through various R code to pull information out of a multilevel model (and OLS models as well, since the methods generally work on everything).\nFor illustration, we will use a random-slope model on the HS&B dataset with some level 1 and level 2 fixed effects.\n\n### Libraries\n\nWe use the following libraries in this file:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary( lme4 )\nlibrary( foreign ) ## to load data\nlibrary( arm )\nlibrary( tidyverse )\n```\n:::\n\n\n### Loading the data\n\nLoading the data is simple.\nWe read student and school level data and merge:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndat = read.spss( \"data/hsb1.sav\", to.data.frame=TRUE )\nsdat = read.spss( \"data/hsb2.sav\", to.data.frame=TRUE )\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nre-encoding from CP1252\n```\n:::\n\n```{.r .cell-code}\ndat = merge( dat, sdat, by=\"id\", all.x=TRUE )\nhead( dat, 3 )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    id minority female    ses mathach size sector pracad disclim himinty\n1 1224        0      1 -1.528   5.876  842      0   0.35   1.597       0\n2 1224        0      1 -0.588  19.708  842      0   0.35   1.597       0\n3 1224        0      0 -0.528  20.349  842      0   0.35   1.597       0\n  meanses\n1  -0.428\n2  -0.428\n3  -0.428\n```\n:::\n:::\n\n\n## Fitting and viewing the model\n\nNow we fit the random slope model with the level-2 covariates:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nM1 = lmer( mathach ~ 1 + ses + meanses + (1 + ses|id), data=dat )\n```\n:::\n\n\nTo get an overview of what our fitted model is, use `arm`'s `display()` method:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndisplay( M1 )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nlmer(formula = mathach ~ 1 + ses + meanses + (1 + ses | id), \n    data = dat)\n            coef.est coef.se\n(Intercept) 12.65     0.15  \nses          2.19     0.12  \nmeanses      3.78     0.38  \n\nError terms:\n Groups   Name        Std.Dev. Corr  \n id       (Intercept) 1.64           \n          ses         0.67     -0.21 \n Residual             6.07           \n---\nnumber of obs: 7185, groups: id, 160\nAIC = 46575.4, DIC = 46552.4\ndeviance = 46556.9 \n```\n:::\n:::\n\n\n### The `summary()` method\n\nWe can also look at the messier default `summary()` command, which gives you more output.\nThe real win is if we use the `lmerTest` library and fit our model with that package loaded, our `summary()` is more exciting and has $p$-values:\n\n\n::: {.cell warnings='false'}\n\n```{.r .cell-code}\nlibrary( lmerTest )\nM1 = lmer( mathach ~ 1 + ses + meanses + (1 + ses|id), data=dat )\nsummary( M1 )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: mathach ~ 1 + ses + meanses + (1 + ses | id)\n   Data: dat\n\nREML criterion at convergence: 46561.4\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.1671 -0.7270  0.0163  0.7547  2.9646 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr \n id       (Intercept)  2.695   1.6417        \n          ses          0.453   0.6731   -0.21\n Residual             36.796   6.0659        \nNumber of obs: 7185, groups:  id, 160\n\nFixed effects:\n            Estimate Std. Error       df t value Pr(>|t|)    \n(Intercept)  12.6513     0.1506 152.9599  84.000   <2e-16 ***\nses           2.1903     0.1218 178.2055  17.976   <2e-16 ***\nmeanses       3.7812     0.3826 181.7675   9.883   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n        (Intr) ses   \nses     -0.080       \nmeanses -0.028 -0.256\n```\n:::\n:::\n\n\nIf we just print the object, e.g., by typing the name of the model on the console, we get minimal information:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nM1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear mixed model fit by REML ['lmerModLmerTest']\nFormula: mathach ~ 1 + ses + meanses + (1 + ses | id)\n   Data: dat\nREML criterion at convergence: 46561.42\nRandom effects:\n Groups   Name        Std.Dev. Corr \n id       (Intercept) 1.6417        \n          ses         0.6731   -0.21\n Residual             6.0659        \nNumber of obs: 7185, groups:  id, 160\nFixed Effects:\n(Intercept)          ses      meanses  \n     12.651        2.190        3.781  \n```\n:::\n:::\n\n\n## Obtaining Fixed Effects\n\nR thinks of models in reduced form.\nThus when we get the fixed effects we get both the level-1 and level-2 fixed effects\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfixef( M1 )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)         ses     meanses \n  12.651300    2.190350    3.781218 \n```\n:::\n:::\n\n\nThe above is a vector of numbers.\nEach element is named, but we can index them as so:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfixef( M1 )[2]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    ses \n2.19035 \n```\n:::\n:::\n\n\nWe can also use the `[[]]` which means \"give me that element not as a list but as just the element!\" When in doubt, if you want one thing out of a list or vector, use `[[]]` instead of `[]`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfixef( M1 )[[2]]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.19035\n```\n:::\n:::\n\n\nSee how it gives you the number without the name here?\n\n## Variance and Covariance estimates of Random Effects\n\nWe can get the Variance-Covariance matrix of the random effects with `VarCorr`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nVarCorr( M1 )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n Groups   Name        Std.Dev. Corr  \n id       (Intercept) 1.64174        \n          ses         0.67309  -0.212\n Residual             6.06594        \n```\n:::\n:::\n\n\nIt displays nicely if you just print it out, but inside it are covariance matrices for each random effect group.\n(In our model we only have one group, `id`.) These matrices also have correlation matrices for reference.\nHere is how to get these pieces:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvc = VarCorr( M1 )$id\nvc\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            (Intercept)        ses\n(Intercept)   2.6953203 -0.2339045\nses          -0.2339045  0.4530494\nattr(,\"stddev\")\n(Intercept)         ses \n  1.6417431   0.6730894 \nattr(,\"correlation\")\n            (Intercept)        ses\n(Intercept)   1.0000000 -0.2116707\nses          -0.2116707  1.0000000\n```\n:::\n:::\n\n\nYou might be wondering what all the `attr` stuff is.\nR can \"tack on\" extra information to a variable via \"attributes\".\nAttributes are not part of the variable exactly, but they follows their variable around.\nThe `attr` (for attribute) method is a way to get these extra bits of information.\nIn the above, R is tacking the correlation matrix on to the variance-covariance matrix to save you the trouble of calculating it yourself.\nGet it as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nattr( vc, \"correlation\" )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            (Intercept)        ses\n(Intercept)   1.0000000 -0.2116707\nses          -0.2116707  1.0000000\n```\n:::\n:::\n\n\nYou can also just use the `vc` object as a matrix.\nHere we take the diagonal of it\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiag( vc )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)         ses \n  2.6953203   0.4530494 \n```\n:::\n:::\n\n\nIf you want an element from a matrix use row-column indexing like so:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvc[1,2]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -0.2339045\n```\n:::\n:::\n\n\nfor row 1 and column 2.\n\n#### The `sigma.hat()` and `sigma()` methods\n\nIf you just want the variances and standard deviations of your random effects, use `sigma.hat()`.\nThis also gives you the residual standard deviation as well.\nThe output is a weird object, with a list of things that are themselves lists in it.\nLet's examine it.\nFirst we look at what the whole thing is:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsigma.hat( M1 )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$sigma\n$sigma$data\n[1] 6.065939\n\n$sigma$id\n(Intercept)         ses \n  1.6417431   0.6730894 \n\n\n$cors\n$cors$data\n[1] NA\n\n$cors$id\n            (Intercept)        ses\n(Intercept)   1.0000000 -0.2116707\nses          -0.2116707  1.0000000\n```\n:::\n\n```{.r .cell-code}\nnames( sigma.hat( M1 ) )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"sigma\" \"cors\" \n```\n:::\n\n```{.r .cell-code}\nsigma.hat( M1 )$sigma\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$data\n[1] 6.065939\n\n$id\n(Intercept)         ses \n  1.6417431   0.6730894 \n```\n:::\n:::\n\n\nOur standard deviations of the random effects are\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsigma.hat( M1 )$sigma$id\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)         ses \n  1.6417431   0.6730894 \n```\n:::\n:::\n\n\nWe can get our residual variance by this weird thing (we are getting `data` from the `sigma` inside of `sigma.hat( M1 )`):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsigma.hat( M1 )$sigma$data\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 6.065939\n```\n:::\n:::\n\n\nBut here is an easier way using the `sigma()` utility function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsigma( M1 )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 6.065939\n```\n:::\n:::\n\n\n## Obtaining Emperical Bayes Estimates of the Random Effects\n\nRandom effects come out of the `ranef()` method.\nEach random effect is its own object inside the returned object.\nYou refer to these sets of effects by name.\nHere our random effect is called `id`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nests = ranef( M1 )$id\nhead( ests )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     (Intercept)         ses\n1224 -0.26204371  0.08765385\n1288  0.03805199  0.11841937\n1296 -1.91525901  0.03572247\n1308  0.30485682 -0.10500515\n1317 -1.15834807 -0.10815301\n1358 -0.98212459  0.44612877\n```\n:::\n:::\n\n\nGenerally, what you get back from these calls is a new data frame with a row for each group.\nThe rows are named with the original id codes for the groups, but if you want to connect it back to your group-level information you are going to want to merge stuff.\nTo do this, and to keep things organized, I recommend adding the id as a column to your dataframe:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(ests) = c( \"u0\", \"u1\" )\nests$id = rownames( ests )\nhead( ests )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              u0          u1   id\n1224 -0.26204371  0.08765385 1224\n1288  0.03805199  0.11841937 1288\n1296 -1.91525901  0.03572247 1296\n1308  0.30485682 -0.10500515 1308\n1317 -1.15834807 -0.10815301 1317\n1358 -0.98212459  0.44612877 1358\n```\n:::\n:::\n\n\nWe also renamed our columns of our dataframe to give them names nicer than `(Intercept)`.\nYou can use these names if you wish, however.\nYou just need to quote them with back ticks (this code is not run):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead( ests$`(Intercept)` )\n```\n:::\n\n\n### The `coef()` method\n\nWe can also get a slighly different (but generally easier to use) version these things through `coef()`.\nWhat `coef()` does is give you the estimated regression lines for each group in your data by combining the random effect for each group with the corresponding fixed effects.\nNote how in the following the `meanses` coefficient is the same, but the others vary due to the random slope and random intercept.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoefs = coef( M1 )$id\nhead( coefs )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     (Intercept)      ses  meanses\n1224    12.38926 2.278004 3.781218\n1288    12.68935 2.308769 3.781218\n1296    10.73604 2.226072 3.781218\n1308    12.95616 2.085345 3.781218\n1317    11.49295 2.082197 3.781218\n1358    11.66918 2.636479 3.781218\n```\n:::\n:::\n\n\nNote that if we have level 2 covariates in our model, they are not incorperated in the intercept and slope via `coef()`.\nWe have to do that by hand:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames( coefs ) = c( \"beta0.adj\", \"beta.ses\", \"beta.meanses\" )\ncoefs$id = rownames( coefs )\ncoefs = merge( coefs, sdat, by=\"id\" )\ncoefs = mutate( coefs, beta0 = beta0.adj + beta.meanses * meanses )\ncoefs$beta.meanses = NULL\n```\n:::\n\n\nHere we added in the impact of mean ses to the intercept (as specified by our model).\nNow if we look at the intercepts (the beta0 variables) they will incorperate the level 2 covariate effects.\nIf we then plotted a line using beta0 and beta.ses for each school, we would get the estimated lines for each school including the school-level covariate impacts.\n\n## Standard errors\n\nWe can get an object with all the standard errors of the coefficients, including the individual Emperical Bayes estimates for the individual random effects.\nThis is a lot of information.\nWe first look at the Standard Errors for the fixed effects, and then for the random effects.\nStandard errors for the variance terms are not given (this is tricker to calculate).\n\n### Fixed effect standard errors\n\n\n::: {.cell}\n\n```{.r .cell-code}\nses = se.coef( M1 )\nnames( ses )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"fixef\" \"id\"   \n```\n:::\n:::\n\n\nOur fixed effect standard errors:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nses$fixef\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1506106 0.1218474 0.3826085\n```\n:::\n:::\n\n\nYou can also get the uncertainty estimates of your fixed effects as a variance-covariance matrix:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvcov( M1 )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n3 x 3 Matrix of class \"dpoMatrix\"\n             (Intercept)          ses      meanses\n(Intercept)  0.022683560 -0.001465374 -0.001619405\nses         -0.001465374  0.014846788 -0.011954182\nmeanses     -0.001619405 -0.011954182  0.146389292\n```\n:::\n:::\n\n\nThe standard errors are the diagonal of this matrix, square-rooted.\nSee how they line up?:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsqrt( diag( vcov( M1 ) ) )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(Intercept)         ses     meanses \n  0.1506106   0.1218474   0.3826085 \n```\n:::\n:::\n\n\n### Random effect standard errors\n\nOur random effect standard errors for our EB estimates:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead( ses$id )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     (Intercept)       ses\n1224   0.7845859 0.5804186\n1288   0.9819216 0.6277115\n1296   0.7779963 0.5766319\n1308   1.0911690 0.6556607\n1317   0.8045695 0.6188535\n1358   0.9163545 0.6173954\n```\n:::\n:::\n\n\nWarning: these come as a matrix, not data frame.\nIt is probably best to do this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSEs = as.data.frame( se.coef( M1 )$id )\nhead( SEs )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     (Intercept)       ses\n1224   0.7845859 0.5804186\n1288   0.9819216 0.6277115\n1296   0.7779963 0.5766319\n1308   1.0911690 0.6556607\n1317   0.8045695 0.6188535\n1358   0.9163545 0.6173954\n```\n:::\n:::\n\n\n## Confidence intervals and uncertainty\n\nWe can compute profile confidence intervals (warnings have been suppressed)\n\n\n::: {.cell hash='lmer_extract_cache/html/unnamed-chunk-30_4780f94a1bf7632c0c2262bb9c6f8265'}\n\n```{.r .cell-code}\nconfint( M1 )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 2.5 %     97.5 %\n.sig01       1.4012799  1.8897548\n.sig02      -0.8733603  0.1945989\n.sig03       0.2274189  0.9849964\n.sigma       5.9659922  6.1689341\n(Intercept) 12.3559620 12.9462385\nses          1.9512025  2.4296954\nmeanses      3.0278219  4.5329237\n```\n:::\n:::\n\n\n## Fitted values\n\nFitted values are the predicted value for each individual given the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nyhat = fitted( M1 )\nhead( yhat )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        1         2         3         4         5         6 \n 7.290105  9.431429  9.568109  9.249189 10.410971 10.821011 \n```\n:::\n:::\n\n\nResiduals are the difference between predicted and observed:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresids = resid( M1 )\nhead( resids )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         1          2          3          4          5          6 \n-1.4141055 10.2765710 10.7808908 -0.4681887  7.4870293 -6.2380113 \n```\n:::\n:::\n\n\nWe can also predict for hypothetical new data.\nHere we predict the outcome for a random student with ses of -1, 0, and 1 in a school with mean ses of 0:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nndat = data.frame( ses = c( -1, 0, 1 ), meanses=c(0,0,0), id = -1 )\npredict( M1, newdata=ndat, allow.new.levels=TRUE )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1        2        3 \n10.46095 12.65130 14.84165 \n```\n:::\n:::\n\n\nThe `allow.new.levels=TRUE` bit says to predict for a new school (our fake school id of -1 in `ndat` above).\nIn this case it assumes the new school is typical, with 0s for the random effect residuals.\n\nIf we predict for a current school, the random effect estimates are incorporated:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nndat$id = 1296\npredict( M1, newdata=ndat )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        1         2         3 \n 8.509969 10.736041 12.962114 \n```\n:::\n:::\n\n\n## Appendix: the guts of the object\n\nWhen we fit our model and store it in a variable, R stores *a lot* of stuff.\nThe following lists some other functions that pull out bits and pieces of that stuff.\n\nFirst, to get the model matrix (otherwise called the design matrix)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmm = model.matrix( M1 )\nhead( mm )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  (Intercept)    ses meanses\n1           1 -1.528  -0.428\n2           1 -0.588  -0.428\n3           1 -0.528  -0.428\n4           1 -0.668  -0.428\n5           1 -0.158  -0.428\n6           1  0.022  -0.428\n```\n:::\n:::\n\n\nThis can be useful for predicting individual group mean outcomes, for example.\n\nWe can also ask questions such as number of groups, number of individuals:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nngrps( M1 )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n id \n160 \n```\n:::\n\n```{.r .cell-code}\nnobs( M1 )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 7185\n```\n:::\n:::\n\n\nWe can list all methods for the object (`merMod` is a more generic version of `lmerMod` and has a lot of methods we can use)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclass( M1 )\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"lmerModLmerTest\"\nattr(,\"package\")\n[1] \"lmerTest\"\n```\n:::\n\n```{.r .cell-code}\nmethods(class = \"lmerMod\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] coerce      coerce<-    contest     contest1D   contestMD   display    \n [7] getL        mcsamp      se.coef     show        sim         standardize\nsee '?methods' for accessing help and source code\n```\n:::\n\n```{.r .cell-code}\nmethods(class = \"merMod\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] anova          as.function    coef           confint        cooks.distance\n [6] deviance       df.residual    display        drop1          extractAIC    \n[11] extractDIC     family         fitted         fixef          formula       \n[16] fortify        getData        getL           getME          hatvalues     \n[21] influence      isGLMM         isLMM          isNLMM         isREML        \n[26] logLik         mcsamp         model.frame    model.matrix   ngrps         \n[31] nobs           plot           predict        print          profile       \n[36] ranef          refit          refitML        rePCA          residuals     \n[41] rstudent       se.coef        show           sigma.hat      sigma         \n[46] sim            simulate       standardize    summary        terms         \n[51] update         VarCorr        vcov           weights       \nsee '?methods' for accessing help and source code\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}