{"title":"Walk-through of calculating robust standard errors","markdown":{"yaml":{"title":"Walk-through of calculating robust standard errors","author":"Luke Miratrix","editor":{"markdown":{"wrap":"sentence"}}},"headingText":"Robust errors (no clustering)","containsRefs":false,"markdown":"\n\n```{r include=FALSE}\nlibrary( arm )\nrequire( foreign )\nlibrary( lmtest )\n\n```\n\nIn this document, we'll discuss approaches to dealing with clustered data which focus on getting the standard errors for the coefficients right, without bothering with modeling the second level.\nWe'll start by discussing an approach for correcting for heteroscedasticity (unequal variance in the residuals at different levels of the predictors), and then show how to use a similar technique to correct for residuals which may be correlated within clusters.\n\nThe goal is to show you how to use *cluster-robust standard errors* to correct for biased standard errors introduced by working with clustered data.\nWe'll also show you how you can implement some model-fitting techniques using the matrix operations in R.\n\nWe'll be working with data we've seen before (The High School and Beyond dataset.)\n\nWhile this document shows how to calculate things by hand, it also shows the relevant R packages to automate it so you don't have to bother.\nThe \"by-hand\" stuff is for interest, and to see what is happening under the hood.\n\n\nThe (no clustering, ordinary) linear regression model assumes that\n\n$$y = X\\beta + \\varepsilon$$\n\nwith the $\\varepsilon$'s independently and identically normally distributed with variance $\\sigma^2$.\nHere $\\beta$ is a column vector of regression coefficients, $(\\beta_0, \\beta_1)$ in our example.\n$y$ is a vector of the outcomes and $\\varepsilon$ is a vector of the residuals.\n$X$ is a $n$ by $p$ matrix referred to as the *model matrix* (p is the number of predictors, including the intercept).\nIn this example, the first column of the matrix is all 1's, for the intercept, and the second column is each person's value for ses.\nThe third is each person's value for sector (which will be the same for all students in a single school).\n\n```{r, warning=FALSE, message=FALSE}\ndat = read.spss( \"data/hsb1.sav\", to.data.frame=TRUE )\nsdat = read.spss( \"data/hsb2.sav\", to.data.frame=TRUE )\ndat = merge( dat, sdat, by=\"id\", all.x=TRUE )\ndat = dat[ c( \"id\", \"mathach\", \"ses\", \"sector\" ) ]\ndat$id <- factor( dat$id ) ### make the school variable a factor\nhead( dat )\n```\n\nMaking a model matrix from a regression\n\n```{r, echo=TRUE, warning=FALSE, message=FALSE}\nX <- model.matrix( mathach ~ ses + sector, data = dat )\nhead( X )\ny <- dat$mathach\nhead( y )\n```\n\nWith these assumptions, our estimate for $\\beta$ using the OLS criterion is $\\hat{\\beta} = (X^TX)^{-1}X^Ty$.\nWe can calculate this directly with R.\n\n```{r, echo=TRUE, warning=FALSE, message=FALSE}\nsolve(t(X) %*% X) %*% t(X) %*% y ##(X'X)^{-1}X'y\n```\n\nCompare with lm: they are the same!\n\n```{r}\nmod = lm(mathach ~ ses + sector, data = dat)\nmod \n```\n\nWe can also estimate standard errors for the coefficients by taking $\\sqrt{\\hat{\\sigma}^2diag((X^TX)^{-1})}$.\n\n```{r, echo=TRUE, warning=FALSE, message=FALSE}\nbeta_hat <- solve(t(X) %*% X) %*% t(X) %*% y\npreds <- X %*% beta_hat\nresids <- y - preds\nsigma_2_hat <- sum(resids^2)/(nrow(X)-3) ### estimate of the residual variance\nsqrt(sigma_2_hat * diag(solve(t(X) %*% X))) ### using the matrix algebra\n```\n\nAgain, compare:\n\n```{r}\nlibrary( arm )\ndisplay( mod ) ### same results\n```\n\nBut notice that this assumes that the residuals have a single variance, $\\sigma^2$.\nFrequently this assumption is implausible, in which case the standard errors we derive may not be correct.\nIt would be useful to have a way to derive standard errors which does not require us to assume that the residuals are homoscedastic.\nThis is where *heteroscedasticity-robust standard errors*, or Huber-White standard errors, come in.\nHuber-White standard errors are asymptotically correct, even if the residual variance is not constant at all values of the predictor.\n\nThe basic idea behind Huber-White standard errors is that we let each individual residual serve as an estimate of the variance of the residuals at that value of the predictors.\nIf we let $V = (X^TX)^{-1},$ $N$ be the number of observations, and $K$ be the number of predictors, including the intercept, then the formula for the standard errors is\n\n$$ SE^2 = \\frac{N}{N-K} \\cdot diag\\left( V \\cdot \\left( \\sum X_i X_i^T \\varepsilon_i^2 \\right) \\cdot V\\right) $$\n\nThis is called a sandwich estimator, where $V$ is the bread and $\\sum X_i X_i^T \\varepsilon_i^2$ (which is a $K$ by $K$ matrix) is the meat.\nBelow, we implement this in R.\n\n```{r, echo=TRUE, warning=FALSE, message=FALSE}\nN <- nrow(dat) ### number of observations\nK <- 3 ### number of regression coefficients, including the intercept\nV <- solve(t(X) %*% X) ### the bread\nV\n\nmeat <- matrix(0, nrow = K, ncol = K) ### we'll build the meat as we go, iterating over the \n                                      ### individual rows\nfor(i in 1:nrow(dat)){\n  this_point <- X[i, ] %*% t(X[i, ]) * resids[i]^2 ### the contribution of this particular \n                                                   ### point\n  meat <- meat + this_point ### take the current meat, and add this point's contribution\n}\nmeat\n\nSEs = sqrt(diag(N/(N-K) * V %*% meat %*% V)) ### standard errors\nSEs\n```\n\nNotice that the estimated standard errors haven't changed much, so whatever heteroscedasticity is present in this association doesn't seem to be affecting them.\n\nCombining the above steps in a tidy bit of code gives:\n\n```{r}\nmod <- lm(mathach ~ ses + sector, data = dat)\nresids = resid( mod )\n\nX <- model.matrix(mathach ~ ses + sector, data = dat)\n\nV <- solve(t(X) %*% X) ### the bread\nvcov_hw = V %*% t(X) %*% diag(resids^2) %*% X %*% V\n\nvcov_hw\nsqrt(diag(vcov_hw)) ### standard errors\nsqrt( diag( vcov( mod ) ) )\n```\n\n### R Packages to do all this for you\n\nThere is an R package to do all of this for us.\nThe following gives us the \"Variance Covariance\" matrix:\n\n```{r}\nlibrary(sandwich)\nvc <- vcovHC( mod, type = \"HC0\")\nprint( vc, digits=3 )\n```\n\nThe square root of the diagonal are our standard errors\n\n```{r}\nsqrt( diag( vc ) )\n```\n\nThey are what we hand-calculated above (up to some rounding error).\nObserve how the differences are all very close to zero:\n\n```{r}\nsqrt( diag( vc ) ) - SEs\n```\n\nWe can use them for testing as follows\n\n```{r}\nlibrary( lmtest )\ncoeftest( mod, vcov. = vc )\n```\n\n(Note the weird \".\". I don't know why it is part of the name.)\n\nIn fact, these packages play well together, so you can tell `lmtest` to use the `vcovHC` function as follows:\n\n```{r}\ncoeftest( mod, vcov. = vcovHC )\n```\n\nAll this is well and good, but everything we have done so far is **WRONG** because we have failed to account for the clustering of students within schools.\nHuber-White (Sandwich) corrections only deal with heteroskedasticity, not clustering.\nWe extend these ideas to do clustering next.\n\n## Cluster Robust Standard Errors\n\nThe next step is to get standard errors which allow the residuals to be correlated within clusters and to have non-0 means within clusters (which violates the assumption of independence of residuals).\nThe math here is harder to explain.\nWe start by calculating $X*\\varepsilon$, multiplying each row in $X$ by the associated residual.\nThen we take the column sum of $X$ within each cluster.\nThis is easiest to understand for the intercept column, where the sum is simply equal to the sum of the residuals in that cluster.\nIf all of the residuals in a cluster are large and positive (or large and negative), then this sum will be very large; if the residuals are close to mean 0 in a cluster, the sum will be small.\nWe then bind the results into a $M$ by $K$ matrix, where $M$ is the number of clusters, each row corresponds to a cluster, and each column corresponds to a coefficient, which we'll call $U$.\nThis is the meat which we sandwich with $V$.\nFinally, we take\n\n$$\\sqrt{ diag( \\frac{M}{M-1}\\frac{N-1}{N-K} VU^TUV)}$$\n\nwhich gives us estimated standard errors for the regression coefficients.\n\nThe intuition isn't so clear here, but notice that the more highly correlated residuals are within clusters (especially clusters with extreme values of the predictors), the larger $U^TU$ will be, and the less precise our estimates.\n\nHere's a \"by hand\" implementation in R.\n\n```{r, echo=TRUE, warning=FALSE, message=FALSE}\ncluster <- dat$id\nM <- length(unique(cluster))\nweight_mat <- as.vector(resids) * X ### start by calculating for each X predictor values \n                                    ### weighted by the residuals\nhead( weight_mat )\nu_icept <- tapply(weight_mat[, '(Intercept)'], cluster, sum) ### sum up the weighted \n                                                             ### intercepts in each cluster\nu_ses <- tapply(weight_mat[, 'ses'], cluster, sum) ### sum up the weighted slopes in \n                                                       ### each cluster\nu_sector <- tapply(weight_mat[, 'sector'], cluster, sum)\n\nu <- cbind(u_icept, u_ses, u_sector)\n\n### cluster-robust standard errors\nSE.adj.hand = sqrt((M/(M-1))*((N-1)/(N-K)) * diag(V %*% t(u) %*% u %*% V)) \nSE.adj.hand\n```\n\nThese are a lot higher than before; there's a lot of within-cluster correlation, and our OLS-based estimated standard errors are unrealistically small.\n\nYou can use these standard errors in general if you're not interested in modeling what's happening at the cluster level and just want to get the right standard errors for your fixed effects.\n\n### Using R Packages\n\nThere is a package that gives you the cluster-robust estimate of the variance-covariance matrix.\nYou can then use this matrix to get your adjusted standard errors:\n\n```{r}\nlibrary( multiwayvcov )\n\nm1 <- lm( mathach ~ ses + sector, data=dat )\nvcov_id <- cluster.vcov(m1, dat$id)\ncoeftest(m1, vcov_id)\n```\n\nCompare to if we ignored clustering:\n\n```{r}\ncoeftest( m1 )  ## BAD!!\n```\n\nWe can look at how much bigger they are:\n\n```{r}\nSE.adj = sqrt( diag( vcov_id ) )\nSE.bad = sqrt( diag( vcov( m1 ) ) )\nSE.adj / SE.bad\n```\n\nMore than 100% bigger for our sector variable and intercept.\nThe ses variable is less so, since it varies within cluster.\n\nFinally, we check to see that our hand-calculation is the same as the package:\n\n```{r}\nSE.adj.hand - SE.adj\n```\n\nUp to rounding errors, we are the same!\n\n### Aside: Making your own function\n\nThe following is code to generate the var-cor matrix more efficiently.\nFor reference (or to ignore):\n\n```{r}\n cl <- function(dat, fm, cluster){\n   attach(dat, warn.conflicts = F)\n   require(sandwich)\n   require(lmtest)\n   M <- length(unique(cluster))\n   N <- length(cluster)\n   K <- fm$rank\n   dfc <- (M/(M-1))*((N-1)/(N-K))\n   uj  <- apply(estfun(fm), 2, function(x) \n                       tapply(x, cluster, sum));\n   vcovCL <- dfc*sandwich(fm, meat=crossprod(uj)/N)\n   coeftest(fm, vcovCL)\n }\n \ncl(dat, mod, dat$id)\n```\n","srcMarkdownNoYaml":"\n\n```{r include=FALSE}\nlibrary( arm )\nrequire( foreign )\nlibrary( lmtest )\n\n```\n\nIn this document, we'll discuss approaches to dealing with clustered data which focus on getting the standard errors for the coefficients right, without bothering with modeling the second level.\nWe'll start by discussing an approach for correcting for heteroscedasticity (unequal variance in the residuals at different levels of the predictors), and then show how to use a similar technique to correct for residuals which may be correlated within clusters.\n\nThe goal is to show you how to use *cluster-robust standard errors* to correct for biased standard errors introduced by working with clustered data.\nWe'll also show you how you can implement some model-fitting techniques using the matrix operations in R.\n\nWe'll be working with data we've seen before (The High School and Beyond dataset.)\n\nWhile this document shows how to calculate things by hand, it also shows the relevant R packages to automate it so you don't have to bother.\nThe \"by-hand\" stuff is for interest, and to see what is happening under the hood.\n\n## Robust errors (no clustering)\n\nThe (no clustering, ordinary) linear regression model assumes that\n\n$$y = X\\beta + \\varepsilon$$\n\nwith the $\\varepsilon$'s independently and identically normally distributed with variance $\\sigma^2$.\nHere $\\beta$ is a column vector of regression coefficients, $(\\beta_0, \\beta_1)$ in our example.\n$y$ is a vector of the outcomes and $\\varepsilon$ is a vector of the residuals.\n$X$ is a $n$ by $p$ matrix referred to as the *model matrix* (p is the number of predictors, including the intercept).\nIn this example, the first column of the matrix is all 1's, for the intercept, and the second column is each person's value for ses.\nThe third is each person's value for sector (which will be the same for all students in a single school).\n\n```{r, warning=FALSE, message=FALSE}\ndat = read.spss( \"data/hsb1.sav\", to.data.frame=TRUE )\nsdat = read.spss( \"data/hsb2.sav\", to.data.frame=TRUE )\ndat = merge( dat, sdat, by=\"id\", all.x=TRUE )\ndat = dat[ c( \"id\", \"mathach\", \"ses\", \"sector\" ) ]\ndat$id <- factor( dat$id ) ### make the school variable a factor\nhead( dat )\n```\n\nMaking a model matrix from a regression\n\n```{r, echo=TRUE, warning=FALSE, message=FALSE}\nX <- model.matrix( mathach ~ ses + sector, data = dat )\nhead( X )\ny <- dat$mathach\nhead( y )\n```\n\nWith these assumptions, our estimate for $\\beta$ using the OLS criterion is $\\hat{\\beta} = (X^TX)^{-1}X^Ty$.\nWe can calculate this directly with R.\n\n```{r, echo=TRUE, warning=FALSE, message=FALSE}\nsolve(t(X) %*% X) %*% t(X) %*% y ##(X'X)^{-1}X'y\n```\n\nCompare with lm: they are the same!\n\n```{r}\nmod = lm(mathach ~ ses + sector, data = dat)\nmod \n```\n\nWe can also estimate standard errors for the coefficients by taking $\\sqrt{\\hat{\\sigma}^2diag((X^TX)^{-1})}$.\n\n```{r, echo=TRUE, warning=FALSE, message=FALSE}\nbeta_hat <- solve(t(X) %*% X) %*% t(X) %*% y\npreds <- X %*% beta_hat\nresids <- y - preds\nsigma_2_hat <- sum(resids^2)/(nrow(X)-3) ### estimate of the residual variance\nsqrt(sigma_2_hat * diag(solve(t(X) %*% X))) ### using the matrix algebra\n```\n\nAgain, compare:\n\n```{r}\nlibrary( arm )\ndisplay( mod ) ### same results\n```\n\nBut notice that this assumes that the residuals have a single variance, $\\sigma^2$.\nFrequently this assumption is implausible, in which case the standard errors we derive may not be correct.\nIt would be useful to have a way to derive standard errors which does not require us to assume that the residuals are homoscedastic.\nThis is where *heteroscedasticity-robust standard errors*, or Huber-White standard errors, come in.\nHuber-White standard errors are asymptotically correct, even if the residual variance is not constant at all values of the predictor.\n\nThe basic idea behind Huber-White standard errors is that we let each individual residual serve as an estimate of the variance of the residuals at that value of the predictors.\nIf we let $V = (X^TX)^{-1},$ $N$ be the number of observations, and $K$ be the number of predictors, including the intercept, then the formula for the standard errors is\n\n$$ SE^2 = \\frac{N}{N-K} \\cdot diag\\left( V \\cdot \\left( \\sum X_i X_i^T \\varepsilon_i^2 \\right) \\cdot V\\right) $$\n\nThis is called a sandwich estimator, where $V$ is the bread and $\\sum X_i X_i^T \\varepsilon_i^2$ (which is a $K$ by $K$ matrix) is the meat.\nBelow, we implement this in R.\n\n```{r, echo=TRUE, warning=FALSE, message=FALSE}\nN <- nrow(dat) ### number of observations\nK <- 3 ### number of regression coefficients, including the intercept\nV <- solve(t(X) %*% X) ### the bread\nV\n\nmeat <- matrix(0, nrow = K, ncol = K) ### we'll build the meat as we go, iterating over the \n                                      ### individual rows\nfor(i in 1:nrow(dat)){\n  this_point <- X[i, ] %*% t(X[i, ]) * resids[i]^2 ### the contribution of this particular \n                                                   ### point\n  meat <- meat + this_point ### take the current meat, and add this point's contribution\n}\nmeat\n\nSEs = sqrt(diag(N/(N-K) * V %*% meat %*% V)) ### standard errors\nSEs\n```\n\nNotice that the estimated standard errors haven't changed much, so whatever heteroscedasticity is present in this association doesn't seem to be affecting them.\n\nCombining the above steps in a tidy bit of code gives:\n\n```{r}\nmod <- lm(mathach ~ ses + sector, data = dat)\nresids = resid( mod )\n\nX <- model.matrix(mathach ~ ses + sector, data = dat)\n\nV <- solve(t(X) %*% X) ### the bread\nvcov_hw = V %*% t(X) %*% diag(resids^2) %*% X %*% V\n\nvcov_hw\nsqrt(diag(vcov_hw)) ### standard errors\nsqrt( diag( vcov( mod ) ) )\n```\n\n### R Packages to do all this for you\n\nThere is an R package to do all of this for us.\nThe following gives us the \"Variance Covariance\" matrix:\n\n```{r}\nlibrary(sandwich)\nvc <- vcovHC( mod, type = \"HC0\")\nprint( vc, digits=3 )\n```\n\nThe square root of the diagonal are our standard errors\n\n```{r}\nsqrt( diag( vc ) )\n```\n\nThey are what we hand-calculated above (up to some rounding error).\nObserve how the differences are all very close to zero:\n\n```{r}\nsqrt( diag( vc ) ) - SEs\n```\n\nWe can use them for testing as follows\n\n```{r}\nlibrary( lmtest )\ncoeftest( mod, vcov. = vc )\n```\n\n(Note the weird \".\". I don't know why it is part of the name.)\n\nIn fact, these packages play well together, so you can tell `lmtest` to use the `vcovHC` function as follows:\n\n```{r}\ncoeftest( mod, vcov. = vcovHC )\n```\n\nAll this is well and good, but everything we have done so far is **WRONG** because we have failed to account for the clustering of students within schools.\nHuber-White (Sandwich) corrections only deal with heteroskedasticity, not clustering.\nWe extend these ideas to do clustering next.\n\n## Cluster Robust Standard Errors\n\nThe next step is to get standard errors which allow the residuals to be correlated within clusters and to have non-0 means within clusters (which violates the assumption of independence of residuals).\nThe math here is harder to explain.\nWe start by calculating $X*\\varepsilon$, multiplying each row in $X$ by the associated residual.\nThen we take the column sum of $X$ within each cluster.\nThis is easiest to understand for the intercept column, where the sum is simply equal to the sum of the residuals in that cluster.\nIf all of the residuals in a cluster are large and positive (or large and negative), then this sum will be very large; if the residuals are close to mean 0 in a cluster, the sum will be small.\nWe then bind the results into a $M$ by $K$ matrix, where $M$ is the number of clusters, each row corresponds to a cluster, and each column corresponds to a coefficient, which we'll call $U$.\nThis is the meat which we sandwich with $V$.\nFinally, we take\n\n$$\\sqrt{ diag( \\frac{M}{M-1}\\frac{N-1}{N-K} VU^TUV)}$$\n\nwhich gives us estimated standard errors for the regression coefficients.\n\nThe intuition isn't so clear here, but notice that the more highly correlated residuals are within clusters (especially clusters with extreme values of the predictors), the larger $U^TU$ will be, and the less precise our estimates.\n\nHere's a \"by hand\" implementation in R.\n\n```{r, echo=TRUE, warning=FALSE, message=FALSE}\ncluster <- dat$id\nM <- length(unique(cluster))\nweight_mat <- as.vector(resids) * X ### start by calculating for each X predictor values \n                                    ### weighted by the residuals\nhead( weight_mat )\nu_icept <- tapply(weight_mat[, '(Intercept)'], cluster, sum) ### sum up the weighted \n                                                             ### intercepts in each cluster\nu_ses <- tapply(weight_mat[, 'ses'], cluster, sum) ### sum up the weighted slopes in \n                                                       ### each cluster\nu_sector <- tapply(weight_mat[, 'sector'], cluster, sum)\n\nu <- cbind(u_icept, u_ses, u_sector)\n\n### cluster-robust standard errors\nSE.adj.hand = sqrt((M/(M-1))*((N-1)/(N-K)) * diag(V %*% t(u) %*% u %*% V)) \nSE.adj.hand\n```\n\nThese are a lot higher than before; there's a lot of within-cluster correlation, and our OLS-based estimated standard errors are unrealistically small.\n\nYou can use these standard errors in general if you're not interested in modeling what's happening at the cluster level and just want to get the right standard errors for your fixed effects.\n\n### Using R Packages\n\nThere is a package that gives you the cluster-robust estimate of the variance-covariance matrix.\nYou can then use this matrix to get your adjusted standard errors:\n\n```{r}\nlibrary( multiwayvcov )\n\nm1 <- lm( mathach ~ ses + sector, data=dat )\nvcov_id <- cluster.vcov(m1, dat$id)\ncoeftest(m1, vcov_id)\n```\n\nCompare to if we ignored clustering:\n\n```{r}\ncoeftest( m1 )  ## BAD!!\n```\n\nWe can look at how much bigger they are:\n\n```{r}\nSE.adj = sqrt( diag( vcov_id ) )\nSE.bad = sqrt( diag( vcov( m1 ) ) )\nSE.adj / SE.bad\n```\n\nMore than 100% bigger for our sector variable and intercept.\nThe ses variable is less so, since it varies within cluster.\n\nFinally, we check to see that our hand-calculation is the same as the package:\n\n```{r}\nSE.adj.hand - SE.adj\n```\n\nUp to rounding errors, we are the same!\n\n### Aside: Making your own function\n\nThe following is code to generate the var-cor matrix more efficiently.\nFor reference (or to ignore):\n\n```{r}\n cl <- function(dat, fm, cluster){\n   attach(dat, warn.conflicts = F)\n   require(sandwich)\n   require(lmtest)\n   M <- length(unique(cluster))\n   N <- length(cluster)\n   K <- fm$rank\n   dfc <- (M/(M-1))*((N-1)/(N-K))\n   uj  <- apply(estfun(fm), 2, function(x) \n                       tapply(x, cluster, sum));\n   vcovCL <- dfc*sandwich(fm, meat=crossprod(uj)/N)\n   coeftest(fm, vcovCL)\n }\n \ncl(dat, mod, dat$id)\n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"pygments","output-file":"cluster_demo.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","bibliography":["references.bib"],"output_dir":"docs","editor":{"markdown":{"wrap":"sentence"}},"theme":"cosmo","code-copy":true,"title":"Walk-through of calculating robust standard errors","author":"Luke Miratrix"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","highlight-style":"pygments","output-file":"cluster_demo.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"block-headings":true,"bibliography":["references.bib"],"output_dir":"docs","editor":{"markdown":{"wrap":"sentence"}},"documentclass":"scrreprt","title":"Walk-through of calculating robust standard errors","author":"Luke Miratrix"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}