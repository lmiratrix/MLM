{"title":"Interpreting GLMs","markdown":{"yaml":{"title":"Interpreting GLMs","author":"Luke Miratrix, Joe McIntyre, and Josh Gilbert","editor":{"markdown":{"wrap":"sentence"}},"editor_options":{"chunk_output_type":"console"}},"headingText":"Dichotomous regression models (logistic regression)","containsRefs":false,"markdown":"\n\n\nWhen predicting either successes and failures, or proportions, we can use a model with a binomial outcome.\nHere we'll focus on models where the data is represented as individual successes and failures.\nThe canonical model for these data is logistic regression, where\n\n$$logit(E[Y|X]) \\equiv \\log\\left(\\frac{P(Y=1|X)}{1-P(Y=1|X)}\\right) = \\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p$$ $$Y \\sim Binomial(1, E[Y|X])$$\n\nWe can rewrite this model as\n\n$$odds(Y) = \\frac{P(Y=1|X)}{1-P(Y=1|X)} = e^{\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p}$$\n\nor\n\n$$P(Y=1|X) = \\frac{e^{\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p}}{1 + e^{\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p}}$$\n\nWe can interpret $\\beta_0$ as follows: for observations which are 0 on all of the predictors, we estimate that the mean value of the outcome will be $\\frac{e^{\\beta_0}}{1 + e^{\\beta_0}}$.\nThat is, we estimate that the probability of the outcome being a 'success' (assuming 'success' is coded as a 1) will be $\\frac{e^{\\beta_0}}{1 + e^{\\beta_0}}$.\n\nWe can interpret $\\beta_1$ as follows: adjusting for the other predictors, a one-unit difference in $X_1$ predicts a $\\beta_1$ difference in the log-odds of the outcome being one, or a $(e^{\\beta_1}-1)\\times100\\%$ difference in the odds of the outcome.\nUnfortunately, the change in probability of a unit change depends on where the starting point is, so there is no easy way to interpret these coefficients in terms of direct probability.\nOne can calculate the estimated change for specific units, however, and look at the distribution of those changes.\n\nOther possible link functions include the probit (which uses a Normal CDF to link $\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p$ to $P(Y=1|X)$), or the complementary log-log (which allows $P(Y = 1|X)$ to be asymmetric in the predictors), among others.\n\n### How to fit a GLM\n\nWe can fit a logistic regression model by writing\n\nglm(Y $\\sim$ X, family = binomial(link = 'logit'))\n\nWe can fit a probit regression model by writing\n\nglm(Y $\\sim$ X, family = binomial(link = 'probit'))\n\nWe can fit a complementary log-log model by writing\n\nglm(Y $\\sim$ X, family = binomial(link = 'cloglog'))\n\nWe can allow a random slope and intercept by writing\n\nglmer(Y $\\sim$ 1 + X + (1 + X\\|grp), family = binomial(link = 'logit'))\n\n## Interpreting multilevel logistic regressions\n\nIn this section, we give some further discussion about logistic regression and interpretation.\nThis section supplements Packet 6.2 (logistic and longitudinal, or the toenail data lecture).\n\n\n```{r setup_toenail, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\noptions( digits=3 )\n\n# load packages \nlibrary(tidyverse)\nlibrary(arm)\nlibrary(ggsci) # for cool color pallets\nlibrary( foreign )\ntoes = read.dta( \"data/toenail.dta\" )\nhead( toes )\n\ntoes$Tx = factor( toes$treatment, levels=c(0,1),\n                            labels=c(\"Terbinafine\", \"Itraconazole\") )\n\n```\n\nWe first fit our toenail data using a random intercept model:\n\n```{r}\nM1 = glmer( outcome ~ Tx * month + (1|patient),\n            family=binomial,\n            data=toes )\n\n\ndisplay( M1 )\n```\n\nNow let's interpret.  We have three different ways of looking at these model results, log-odds (or logits), odds, or probabilities themselves.\n\n**log-odds:** The predicted values and coefficients are in the log-odds space for a logistic model. The coefficient of `month` means each month the log-odds goes down by 0.40.  The baseline intercept of -2.51 mean that a control patient at `month=0` has a log-odds of detachment of -2.51.\n\nUsing our model, if we wanted to know the chance of detachment for a median treated patient 3 months into the trial we could calculate:\n```{r}\nfes = fixef(M1)\nlog_odds = fes[[1]] + fes[[2]] + (fes[[3]] + fes[[4]])*3\nlog_odds\n```\n\nFor a patient who has a 1 SD above-average proclivity for detachment, we would add our standard deviation of 4.56:\n```{r}\nlog_odds + 4.56\n```\n\n**odds:** The _odds_ of something happening are the chance of happening divided by the chance of not happening, or $odds = p/(1-p)$.\nTo convert log-odds to odds we just exponentiate:\n\n```{r}\nORs = exp( fixef( M1 ) )\nORs\n```\n\nThe intercept is our base odds: the odds of detachment at `month=0` for a control patient.  The rest of the coefficients are odds multipliers, multiplying our baseline (starting) odds.  For example, each month a control patient's odds of detachment gets multiplied by 0.671.\n\nNote that exponentiation and logs play like this (for a control patient at 2 months, in this example)\n$$ odds = exp( -2.51 + 2 * -0.40 ) = exp( -2.51 ) \\cdot exp( 2 * -0.40 ) = exp( -2.51 ) \\cdot exp( -0.40 )^2 $$\nSee how $exp( -0.40 )$ is a multiplier on the baseline $exp( -2.51 )$?\n\nWe can look at the math to get a bit more here:\n\n$$\n\\begin{aligned}\nlogit( Pr( Y_{ij} = 1 ) ) &= \\log odds( Pr( Y_{ij} = 1 ) ) = \\gamma_{00} + \\gamma_{01} Z_j + \\gamma_{10} Time_{ij} + \\gamma_{11} Z_j Time_{ij} + u_j\n\\end{aligned}\n$$\n(logit means log odds)\n\nWe can rewrite this as\n$$\n\\begin{aligned}\nodds( Y_{ij} = 1 ) &= \\exp\\left[ \\gamma_{00} + \\gamma_{01} Z_j + \\gamma_{10} Time_{ij} + \\gamma_{11} Z_j Time_{ij} + u_j \\right] \\\\\n&= e^{\\gamma_{00}} + e^{\\gamma_{01} Z_j} + e^{\\gamma_{10} Time_{ij}} + e^{\\gamma_{11} Z_j Time_{ij}} + e^{u_j} \\\\\n&= e^{\\gamma_{00}} \\cdot e^{\\gamma_{01} Z_j} \\cdot \\left(e^{\\gamma_{10}}\\right)^{Time_{ij}} \\cdot \\left(e^{\\gamma_{11}}\\right)^{Z_j Time_{ij}} \\cdot e^{u_j} \n\\end{aligned}\n$$\nSee how all our additive covariates turn into multiplicative factors?  And time exponentiates our factors, so we keep multiplying by the factor for each extra month.\n\nFor our two 3 month, treated patients, we have the odds of detachment of\n\n```{r}\nexp( c( log_odds, log_odds + 4.56 ) )\n```\n\n**Probabilities:** Finally, we have probabilities, which we can calculate directly with `invlogit` in the `arm` package or `plogis` in the base package:\n```{r}\nplogis( c( log_odds, log_odds + 4.56 ) )\n```\nHere we have a 1% chance of detachment at baseline for our median patient, and 53% chance for our 1SD above average patient.\n\n\n### Some math formula for reference\nThe relevant formula are:\n\n$$\nodds = \\frac{ prob }{1 - prob}\n$$\n\ngiving (letting $\\eta$ denote our log odds)\n$$\nprob = \\frac{ odds }{ 1 + odds } = \\frac{ \\exp( \\eta ) }{ 1 + \\exp(\\eta)} = \\frac{1}{1 + \\exp(-\\eta) }\n$$\nThe second equality is a simple algebraic trick to write the probability as a function where the log-odds ($\\eta$) appears only once.\n\n### More on the random intercept\n\nThe random intercepts represent each patients overall proclivity to have a detachment. High values means that patient just has a higher odds of detachment, and low values means less.\n\nIf we exponentiate our Empirical Bayes estimated random intercepts, we get multiplicative factors of how each patient's odds are just shifted by some amount.  E.g.,\n```{r}\nREs = ranef( M1 )$patient$`(Intercept)`\nhead( REs )\nsummary( REs )\nquantile( exp(REs), c( 0.05, 0.95 ) )\n```\n\nThis means the odds of detachment for some patients (the 5% least likely to have detachment) is 30\\% of the baseline detachment.\nFor a 95th percentile patient, we have an odds multiplier of around 470---they are much, much more likely to have detachment at any moment in time.\nThis is why the curves for the patients in the main lecture are so different.\n\nTo recap: our model says the baseline median patient has a very low chance of detachment. For many patients it is even lower than that, but many other patients have very high random intercepts which makes their chance of detachment much, much higher.\n\n### Growth should have random slopes?\n\nWe can try to fit a random slope model, allowing for each patient's growth in their log-odds to be different.  This is a longitudinal linear growth model, with binary outcome:\n\n```{r}\nM2 = glmer( outcome ~ Tx * month + (1+month|patient),\n            family=binomial,\n            data=toes )\n\n\ndisplay( M2 )\nanova( M1, M2 )\n```\n\nThe random slope model is strongly preferred, and also has a larger estimated effect of treatment, although the standard errors have also grown considerably.\nWhat is likely happening is the autoregressive pattern in our outcomes (note how we tend to see 1s followed by 0s, with not a lot of back and forth) coupled with the limited information we have for each patient, makes it hard to nail down differences in individual student growth vs. differences in treatment and control average growth.\nThe random intercept model focuses on within-person change, but is an easier to estimate model.\nDue to randomization, it is also trustworthy--we do not have to worry much about the assumptions.\n\n\n\n## Poisson regression models\n\nPoisson regression is sometimes used to model count data.\nThe canonical form of a Poisson (log-linear) regression model is $$\\log(E[Y|X]) = \\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p$$ $$Y \\sim Poisson(E[Y|X])$$\n\nThe Poisson distribution has only one parameter, the mean, which is also the variance of the distribution.\nSo in estimating $E[Y|X]$, we are also estimating $Var(Y|X)$.\nThis is a potential drawback to the Poisson model, because there is no variance parameter to estimate, and so incorrect models can give wildly inaccurate standard errors (frequently unrealistically small).\nA better model is a quasi-Poisson model, for which the variance is proportional to the mean, but not necessarily equal to it.\nThe negative binomial regression model is also commonly used to address over-dispersed count data where the variance exceeds the mean.\n\nThe canonical link function for Poisson outcomes is the natural logarithm.\nWhen we use a log-link, we can write\n\n$$E[Y|X] = e^{\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p}.$$\n\nWe can interpret $\\beta_0$ as follows: for observations which are 0 on all of the predictors, we estimate that the mean (expected) value of the outcome will be $e^{\\beta_0}$.\n\nWe can interpret $\\beta_1$ as follows: adjusting for the other predictors, a one-unit difference in $X_1$ predicts a $(e^{\\beta_1}-1)\\times100 \\%$ difference in the outcome.\n\nGenerally, when using a log-link, we assume that differences in the predictors are associated with multiplicative differences in the outcome.\n\nSome advantages to using an exponential link are\n\n1.  the model is mathematically more tractable and simpler to fit\n\n2.  the model parameters are easy to interpret\n\n3.  the mean of $Y$ is guaranteed to be positive for all values of $X$, which is required by the Poisson distribution\n\n### How to fit a poisson regression\n\nWe can fit a Poisson log-linear regression by writing\n\nglm(Y $\\sim$ X, family = poisson(link = 'log'))\n\nTo fit a quasi-Poisson model, write\n\nglm(Y $\\sim$ X, family = quasipoisson(link = 'log'))\n\nTo fit a negative binomial regression model, write (after loading the `MASS` library)\n\nglm.nb(Y $\\sim$ X, link='log')\n\nTo fit a Poisson regression with an identity link (where coefficients are interpreted as expected differences in the outcome associated with unit differences in the predictor), write\n\nglm(Y $\\sim$ X, family = poisson(link = 'identity'))\n\nTo fit a Poisson regression with a square root link, which is vaguely like a compromise between an identity link and a log link (and is harder to interpret than either), write\n\nglm(Y $\\sim$ X, family = poisson(link = 'sqrt'))\n\nTo fit a Poisson log-linear model with a random intercept and slope, write\n\nglmer(Y $\\sim$ X + (X\\|grp), family = poisson(link = 'log'))\n\n## GLMs vs. Transformations\n\n```{r, echo = FALSE}\nknitr::opts_chunk$set(echo = TRUE)\nknitr::opts_chunk$set(warning = FALSE)\nknitr::opts_chunk$set(message = FALSE)\n\nset.seed(2023)\n```\n\nThose of you coming from S40 and S52 may recall that when we have non-linear relationships between $X$ and $Y$, we can apply a transformation, such as taking the log, to linearize the relationship.\nIn the words of Jimmy Kim, \"with transformations, we use the *machinery of linear regression to model non-linear relationships.*\" If that's the case, then what is Poisson regression about, which deals with log counts?\nThis is a topic that confused me for many years so hopefully I can clear it up here.\n\n### Making and Graphing the Data\n\nLet's start by making some fake data.\nHere's the data-generating function, which has the relationship that a 1-unit increase in `x` will increase the expected count by $e^.5 = 1.65$.\n\n$$\ny = Poisson(e^{0.5x})\n$$\n\n```{r}\nlibrary(tidyverse)\nlibrary(sjPlot)\nlibrary(ggeffects)\n\ntheme_set(theme_classic())\n\nrm(list = ls())\n\ndat <- tibble(\n  x = runif(1000, 0, 5),\n  y = rpois(1000, exp(0.5*x))\n)\n```\n\nIn the graph, we can see that the relationship between x and y is clearly non linear!\n\n```{r}\nggplot(dat, aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(se = FALSE)\n```\n\nLet's plot `log_y + 1` on `x`.\nAmazing!\nThe relationship is basically linear, which suggests that a 1-unit increase in `x` has some multiplicative effect on `y`.\n\n```{r}\nggplot(dat, aes(x = x, y = log(y + 1))) +\n  geom_point() +\n  geom_smooth(se = FALSE)\n```\n\n### Fitting the Regression Models\n\nLet's use both OLS and Poisson regression to fit the data.\nWe see a few things:\n\n1.  The Poisson model fits drastically better, both in terms of $R^2$ and that the coefficients are close to the data-generating values\n2.  The transformed OLS model understates the slope\n3.  Both models have (seemingly) similar interpretations: a 1-unit increase in `x` causes an $e^\\beta$ increase in `y`. How is this possible?\n\nSo what's going on?\n\nThe answer is that there is a very subtle difference between a transformed OLS regression and a Poisson regression.\nIn transformed OLS, we are modeling the mean of the log of Y, or $E(ln(y|x))$.\nIn Poisson, we're modeling the log of the mean of Y, or $ln(E(y|x))$.\nThese are not equivalent!\nIn essence, Poisson regression is a model for the arithmetic mean, whereas OLS is a model for the geometric mean.\nThis means that when we exponentiate the Poisson model, we can get predicted counts, but this is *not* true of the OLS model.\n\n```{r}\nm1 <- lm(log(y + 1) ~ x, dat)\nm2 <- glm(y ~ x, dat, family = poisson)\n\ntab_model(m1, m2,\n          p.style = \"stars\",\n          show.ci = FALSE,\n          show.se = TRUE,\n          digits = 3,\n          transform = NULL,\n          dv.labels = c(\"Log(Y+1)\", \"Poisson\"))\n```\n\n### More Intuition: An Example with Means\n\nLet's create a super simple data set, `s`.\n\n```{r}\ns <- c(1, 10, 100)\n```\n\nIt's clearly skewed.\nBut I can still take the mean.\nI could take the arithmetic mean, or the geometric mean.\nThese are clearly different quantities.\n\n```{r}\nmean(s) # arithmetic\nexp(mean(log((s)))) # geometric\n```\n\nThe idea of Poisson is to take the log of the mean and fit a linear model for that:\n\n```{r}\nlog_mean <- log(mean(s))\nlog_mean\n```\n\nThe idea of transformed OLS is to take the mean of the log and fit a linear model for that:\n\n```{r}\nmean_log <- mean(log(s))\nmean_log\n```\n\nWhen I exponentiate the log of the mean, I get back the original arithmetic mean.\nThis is what Poisson is doing:\n\n```{r}\nexp(log_mean)\n```\n\nWhen I exponentiate the mean of the log, I get back the original geometric mean.\nThis is what transformed OLS is doing:\n\n```{r}\nexp(mean_log)\n```\n\n### Further Reading\n\n<https://www.theanalysisfactor.com/the-difference-between-link-functions-and-data-transformations/>\n","srcMarkdownNoYaml":"\n\n## Dichotomous regression models (logistic regression)\n\nWhen predicting either successes and failures, or proportions, we can use a model with a binomial outcome.\nHere we'll focus on models where the data is represented as individual successes and failures.\nThe canonical model for these data is logistic regression, where\n\n$$logit(E[Y|X]) \\equiv \\log\\left(\\frac{P(Y=1|X)}{1-P(Y=1|X)}\\right) = \\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p$$ $$Y \\sim Binomial(1, E[Y|X])$$\n\nWe can rewrite this model as\n\n$$odds(Y) = \\frac{P(Y=1|X)}{1-P(Y=1|X)} = e^{\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p}$$\n\nor\n\n$$P(Y=1|X) = \\frac{e^{\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p}}{1 + e^{\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p}}$$\n\nWe can interpret $\\beta_0$ as follows: for observations which are 0 on all of the predictors, we estimate that the mean value of the outcome will be $\\frac{e^{\\beta_0}}{1 + e^{\\beta_0}}$.\nThat is, we estimate that the probability of the outcome being a 'success' (assuming 'success' is coded as a 1) will be $\\frac{e^{\\beta_0}}{1 + e^{\\beta_0}}$.\n\nWe can interpret $\\beta_1$ as follows: adjusting for the other predictors, a one-unit difference in $X_1$ predicts a $\\beta_1$ difference in the log-odds of the outcome being one, or a $(e^{\\beta_1}-1)\\times100\\%$ difference in the odds of the outcome.\nUnfortunately, the change in probability of a unit change depends on where the starting point is, so there is no easy way to interpret these coefficients in terms of direct probability.\nOne can calculate the estimated change for specific units, however, and look at the distribution of those changes.\n\nOther possible link functions include the probit (which uses a Normal CDF to link $\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p$ to $P(Y=1|X)$), or the complementary log-log (which allows $P(Y = 1|X)$ to be asymmetric in the predictors), among others.\n\n### How to fit a GLM\n\nWe can fit a logistic regression model by writing\n\nglm(Y $\\sim$ X, family = binomial(link = 'logit'))\n\nWe can fit a probit regression model by writing\n\nglm(Y $\\sim$ X, family = binomial(link = 'probit'))\n\nWe can fit a complementary log-log model by writing\n\nglm(Y $\\sim$ X, family = binomial(link = 'cloglog'))\n\nWe can allow a random slope and intercept by writing\n\nglmer(Y $\\sim$ 1 + X + (1 + X\\|grp), family = binomial(link = 'logit'))\n\n## Interpreting multilevel logistic regressions\n\nIn this section, we give some further discussion about logistic regression and interpretation.\nThis section supplements Packet 6.2 (logistic and longitudinal, or the toenail data lecture).\n\n\n```{r setup_toenail, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\noptions( digits=3 )\n\n# load packages \nlibrary(tidyverse)\nlibrary(arm)\nlibrary(ggsci) # for cool color pallets\nlibrary( foreign )\ntoes = read.dta( \"data/toenail.dta\" )\nhead( toes )\n\ntoes$Tx = factor( toes$treatment, levels=c(0,1),\n                            labels=c(\"Terbinafine\", \"Itraconazole\") )\n\n```\n\nWe first fit our toenail data using a random intercept model:\n\n```{r}\nM1 = glmer( outcome ~ Tx * month + (1|patient),\n            family=binomial,\n            data=toes )\n\n\ndisplay( M1 )\n```\n\nNow let's interpret.  We have three different ways of looking at these model results, log-odds (or logits), odds, or probabilities themselves.\n\n**log-odds:** The predicted values and coefficients are in the log-odds space for a logistic model. The coefficient of `month` means each month the log-odds goes down by 0.40.  The baseline intercept of -2.51 mean that a control patient at `month=0` has a log-odds of detachment of -2.51.\n\nUsing our model, if we wanted to know the chance of detachment for a median treated patient 3 months into the trial we could calculate:\n```{r}\nfes = fixef(M1)\nlog_odds = fes[[1]] + fes[[2]] + (fes[[3]] + fes[[4]])*3\nlog_odds\n```\n\nFor a patient who has a 1 SD above-average proclivity for detachment, we would add our standard deviation of 4.56:\n```{r}\nlog_odds + 4.56\n```\n\n**odds:** The _odds_ of something happening are the chance of happening divided by the chance of not happening, or $odds = p/(1-p)$.\nTo convert log-odds to odds we just exponentiate:\n\n```{r}\nORs = exp( fixef( M1 ) )\nORs\n```\n\nThe intercept is our base odds: the odds of detachment at `month=0` for a control patient.  The rest of the coefficients are odds multipliers, multiplying our baseline (starting) odds.  For example, each month a control patient's odds of detachment gets multiplied by 0.671.\n\nNote that exponentiation and logs play like this (for a control patient at 2 months, in this example)\n$$ odds = exp( -2.51 + 2 * -0.40 ) = exp( -2.51 ) \\cdot exp( 2 * -0.40 ) = exp( -2.51 ) \\cdot exp( -0.40 )^2 $$\nSee how $exp( -0.40 )$ is a multiplier on the baseline $exp( -2.51 )$?\n\nWe can look at the math to get a bit more here:\n\n$$\n\\begin{aligned}\nlogit( Pr( Y_{ij} = 1 ) ) &= \\log odds( Pr( Y_{ij} = 1 ) ) = \\gamma_{00} + \\gamma_{01} Z_j + \\gamma_{10} Time_{ij} + \\gamma_{11} Z_j Time_{ij} + u_j\n\\end{aligned}\n$$\n(logit means log odds)\n\nWe can rewrite this as\n$$\n\\begin{aligned}\nodds( Y_{ij} = 1 ) &= \\exp\\left[ \\gamma_{00} + \\gamma_{01} Z_j + \\gamma_{10} Time_{ij} + \\gamma_{11} Z_j Time_{ij} + u_j \\right] \\\\\n&= e^{\\gamma_{00}} + e^{\\gamma_{01} Z_j} + e^{\\gamma_{10} Time_{ij}} + e^{\\gamma_{11} Z_j Time_{ij}} + e^{u_j} \\\\\n&= e^{\\gamma_{00}} \\cdot e^{\\gamma_{01} Z_j} \\cdot \\left(e^{\\gamma_{10}}\\right)^{Time_{ij}} \\cdot \\left(e^{\\gamma_{11}}\\right)^{Z_j Time_{ij}} \\cdot e^{u_j} \n\\end{aligned}\n$$\nSee how all our additive covariates turn into multiplicative factors?  And time exponentiates our factors, so we keep multiplying by the factor for each extra month.\n\nFor our two 3 month, treated patients, we have the odds of detachment of\n\n```{r}\nexp( c( log_odds, log_odds + 4.56 ) )\n```\n\n**Probabilities:** Finally, we have probabilities, which we can calculate directly with `invlogit` in the `arm` package or `plogis` in the base package:\n```{r}\nplogis( c( log_odds, log_odds + 4.56 ) )\n```\nHere we have a 1% chance of detachment at baseline for our median patient, and 53% chance for our 1SD above average patient.\n\n\n### Some math formula for reference\nThe relevant formula are:\n\n$$\nodds = \\frac{ prob }{1 - prob}\n$$\n\ngiving (letting $\\eta$ denote our log odds)\n$$\nprob = \\frac{ odds }{ 1 + odds } = \\frac{ \\exp( \\eta ) }{ 1 + \\exp(\\eta)} = \\frac{1}{1 + \\exp(-\\eta) }\n$$\nThe second equality is a simple algebraic trick to write the probability as a function where the log-odds ($\\eta$) appears only once.\n\n### More on the random intercept\n\nThe random intercepts represent each patients overall proclivity to have a detachment. High values means that patient just has a higher odds of detachment, and low values means less.\n\nIf we exponentiate our Empirical Bayes estimated random intercepts, we get multiplicative factors of how each patient's odds are just shifted by some amount.  E.g.,\n```{r}\nREs = ranef( M1 )$patient$`(Intercept)`\nhead( REs )\nsummary( REs )\nquantile( exp(REs), c( 0.05, 0.95 ) )\n```\n\nThis means the odds of detachment for some patients (the 5% least likely to have detachment) is 30\\% of the baseline detachment.\nFor a 95th percentile patient, we have an odds multiplier of around 470---they are much, much more likely to have detachment at any moment in time.\nThis is why the curves for the patients in the main lecture are so different.\n\nTo recap: our model says the baseline median patient has a very low chance of detachment. For many patients it is even lower than that, but many other patients have very high random intercepts which makes their chance of detachment much, much higher.\n\n### Growth should have random slopes?\n\nWe can try to fit a random slope model, allowing for each patient's growth in their log-odds to be different.  This is a longitudinal linear growth model, with binary outcome:\n\n```{r}\nM2 = glmer( outcome ~ Tx * month + (1+month|patient),\n            family=binomial,\n            data=toes )\n\n\ndisplay( M2 )\nanova( M1, M2 )\n```\n\nThe random slope model is strongly preferred, and also has a larger estimated effect of treatment, although the standard errors have also grown considerably.\nWhat is likely happening is the autoregressive pattern in our outcomes (note how we tend to see 1s followed by 0s, with not a lot of back and forth) coupled with the limited information we have for each patient, makes it hard to nail down differences in individual student growth vs. differences in treatment and control average growth.\nThe random intercept model focuses on within-person change, but is an easier to estimate model.\nDue to randomization, it is also trustworthy--we do not have to worry much about the assumptions.\n\n\n\n## Poisson regression models\n\nPoisson regression is sometimes used to model count data.\nThe canonical form of a Poisson (log-linear) regression model is $$\\log(E[Y|X]) = \\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p$$ $$Y \\sim Poisson(E[Y|X])$$\n\nThe Poisson distribution has only one parameter, the mean, which is also the variance of the distribution.\nSo in estimating $E[Y|X]$, we are also estimating $Var(Y|X)$.\nThis is a potential drawback to the Poisson model, because there is no variance parameter to estimate, and so incorrect models can give wildly inaccurate standard errors (frequently unrealistically small).\nA better model is a quasi-Poisson model, for which the variance is proportional to the mean, but not necessarily equal to it.\nThe negative binomial regression model is also commonly used to address over-dispersed count data where the variance exceeds the mean.\n\nThe canonical link function for Poisson outcomes is the natural logarithm.\nWhen we use a log-link, we can write\n\n$$E[Y|X] = e^{\\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p}.$$\n\nWe can interpret $\\beta_0$ as follows: for observations which are 0 on all of the predictors, we estimate that the mean (expected) value of the outcome will be $e^{\\beta_0}$.\n\nWe can interpret $\\beta_1$ as follows: adjusting for the other predictors, a one-unit difference in $X_1$ predicts a $(e^{\\beta_1}-1)\\times100 \\%$ difference in the outcome.\n\nGenerally, when using a log-link, we assume that differences in the predictors are associated with multiplicative differences in the outcome.\n\nSome advantages to using an exponential link are\n\n1.  the model is mathematically more tractable and simpler to fit\n\n2.  the model parameters are easy to interpret\n\n3.  the mean of $Y$ is guaranteed to be positive for all values of $X$, which is required by the Poisson distribution\n\n### How to fit a poisson regression\n\nWe can fit a Poisson log-linear regression by writing\n\nglm(Y $\\sim$ X, family = poisson(link = 'log'))\n\nTo fit a quasi-Poisson model, write\n\nglm(Y $\\sim$ X, family = quasipoisson(link = 'log'))\n\nTo fit a negative binomial regression model, write (after loading the `MASS` library)\n\nglm.nb(Y $\\sim$ X, link='log')\n\nTo fit a Poisson regression with an identity link (where coefficients are interpreted as expected differences in the outcome associated with unit differences in the predictor), write\n\nglm(Y $\\sim$ X, family = poisson(link = 'identity'))\n\nTo fit a Poisson regression with a square root link, which is vaguely like a compromise between an identity link and a log link (and is harder to interpret than either), write\n\nglm(Y $\\sim$ X, family = poisson(link = 'sqrt'))\n\nTo fit a Poisson log-linear model with a random intercept and slope, write\n\nglmer(Y $\\sim$ X + (X\\|grp), family = poisson(link = 'log'))\n\n## GLMs vs. Transformations\n\n```{r, echo = FALSE}\nknitr::opts_chunk$set(echo = TRUE)\nknitr::opts_chunk$set(warning = FALSE)\nknitr::opts_chunk$set(message = FALSE)\n\nset.seed(2023)\n```\n\nThose of you coming from S40 and S52 may recall that when we have non-linear relationships between $X$ and $Y$, we can apply a transformation, such as taking the log, to linearize the relationship.\nIn the words of Jimmy Kim, \"with transformations, we use the *machinery of linear regression to model non-linear relationships.*\" If that's the case, then what is Poisson regression about, which deals with log counts?\nThis is a topic that confused me for many years so hopefully I can clear it up here.\n\n### Making and Graphing the Data\n\nLet's start by making some fake data.\nHere's the data-generating function, which has the relationship that a 1-unit increase in `x` will increase the expected count by $e^.5 = 1.65$.\n\n$$\ny = Poisson(e^{0.5x})\n$$\n\n```{r}\nlibrary(tidyverse)\nlibrary(sjPlot)\nlibrary(ggeffects)\n\ntheme_set(theme_classic())\n\nrm(list = ls())\n\ndat <- tibble(\n  x = runif(1000, 0, 5),\n  y = rpois(1000, exp(0.5*x))\n)\n```\n\nIn the graph, we can see that the relationship between x and y is clearly non linear!\n\n```{r}\nggplot(dat, aes(x = x, y = y)) +\n  geom_point() +\n  geom_smooth(se = FALSE)\n```\n\nLet's plot `log_y + 1` on `x`.\nAmazing!\nThe relationship is basically linear, which suggests that a 1-unit increase in `x` has some multiplicative effect on `y`.\n\n```{r}\nggplot(dat, aes(x = x, y = log(y + 1))) +\n  geom_point() +\n  geom_smooth(se = FALSE)\n```\n\n### Fitting the Regression Models\n\nLet's use both OLS and Poisson regression to fit the data.\nWe see a few things:\n\n1.  The Poisson model fits drastically better, both in terms of $R^2$ and that the coefficients are close to the data-generating values\n2.  The transformed OLS model understates the slope\n3.  Both models have (seemingly) similar interpretations: a 1-unit increase in `x` causes an $e^\\beta$ increase in `y`. How is this possible?\n\nSo what's going on?\n\nThe answer is that there is a very subtle difference between a transformed OLS regression and a Poisson regression.\nIn transformed OLS, we are modeling the mean of the log of Y, or $E(ln(y|x))$.\nIn Poisson, we're modeling the log of the mean of Y, or $ln(E(y|x))$.\nThese are not equivalent!\nIn essence, Poisson regression is a model for the arithmetic mean, whereas OLS is a model for the geometric mean.\nThis means that when we exponentiate the Poisson model, we can get predicted counts, but this is *not* true of the OLS model.\n\n```{r}\nm1 <- lm(log(y + 1) ~ x, dat)\nm2 <- glm(y ~ x, dat, family = poisson)\n\ntab_model(m1, m2,\n          p.style = \"stars\",\n          show.ci = FALSE,\n          show.se = TRUE,\n          digits = 3,\n          transform = NULL,\n          dv.labels = c(\"Log(Y+1)\", \"Poisson\"))\n```\n\n### More Intuition: An Example with Means\n\nLet's create a super simple data set, `s`.\n\n```{r}\ns <- c(1, 10, 100)\n```\n\nIt's clearly skewed.\nBut I can still take the mean.\nI could take the arithmetic mean, or the geometric mean.\nThese are clearly different quantities.\n\n```{r}\nmean(s) # arithmetic\nexp(mean(log((s)))) # geometric\n```\n\nThe idea of Poisson is to take the log of the mean and fit a linear model for that:\n\n```{r}\nlog_mean <- log(mean(s))\nlog_mean\n```\n\nThe idea of transformed OLS is to take the mean of the log and fit a linear model for that:\n\n```{r}\nmean_log <- mean(log(s))\nmean_log\n```\n\nWhen I exponentiate the log of the mean, I get back the original arithmetic mean.\nThis is what Poisson is doing:\n\n```{r}\nexp(log_mean)\n```\n\nWhen I exponentiate the mean of the log, I get back the original geometric mean.\nThis is what transformed OLS is doing:\n\n```{r}\nexp(mean_log)\n```\n\n### Further Reading\n\n<https://www.theanalysisfactor.com/the-difference-between-link-functions-and-data-transformations/>\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"pygments","output-file":"interpreting_glms.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.353","bibliography":["references.bib"],"output_dir":"docs","editor":{"markdown":{"wrap":"sentence"}},"theme":"cosmo","code-copy":true,"title":"Interpreting GLMs","author":"Luke Miratrix, Joe McIntyre, and Josh Gilbert","editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","highlight-style":"pygments","output-file":"interpreting_glms.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"block-headings":true,"bibliography":["references.bib"],"output_dir":"docs","editor":{"markdown":{"wrap":"sentence"}},"documentclass":"scrreprt","title":"Interpreting GLMs","author":"Luke Miratrix, Joe McIntyre, and Josh Gilbert","editor_options":{"chunk_output_type":"console"}},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}