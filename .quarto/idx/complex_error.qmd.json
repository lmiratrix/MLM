{"title":"An overview of complex error structures","markdown":{"yaml":{"title":"An overview of complex error structures","author":"Luke Miratrix","editor":{"markdown":{"wrap":"sentence"}}},"headingText":"National Youth Survey running example","containsRefs":false,"markdown":"\n\n```{r loadlibs, echo=FALSE, message=FALSE, warning=FALSE}\nlibrary( tidyverse )\nlibrary( nlme )\nlibrary(foreign)\nlibrary( texreg )\n```\n\nIn Unit 7 we talked about how we can model residuals around an overall population model using different specified structures on the correlation matrices for the students.\nThis handout extends those topics, using the Raudenbush and Bryk Chapter 6 example on National Youth Survey data on deviant attitudes.\nWe're going to do a few things:\n\n1.  Reproduce the models in the book, showing you how to get them in R, using the commands `lme` and `gls`.\n\n2.  Discuss the relationship between `lme` and `gls`, and what it actually means when you include a gls-like \"correlation\" argument when calling `lme`.\n    To make a long story short: `gls` is cleaner and more principled from a mathematical point of view, but in practice you will probably prefer hybrid calls using `lme`.\n\n3.  Give two ways this stuff can actually be useful -- heteroskedasticity and AR\\[1\\] -- and show how to fit realistic models with either and both.\n    We'll interpret and check significance of parameters as appropriate.\n\n\nOur running example is the data as described in Raudenbush and Bryk, and we follow the discussion on page 190.\nThese data are the first cohort of the National Youth Survey (NYS).\nThis data comes from a survey in which the same students were asked yearly about their acceptance of 9 \"deviant\"\\^\\[Wow, has the way we talk about things changed over the years.\\] behaviors (such as smoking marijuana, stealing, etc.).\nThe study began in 1976, and followed two cohorts of children, starting at ages 11 and 14 respectively.\nWe will analyze the first 5 years of data.\n\nAt each time point, we have measures of:\n\n-   ATTIT, the attitude towards deviance, with higher numbers implying higher tolerance for deviant behaviors.\n-   EXPO, the \"exposure\", based on asking the children how many friends they had who had engaged in each of the behaviors. Both of these numbers have been transformed to a logarithmic scale to reduce skew.\n\nFor each student, we also have:\n\n-   Gender (binary)\n-   Minority status (binary)\n-   Family income, in units of \\$10K.\n\nOne reasonable research question would to describe how the cohort evolved.\nFor this question, the parameters of interest would be the average attitudes at each age.\nStandard deviations and intrasubject correlations are, as is often but not always the case, simply nuisance parameters.\nStill, the better we can do at realistically modeling these nuisance parameters, the more precision we will have for the measures of interest, and the power we will have to test relevant hypotheses.\n\n### Getting the data ready\n\nWe'll focus on the first cohort, from ages 11-15.\nFirst, let's read the data.\n\nNote that this table is in \"wide format\".\nThat is, there is only one row for each student, with all the different observations for that student in different columns of that one row.\n\n```{r,  message=FALSE}\nnyswide = read.csv(\"data/nyswide.csv\")\nhead(nyswide)\n```\n\nFor our purposes, we want it in \"long format.\" The `pivot_longer()` command does this for us:\n\n```{r}\n\nnys1.na <- nyswide %>%\n  pivot_longer(\n    cols = c(ATTIT.11:ATTIT.15, EXPO.11:EXPO.15),\n    names_to = c(\".value\", \"AGE\"),\n    names_sep = \"\\\\.\",\n    values_to = c(\"ATTIT\", \"EXPO\")\n  )\n\n## Drop missing ATTIT values\nnys1 = nys1.na[!is.na(nys1.na$ATTIT),] \n\n## Make age a number\nnys1$AGE = as.numeric(nys1$AGE)\nhead( nys1 )\n```\n\nWe also need to make our age a factor so it is treated appropriately as an indicator of what wave the data was collected in.\n\n```{r}\nnys1$agefac = as.factor(nys1$AGE) \n```\n\nJust to get a sense of the data, let's plot each age as a boxplot\n\n```{r, fig.height=3}\nggplot( nys1, aes( agefac, ATTIT ) ) +\n  geom_boxplot() +\n  theme_minimal()\n```\n\nNote some features of the data: First, we see that ATTIT goes up over time.\nSecond, we see the variation of points also goes up over time.\nThis is heteroskedasticity.\n\nIf we plot individual lines we have\n\n```{r, fig.height = 3}\nnys1$AGEjit = jitter(nys1$AGE)\nnys1$ATTITjit = jitter(nys1$ATTIT, amount=0.05)\nggplot( filter( nys1, complete.cases(nys1) ), aes( AGEjit, ATTITjit, group=ID ) ) +\n  geom_line( alpha=0.2 ) +\n  theme_minimal()\n```\n\nNote how we have correlation of residuals, in that some students are systematically low and some are systematically higher (although there is a lot of bouncing around).\n\n## Representation of error structure\n\nIn our data, we have 5 observations $y_{it}$ for each subject i at 5 fixed times $t=1$ through $t=5$.\nWithin each person $i$ (where person is our Level-2 group, and time is our Level-1), we can write\n\n$$\\begin{pmatrix}y_{i1}\\\\\ny_{i2}\\\\\ny_{i3}\\\\\ny_{i4}\\\\\ny_{i5}\n\\end{pmatrix} = \\left(\\begin{array}{c}\n\\mu_{1i}\\\\\n\\mu_{2i}\\\\\n\\mu_{3i}\\\\\n\\mu_{4i}\\\\\n\\mu_{5i}\n\\end{array}\\right) + \\left(\\begin{array}{c}\n\\epsilon_{1i}\\\\\n\\epsilon_{2i}\\\\\n\\epsilon_{3i}\\\\\n\\epsilon_{4i}\\\\\n\\epsilon_{5i}\n\\end{array}\\right)$$ where our set of 5 residuals are the random part, distributed as\n\n$$\\left(\\begin{array}{c}\n\\epsilon_{1i}\\\\\n\\epsilon_{2i}\\\\\n\\epsilon_{3i}\\\\\n\\epsilon_{4i}\\\\\n\\epsilon_{5i}\n\\end{array}\\right)\\sim N\\left[\\left(\\begin{array}{c}\n0\\\\\n0\\\\\n0\\\\\n0\\\\\n0\n\\end{array}\\right),\\left(\\begin{array}{ccccc}\n\\tau_{11} & \\tau_{12} & \\tau_{13} & \\tau_{14} & \\tau_{15}\\\\\n. & \\tau_{22} & \\tau_{23}& \\tau_{24} & \\tau_{25}\\\\\n. & . & \\tau_{33}& \\tau_{34} & \\tau_{35}\\\\\n. & . & . & \\tau_{44} & \\tau_{45}\\\\\n. & . & . & . & \\tau_{55}\n\\end{array}\\right)\\right] = N( 0, \\Sigma ). $$\n\nThe key part is the correlation between the residuals at different times.\nWe call our entire covariance matrix $\\Sigma$.\nThis matrix describes how the residuals within a single individual (with 5 time points of observation) are correlated.\n\nOur regression model gives us the mean vector for any given student (e.g., $(\\mu_{1i}, \\ldots, \\mu_{5i})$ would be $X_i'\\beta$, where $X_i$ is a $5 \\times p$ matrix of covariates for student $i$, and $\\beta$ is our fixed effect parameter vector.\n$X_i$ would have one row per time point and time would be one of the columns, to give our predictions for our 5 time points.\n\nOur error structure model gives us the distribution of the $(\\epsilon_{1i}, \\ldots, \\epsilon_{5i})$ for student $i$.\nDifferent ideas about the data generating process lead to different correlation structures here.\nWe saw a couple of those in class.\n\n## Reproducing R&B's Chapter 6 examples\n\nThe above provides a framework for thinking about grouped data: each group (i.e., student) is a small world with a linear prediction line and a collection of residuals around that line.\nUnder this view, we specify a specific structure on how the residuals relate to each other.\n(E.g., for classic OLS we would have i.i.d. normally distributed residuals, represented as our $\\Sigma$ being a diagonal matrix with $\\sigma^2$ along the diagonal and 0s everywhere else).\nIn R, once we determine what structure we want, we can fit models based on parameterized correlation matrices using the `lme` command from the `nlme` package (You may need to first call `install.packages(\"nlme\")` to get this package), or the `gls` package.\n\nLet's load the `nlme` package now:\n\n```{r}\nlibrary(nlme)\n```\n\nRecall that all of these models include a linear term on age and an intercept (so two fixed effects and no covariate adjustment).\n\n### Compound symmetry (random intercept model)\n\nA \"compound symmetry\" residual covariance structure (all diagonal elements equal, all off-diagonal elements equal) is actually equivalent to a random intercepts model.\nThus, there are 2 ways to get this same model:\n\n```{r, message=FALSE}\nmodelRE = lme(ATTIT ~ AGE, \n              data=nys1,\n              random=~1|ID )\n```\n\nand\n\n```{r}\nmodelCompSymm = gls(ATTIT ~ AGE,\n                    data=nys1,\n                    correlation=corCompSymm(form=~AGE|ID) )\n```\n\nFor reference, using the `lme4` package we again have (we use `lme4::` in front of `lmer` to avoid loading the `lme4` package fully):\n\n```{r}\nmodelRE.lme4 = lme4::lmer(ATTIT ~ AGE + (1|ID), data=nys1 )\n```\n\nWe can get the correlation matrix for individuals #3:\n\n```{r}\nmyVarCovs = getVarCov(modelRE,type=\"marginal\", individual=3)\nmyVarCovs\n```\n\nIf we look at an individual #5, who only has 4 timepoints we get a $4 \\times 4$ matrix:\n\n```{r}\ngetVarCov(modelRE,type=\"marginal\", individual=5)\n```\n\nOther individuals are the same, if they have the same number of time points, given our model. So in this model, we are saying the residuals of a student have the same distribution as any another student with the same number of time points.\n\n#### Comparing the models\n\nThese are two very different ways of specifying the same thing, and the parameter estimates we get out are also not the same.\nCompare the two summary printouts:\n\n```{r,  message=FALSE}\nsummary(modelRE)\n```\n\nand\n\n```{r}\nsummary(modelCompSymm)\n```\n\nThese do not look very similar, do they?\nBut wait:\n\n```{r,  message=FALSE}\nlogLik(modelCompSymm)\nlogLik(modelRE)\nlogLik(modelRE.lme4)\nAIC( modelCompSymm )\nAIC( modelRE )\nAIC( modelRE.lme4 )\n```\n\nIn fact, they have the same AIC, etc., because they are equivalent models.\n\nThe lesson is that it's actually quite hard to see the correspondence between a familiar random-effects model and an equivalent model expressed in terms of a covariance matrix.\nSure, we could do a bunch of math and see that in the end they are the same; but that math is already daunting here, and this is the simplest possible situation.\nThe fitted parameters of a covariance-based model are just really hard to interpret in familiar terms.\n\n### Autoregressive error structure (AR\\[1\\])\n\nOne typical structure used for longitudinal data is the \"autoregressive\" structure.\nThe idea is threefold:\n\n1.  $Var(u_{it}) = \\sigma^2$ - that is, overall marginal variance is staying constant.\n2.  $Cor(u_{it},u_{i(t-1)}) = \\rho$ - that is, residuals are a little bit \"sticky\" over time so residuals from nearby time points tend to be similar.\n3.  $E(u_{it}|u_{i(t-1)},u_{i(t-2)}) = E(u_{it}|u_{i(t-1)})$ - that is, the only way the two-periods-ago measurement tells you anything about the current one is through the intermediate one, with no longer-term effects or \"momentum\".\n\nIn this case, the unconditional two-step correlation $Cor(u_{it},u_{i(t-2)})$ is also easy to calculate.\nIntuitively, we can say that a portion $\\rho$ of the residual \"is the same\" after each step, so that after two steps the portion that \"is the same\" is $\\rho$ of $\\rho$, or $\\rho^2$.\nClearly, then, after three steps the correlation will be $\\rho^3$, and so on.\nIn other words, the part that \"is the same\" is decaying in an exponential pattern.\nIndeed, one could show that (3.), above, requires the correlated part to decay in a memoryless pattern, leaving the Exponential and Hypergeometric distributions (which both show exponential decay) among the few options.\n\nThus, the within-subject correlation structure implied by these postulates is:\n\n$$\\left(\\begin{array}{c}\nu_{1i}\\\\\nu_{2i}\\\\\nu_{3i}\\\\\nu_{4i}\\\\\n...\\\\\nu_{ni}\n\\end{array}\\right)\\sim N\\left[\\left(\\begin{array}{c}\n0\\\\\n0\\\\\n0\\\\\n0\\\\\n...\\\\\n0\n\\end{array}\\right),\\sigma^2\\left(\\begin{array}{cccccc}\n1 & \\rho  & \\rho^2 & \\rho^3 & ... & \\rho^{n-1}\\\\\n. & 1 & \\rho & \\rho^2 & ... & \\rho^{n-2}\\\\\n. & . & 1& \\rho  & ... & \\rho^{n-3}\\\\\n. & . & . & 1 & ... & \\rho^{n-4} \\\\\n... & ... & ... & ... & ... & ... \\\\\n. & . & . & . & ... & 1\n\\end{array}\\right)\\right]\\\\$$\n\nAs you can see, this structure takes advantage of the temporal nature of the data sequence to parameterize the covariance matrix with only two underlying parameters: $\\sigma$ and $\\rho$.\nBy contrast, a random intercept model needs the overall $\\sigma$ and variance of intercepts $\\tau$---also two parameters!\nSame complexity, different structure.\n\n#### Fitting the AR\\[1\\] covariance structure\n\nTo get a true AR\\[1\\] residual covariance structure, we need to leave the world of hierarchical models, and thus use the command `gls`.\nThis is just what we've discussed in class.\nHowever, later on in this document, we'll see how to add AR\\[1\\] structure on top of a hierarchical model, which is messier from a theoretical point of view, but often more useful and interpretable in practice.\n\n```{r, message=FALSE}\nmodelAR1 = gls(ATTIT ~ AGE, \n                    data=nys1,\n                    correlation=corAR1(form=~AGE|ID) )\n\nsummary(modelAR1)\n```\n\nYou have to dig around in the large amount of output to find the parameter estimates, but they are there.\n`Phi1` is the auto-correlation parameter.\nAnd the covariance of residuals:\n\n```{r}\ngetVarCov(modelAR1,type=\"marginal\")\nsummary(modelAR1)$AIC\n```\n\nNote that the AIC of our AR\\[1\\] model is lower by about 45 than the random intercept model; clearly far superior because it is getting nearby residuals being more correlated, while the random intercept model does not do this.\nAlso see the banding structure of the residual correlation matrix.\n\n### Random slopes\n\nIn theory, a random slopes model could be done with `gls` as well as with `lme` by building the final residual matrices as a function of the random slope parameters; in practice, it's much more practical just to do it as a hierarchical model with `lme`:\n\n```{r, message=FALSE}\nmodelRS = lme(ATTIT ~ 1 + AGE, \n              data=nys1,\n              random=~AGE|ID )\n```\n\nWe have separated our fixed and random components with `lme()`.\nWe first include a formula with only fixed effects, and then give a right-side-only formula with terms similar to what you'd put in parentheses with `lmer()` for the random effects.\n\nOur results:\n\n```{r}\nsummary(modelRS)\ngetVarCov(modelRS,type=\"marginal\", individual=3)\nsummary(modelRS)$AIC\n```\n\nThe first thing to note is the residual covariance matrix comes from the structure of the random intercept and random slope.\nIf you squint hard enough at it, you can begin to see the linear structures in its diagonal and off-diagonal elements.\nIf you graphed it, those structures would jump out more clearly.\nBut in practice, it's much easier to think of things in terms of the hierarchical model, not in terms of linear structures in a covariance matrix.\n\nNote also that the AIC has dropped by another 60 points or so; we're continuing to improve the model.\n\nAlso note that this is just using a different package to fit the exact same model we would fit using `lmer`; so far we haven't taken advantage of the `lme` command's additional flexibility.\n\n\n### Random slopes with heteroskedasticity\n\nRelaxing the homoskedasticity assumption in the random slopes model leaves us a bit in between worlds.\nWe're not fully into the world of GLS, because there are still random effects; but we're not fully in the world of hierarchical models because there is structure in the residuals within groups.\nWe'll talk more about this compromise below; for now, let's just do it.\n\n```{r, message=FALSE}\nmodelRSH = lme(ATTIT ~ AGE, \n              data=nys1,\n              random=~AGE|ID,\n              weights=varIdent(form=~1|agefac) )\n```\n\nThe key line is the `varIdent` line: we are saying each age factor level gets its own weight (rescaling) of the residuals---this is heteroskedasticity.\nIn particular, the above says our residual variance will be weighted by a weight for each age factor, so each age level effectively gets its own variance.\nThis is where these models start to get a bit exciting---we have random slopes, and then heteroskedastic residuals (homoskedastic for any given age level), all together.\nOur fit model:\n\n```{r}\nsummary(modelRSH)\n```\n\nNote how we have 5 parameter estimates for the residuals, listed under `agefac`.\nIt appears as if we have more variation in age 13 than other ages.\nAge 11, the baseline, is 1.0; it is our reference scaling.\nThese numbers are all scaling the overall residual variance parameter $\\sigma^2$ of $0.1405^2$.\n\nFor looking at the covariance structure of the residuals, we use `getVarCov()` again:\n\n```{r}\nmyVarCov = getVarCov(modelRSH,type=\"marginal\", individual=3)\nmyVarCov\n```\n\nWe get lists of matrices back from our call.\nWe can convert any one to a correlation matrix:\n\n```{r}\ncov2cor(myVarCov[[1]])\n```\n\nNo amount of squinting will show the structure in the original covariance matrix.\nBut when you convert to a correlation matrix, you can again squint and begin to see the linear structures in its diagonal and off-diagonal elements.\nThe same comment as above still applies: in practice, it's much easier to think of things in terms of the hierarchical model, and only read the diagonals of the covariance matrix.\n\nWe can also get our AIC:\n\n```{r}\nsummary(modelRSH)$AIC\n```\n\nThe AIC has dropped by only another 2.5 points or so; that corresponds to the idea that if one of these two models were exactly true, the odds are about $e^{2.5/2}\\cong 3.5$ in favor of the more complex model.\nAside from the fact that that premise is silly -- we are pretty sure that neither of these models is the exact truth; and in that case, something like BIC would probably be better than AIC -- those odds are also pretty weak; the simpler model is probably better here.\n\nHere's the reported BICs, by the way: `r summary(modelRS)$BIC` for the homoskedastic one, and `r summary(modelRSH)$BIC` for the heteroskedastic.\nAs we expected, the simpler model wins that fight.\n(Though what $N$ to use for BIC is sometimes not obvious with hierarchical models, so you can't trust those numbers too much; see the unit on AIC and BIC and model building.)\n\n### Fully unrestricted model\n\nOK, let's go whole hog, and fit the unrestricted model.\nAgain, this means leaving the world of hierarchical models and using gls.\n\n```{r unrestricted, message=FALSE}\n\nmodelUnrestricted = gls(ATTIT ~ AGE, \n               data=nys1,\n               correlation=corSymm(form=~1|ID),\n               weights=varIdent(form=~1|agefac) )\n\n\nsummary(modelUnrestricted)\n```\n\nAnd our residual structure:\n\n```{r}\nmyvc = getVarCov(modelUnrestricted,type=\"marginal\", individual=3)\nmyvc\n```\n\nAnd AIC:\n\n```{r}\nAIC( modelUnrestricted )\n```\n\nThis unrestricted covariance and correlation matrices have the same structures discussed in the book and in class.\nThe AIC has improved by another 6 or 7 points; that's marginally \"significant\", but in practice probably not substantial enough to make up for the massive loss of interpretability.\nThe lesson we should take from that is that there's not a whole lot of room for improvement just by tinkering with the residual covariance structure; if we want a much better model, we would have to add new fixed or random effects; perhaps other covariates or perhaps a quadratic term in time.\n\n## Having both AR\\[1\\] and Random Slopes\n\nLet's look at an AR1 residual structure along with some covariates in our main model.\nThe following has AR\\[1\\] and *also* a random intercept and slope:\n\n```{r,  message=FALSE}\nnys1$AGE11 = nys1$AGE - 11\nctrl <- lmeControl(opt='optim');\nmodel1 = lme(fixed=ATTIT ~ AGE11 + EXPO + FEMALE + MINORITY + log(INCOME + 1), \n              data=nys1,\n              random=~1 + AGE11|ID,\n              correlation=corAR1(),\n             control = ctrl )\n\nsummary(model1)\n```\n\nIn order to get this model to converge, we had to use the `lmeControl` command above; without it, the model doesn't converge due to not reaching a max in the given number of iterations.  The `lmeControl` with `optim` apparently turns up the juice so it converges without complaint.\n\nLet's compare our fit model to the same model without AR1 correlation\n\n```{r,  message=FALSE}\nmodel1simple = lme(fixed=ATTIT ~ AGE11 + EXPO + FEMALE + MINORITY + log(INCOME + 1), \n             data=nys1,\n             random=~1 + AGE11|ID )\nscreenreg( list( AR=model1, noAR=model1simple ) )\n```\n\nThe AR1 model has a notably lower AIC and thus is significantly better:\n\n\n```{r}\nanova( model1simple, model1 )\n```\n\nAutoregression involves only a single extra parameter--the autoregressive correlation coefficient.\n\nOur hybrid model is actually kind of mixed up, conceptually.\nWe allowed a random slope on age, and also an autoregressive component by age.\nThus, we effectively allowed the covariance matrix to vary in two different ways, at two different levels of our modeling.\n\nIn fact, as we've seen in class, any random effects, whether they be on slope or intercept, are actually equivalent to certain ways of varying the variance-covariance matrix of the residuals within each group.\nFor instance, random intercepts are equivalent to compound symmetry.\nThus, by including both random intercepts and AR1 correlation in the above model, we've effectively fit a model that allows any covariance matrix that can be expressed as a sum of a random slope covariance matrix (with 2 parameters plus a scaling factor) and an AR1 covariance matrix (with 1 parameter plus a scaling factor).\nThat makes 5 degrees of freedom total for our covariance matrix.\nThis is many fewer than the 15 for a fully unconstrained matrix, for comparison.\n\nConceptually this model is nice: people have linear growth trends, but vary around those growth trends in an autoregressive way.\n\n\n## The Kitchen sink: building complex models\n\nWhich brings us to the next point: how do you actually use this stuff in practice?\nIdeally, you'd like both the interpretability (and robustness against MAR missingness) of hierarchical models, along with the ability to add additional residual structure such as AR\\[1\\] and/or heteroskedastic residuals.\nThe good news is, you can get both.\nThe bad news is, there's a bit of a potential for bias due to overfitting.\n\nFor instance, imagine you use both random effects and AR\\[1\\].\nSay that for a given subject you have 5 time points, and all of them are above the values you would have predicted based on fixed effects alone.\nThat might be explained by an above-average random effect, or by a set of correlated residuals that all came in high.\nWhichever one of these is the \"true\" explanation, the MLE will tend to parcel it out between the two.\nThis can lead to downward bias in variance and/or correlation parameter estimates, especially with small numbers of observations per subject--the variation gets pushed into just assuming the residuals are correlated due to the auto-regressive structure.\n\nStill, as long as your focus is on location parameters such as true means or slopes, having hybrid models can be a good way to proceed.\nLet's explore this by first fitting a \"kitchen sink\" model for this data, in which we use all available covariates; and seeing how adding heteroskedasticity, AR\\[1\\] structure, or both changes it (or doesn't).\n\nWhat do we want in this \"kitchen sink\" model?\nLet's first fit a very simple random intercept model with fixed effects for gender, minority status, \"exposure\", and log(income), to see which of these covariates to focus on.\nWe use the `lmerTest` package to get some early $p$-values for these fixed effects.\n\n```{r, message=FALSE}\nmodelKS0 = lmerTest::lmer(ATTIT ~ FEMALE + MINORITY + log(INCOME + 1) + EXPO + (1|ID), data=nys1)\nsummary(modelKS0, correlation=FALSE)\n```\n\n(The `correlation=FALSE` shortens the printout.)\n\nApparently, MINORITY and EXPO are the covariates with significant effects; minority status is correlated with a lower tolerance for deviance, while \"deviant\" friends are of course correlated positively with tolerance of deviance.\nLet's build a few hierarchical models including these in various specifications (can you identify what models are what? Some of these models are not necessarily good choices).\nWe first center our age so we have meaningful intercepts.\n\n```{r kitchenSink, message=FALSE}\nnys1$age13 = nys1$AGE - 13\n\nmodelKS1 = lme(ATTIT ~ MINORITY + age13,\n              data=nys1,\n              random=~age13 + EXPO|ID )\n\nmodelKS2 = lme(ATTIT ~ MINORITY + age13, \n              data=nys1,\n              random=~age13 + EXPO|ID )\n\nmodelKS3 = lme(ATTIT ~ MINORITY + age13, \n              data=nys1,\n              random=~EXPO|ID )\n\nmodelKS4 = lme(ATTIT ~ MINORITY + age13 + EXPO, \n              data=nys1,\n              random=~1|ID )\n```\n\nAnd now we examine them:\n\n```{r}\nlibrary( texreg )\nscreenreg( list( modelKS1, modelKS2, modelKS3, modelKS4 ))\n```\n\nOK, Number 4 seems like a pretty good model.\nLet's see how much it improves when we add AR\\[1\\]:\n\n```{r, message=FALSE}\nmodelKS5 = lme(ATTIT ~ MINORITY + age13 + EXPO, \n              data=nys1,\n              random=~1|ID,\n              correlation=corAR1(form=~AGE|ID) )\nAIC( modelKS5 )\nfixef( modelKS4 )\nfixef( modelKS5 )\n```\n\nNote that the estimates for all the effects are essentially unchanged.\nHowever, the AIC is almost 40 points better.\nAlso, because the model has done a better job explaining residual variance, the $p$-value for the coefficient on MINORITY has dropped from 0.032 to 0.029, as we can see on the summary display below.\nThis is not a large drop, but a noticeable one:\n\n```{r}\nsummary( modelKS5 )\n```\n\nIs any of this drop in the $p$-value due to overfitting?\nGiven the size of the change in AIC, it seems doubtful that that's a significant factor.\n\nLet's try including heteroskedasticity, without AR\\[1\\]:\n\n```{r, message=FALSE}\nmodelKS6 = lme(ATTIT ~ MINORITY + age13 + EXPO, \n              data=nys1,\n              random=~1|ID,\n              weights=varIdent(form=~1|agefac) )\nAIC( modelKS6 )\n```\n\nThis did not improve AIC in this case, so we can avoid looking at this model further.\n\nFor completeness, let's look at a model with both AR(1) and heteroskedasticity:\n\n```{r, message=FALSE}\nmodelKS7 = lme(ATTIT ~ MINORITY + age13 + EXPO, \n              data=nys1,\n              random=~1|ID,\n              correlation=corAR1(form=~AGE|ID),\n              weights=varIdent(form=~1|agefac) )\nAIC( modelKS7 )\n```\n\nAgain, no improvement.\nSo we settle with our AR\\[1\\] model with a random intercept to get overall level of a student.\n\n\n","srcMarkdownNoYaml":"\n\n```{r loadlibs, echo=FALSE, message=FALSE, warning=FALSE}\nlibrary( tidyverse )\nlibrary( nlme )\nlibrary(foreign)\nlibrary( texreg )\n```\n\nIn Unit 7 we talked about how we can model residuals around an overall population model using different specified structures on the correlation matrices for the students.\nThis handout extends those topics, using the Raudenbush and Bryk Chapter 6 example on National Youth Survey data on deviant attitudes.\nWe're going to do a few things:\n\n1.  Reproduce the models in the book, showing you how to get them in R, using the commands `lme` and `gls`.\n\n2.  Discuss the relationship between `lme` and `gls`, and what it actually means when you include a gls-like \"correlation\" argument when calling `lme`.\n    To make a long story short: `gls` is cleaner and more principled from a mathematical point of view, but in practice you will probably prefer hybrid calls using `lme`.\n\n3.  Give two ways this stuff can actually be useful -- heteroskedasticity and AR\\[1\\] -- and show how to fit realistic models with either and both.\n    We'll interpret and check significance of parameters as appropriate.\n\n## National Youth Survey running example\n\nOur running example is the data as described in Raudenbush and Bryk, and we follow the discussion on page 190.\nThese data are the first cohort of the National Youth Survey (NYS).\nThis data comes from a survey in which the same students were asked yearly about their acceptance of 9 \"deviant\"\\^\\[Wow, has the way we talk about things changed over the years.\\] behaviors (such as smoking marijuana, stealing, etc.).\nThe study began in 1976, and followed two cohorts of children, starting at ages 11 and 14 respectively.\nWe will analyze the first 5 years of data.\n\nAt each time point, we have measures of:\n\n-   ATTIT, the attitude towards deviance, with higher numbers implying higher tolerance for deviant behaviors.\n-   EXPO, the \"exposure\", based on asking the children how many friends they had who had engaged in each of the behaviors. Both of these numbers have been transformed to a logarithmic scale to reduce skew.\n\nFor each student, we also have:\n\n-   Gender (binary)\n-   Minority status (binary)\n-   Family income, in units of \\$10K.\n\nOne reasonable research question would to describe how the cohort evolved.\nFor this question, the parameters of interest would be the average attitudes at each age.\nStandard deviations and intrasubject correlations are, as is often but not always the case, simply nuisance parameters.\nStill, the better we can do at realistically modeling these nuisance parameters, the more precision we will have for the measures of interest, and the power we will have to test relevant hypotheses.\n\n### Getting the data ready\n\nWe'll focus on the first cohort, from ages 11-15.\nFirst, let's read the data.\n\nNote that this table is in \"wide format\".\nThat is, there is only one row for each student, with all the different observations for that student in different columns of that one row.\n\n```{r,  message=FALSE}\nnyswide = read.csv(\"data/nyswide.csv\")\nhead(nyswide)\n```\n\nFor our purposes, we want it in \"long format.\" The `pivot_longer()` command does this for us:\n\n```{r}\n\nnys1.na <- nyswide %>%\n  pivot_longer(\n    cols = c(ATTIT.11:ATTIT.15, EXPO.11:EXPO.15),\n    names_to = c(\".value\", \"AGE\"),\n    names_sep = \"\\\\.\",\n    values_to = c(\"ATTIT\", \"EXPO\")\n  )\n\n## Drop missing ATTIT values\nnys1 = nys1.na[!is.na(nys1.na$ATTIT),] \n\n## Make age a number\nnys1$AGE = as.numeric(nys1$AGE)\nhead( nys1 )\n```\n\nWe also need to make our age a factor so it is treated appropriately as an indicator of what wave the data was collected in.\n\n```{r}\nnys1$agefac = as.factor(nys1$AGE) \n```\n\nJust to get a sense of the data, let's plot each age as a boxplot\n\n```{r, fig.height=3}\nggplot( nys1, aes( agefac, ATTIT ) ) +\n  geom_boxplot() +\n  theme_minimal()\n```\n\nNote some features of the data: First, we see that ATTIT goes up over time.\nSecond, we see the variation of points also goes up over time.\nThis is heteroskedasticity.\n\nIf we plot individual lines we have\n\n```{r, fig.height = 3}\nnys1$AGEjit = jitter(nys1$AGE)\nnys1$ATTITjit = jitter(nys1$ATTIT, amount=0.05)\nggplot( filter( nys1, complete.cases(nys1) ), aes( AGEjit, ATTITjit, group=ID ) ) +\n  geom_line( alpha=0.2 ) +\n  theme_minimal()\n```\n\nNote how we have correlation of residuals, in that some students are systematically low and some are systematically higher (although there is a lot of bouncing around).\n\n## Representation of error structure\n\nIn our data, we have 5 observations $y_{it}$ for each subject i at 5 fixed times $t=1$ through $t=5$.\nWithin each person $i$ (where person is our Level-2 group, and time is our Level-1), we can write\n\n$$\\begin{pmatrix}y_{i1}\\\\\ny_{i2}\\\\\ny_{i3}\\\\\ny_{i4}\\\\\ny_{i5}\n\\end{pmatrix} = \\left(\\begin{array}{c}\n\\mu_{1i}\\\\\n\\mu_{2i}\\\\\n\\mu_{3i}\\\\\n\\mu_{4i}\\\\\n\\mu_{5i}\n\\end{array}\\right) + \\left(\\begin{array}{c}\n\\epsilon_{1i}\\\\\n\\epsilon_{2i}\\\\\n\\epsilon_{3i}\\\\\n\\epsilon_{4i}\\\\\n\\epsilon_{5i}\n\\end{array}\\right)$$ where our set of 5 residuals are the random part, distributed as\n\n$$\\left(\\begin{array}{c}\n\\epsilon_{1i}\\\\\n\\epsilon_{2i}\\\\\n\\epsilon_{3i}\\\\\n\\epsilon_{4i}\\\\\n\\epsilon_{5i}\n\\end{array}\\right)\\sim N\\left[\\left(\\begin{array}{c}\n0\\\\\n0\\\\\n0\\\\\n0\\\\\n0\n\\end{array}\\right),\\left(\\begin{array}{ccccc}\n\\tau_{11} & \\tau_{12} & \\tau_{13} & \\tau_{14} & \\tau_{15}\\\\\n. & \\tau_{22} & \\tau_{23}& \\tau_{24} & \\tau_{25}\\\\\n. & . & \\tau_{33}& \\tau_{34} & \\tau_{35}\\\\\n. & . & . & \\tau_{44} & \\tau_{45}\\\\\n. & . & . & . & \\tau_{55}\n\\end{array}\\right)\\right] = N( 0, \\Sigma ). $$\n\nThe key part is the correlation between the residuals at different times.\nWe call our entire covariance matrix $\\Sigma$.\nThis matrix describes how the residuals within a single individual (with 5 time points of observation) are correlated.\n\nOur regression model gives us the mean vector for any given student (e.g., $(\\mu_{1i}, \\ldots, \\mu_{5i})$ would be $X_i'\\beta$, where $X_i$ is a $5 \\times p$ matrix of covariates for student $i$, and $\\beta$ is our fixed effect parameter vector.\n$X_i$ would have one row per time point and time would be one of the columns, to give our predictions for our 5 time points.\n\nOur error structure model gives us the distribution of the $(\\epsilon_{1i}, \\ldots, \\epsilon_{5i})$ for student $i$.\nDifferent ideas about the data generating process lead to different correlation structures here.\nWe saw a couple of those in class.\n\n## Reproducing R&B's Chapter 6 examples\n\nThe above provides a framework for thinking about grouped data: each group (i.e., student) is a small world with a linear prediction line and a collection of residuals around that line.\nUnder this view, we specify a specific structure on how the residuals relate to each other.\n(E.g., for classic OLS we would have i.i.d. normally distributed residuals, represented as our $\\Sigma$ being a diagonal matrix with $\\sigma^2$ along the diagonal and 0s everywhere else).\nIn R, once we determine what structure we want, we can fit models based on parameterized correlation matrices using the `lme` command from the `nlme` package (You may need to first call `install.packages(\"nlme\")` to get this package), or the `gls` package.\n\nLet's load the `nlme` package now:\n\n```{r}\nlibrary(nlme)\n```\n\nRecall that all of these models include a linear term on age and an intercept (so two fixed effects and no covariate adjustment).\n\n### Compound symmetry (random intercept model)\n\nA \"compound symmetry\" residual covariance structure (all diagonal elements equal, all off-diagonal elements equal) is actually equivalent to a random intercepts model.\nThus, there are 2 ways to get this same model:\n\n```{r, message=FALSE}\nmodelRE = lme(ATTIT ~ AGE, \n              data=nys1,\n              random=~1|ID )\n```\n\nand\n\n```{r}\nmodelCompSymm = gls(ATTIT ~ AGE,\n                    data=nys1,\n                    correlation=corCompSymm(form=~AGE|ID) )\n```\n\nFor reference, using the `lme4` package we again have (we use `lme4::` in front of `lmer` to avoid loading the `lme4` package fully):\n\n```{r}\nmodelRE.lme4 = lme4::lmer(ATTIT ~ AGE + (1|ID), data=nys1 )\n```\n\nWe can get the correlation matrix for individuals #3:\n\n```{r}\nmyVarCovs = getVarCov(modelRE,type=\"marginal\", individual=3)\nmyVarCovs\n```\n\nIf we look at an individual #5, who only has 4 timepoints we get a $4 \\times 4$ matrix:\n\n```{r}\ngetVarCov(modelRE,type=\"marginal\", individual=5)\n```\n\nOther individuals are the same, if they have the same number of time points, given our model. So in this model, we are saying the residuals of a student have the same distribution as any another student with the same number of time points.\n\n#### Comparing the models\n\nThese are two very different ways of specifying the same thing, and the parameter estimates we get out are also not the same.\nCompare the two summary printouts:\n\n```{r,  message=FALSE}\nsummary(modelRE)\n```\n\nand\n\n```{r}\nsummary(modelCompSymm)\n```\n\nThese do not look very similar, do they?\nBut wait:\n\n```{r,  message=FALSE}\nlogLik(modelCompSymm)\nlogLik(modelRE)\nlogLik(modelRE.lme4)\nAIC( modelCompSymm )\nAIC( modelRE )\nAIC( modelRE.lme4 )\n```\n\nIn fact, they have the same AIC, etc., because they are equivalent models.\n\nThe lesson is that it's actually quite hard to see the correspondence between a familiar random-effects model and an equivalent model expressed in terms of a covariance matrix.\nSure, we could do a bunch of math and see that in the end they are the same; but that math is already daunting here, and this is the simplest possible situation.\nThe fitted parameters of a covariance-based model are just really hard to interpret in familiar terms.\n\n### Autoregressive error structure (AR\\[1\\])\n\nOne typical structure used for longitudinal data is the \"autoregressive\" structure.\nThe idea is threefold:\n\n1.  $Var(u_{it}) = \\sigma^2$ - that is, overall marginal variance is staying constant.\n2.  $Cor(u_{it},u_{i(t-1)}) = \\rho$ - that is, residuals are a little bit \"sticky\" over time so residuals from nearby time points tend to be similar.\n3.  $E(u_{it}|u_{i(t-1)},u_{i(t-2)}) = E(u_{it}|u_{i(t-1)})$ - that is, the only way the two-periods-ago measurement tells you anything about the current one is through the intermediate one, with no longer-term effects or \"momentum\".\n\nIn this case, the unconditional two-step correlation $Cor(u_{it},u_{i(t-2)})$ is also easy to calculate.\nIntuitively, we can say that a portion $\\rho$ of the residual \"is the same\" after each step, so that after two steps the portion that \"is the same\" is $\\rho$ of $\\rho$, or $\\rho^2$.\nClearly, then, after three steps the correlation will be $\\rho^3$, and so on.\nIn other words, the part that \"is the same\" is decaying in an exponential pattern.\nIndeed, one could show that (3.), above, requires the correlated part to decay in a memoryless pattern, leaving the Exponential and Hypergeometric distributions (which both show exponential decay) among the few options.\n\nThus, the within-subject correlation structure implied by these postulates is:\n\n$$\\left(\\begin{array}{c}\nu_{1i}\\\\\nu_{2i}\\\\\nu_{3i}\\\\\nu_{4i}\\\\\n...\\\\\nu_{ni}\n\\end{array}\\right)\\sim N\\left[\\left(\\begin{array}{c}\n0\\\\\n0\\\\\n0\\\\\n0\\\\\n...\\\\\n0\n\\end{array}\\right),\\sigma^2\\left(\\begin{array}{cccccc}\n1 & \\rho  & \\rho^2 & \\rho^3 & ... & \\rho^{n-1}\\\\\n. & 1 & \\rho & \\rho^2 & ... & \\rho^{n-2}\\\\\n. & . & 1& \\rho  & ... & \\rho^{n-3}\\\\\n. & . & . & 1 & ... & \\rho^{n-4} \\\\\n... & ... & ... & ... & ... & ... \\\\\n. & . & . & . & ... & 1\n\\end{array}\\right)\\right]\\\\$$\n\nAs you can see, this structure takes advantage of the temporal nature of the data sequence to parameterize the covariance matrix with only two underlying parameters: $\\sigma$ and $\\rho$.\nBy contrast, a random intercept model needs the overall $\\sigma$ and variance of intercepts $\\tau$---also two parameters!\nSame complexity, different structure.\n\n#### Fitting the AR\\[1\\] covariance structure\n\nTo get a true AR\\[1\\] residual covariance structure, we need to leave the world of hierarchical models, and thus use the command `gls`.\nThis is just what we've discussed in class.\nHowever, later on in this document, we'll see how to add AR\\[1\\] structure on top of a hierarchical model, which is messier from a theoretical point of view, but often more useful and interpretable in practice.\n\n```{r, message=FALSE}\nmodelAR1 = gls(ATTIT ~ AGE, \n                    data=nys1,\n                    correlation=corAR1(form=~AGE|ID) )\n\nsummary(modelAR1)\n```\n\nYou have to dig around in the large amount of output to find the parameter estimates, but they are there.\n`Phi1` is the auto-correlation parameter.\nAnd the covariance of residuals:\n\n```{r}\ngetVarCov(modelAR1,type=\"marginal\")\nsummary(modelAR1)$AIC\n```\n\nNote that the AIC of our AR\\[1\\] model is lower by about 45 than the random intercept model; clearly far superior because it is getting nearby residuals being more correlated, while the random intercept model does not do this.\nAlso see the banding structure of the residual correlation matrix.\n\n### Random slopes\n\nIn theory, a random slopes model could be done with `gls` as well as with `lme` by building the final residual matrices as a function of the random slope parameters; in practice, it's much more practical just to do it as a hierarchical model with `lme`:\n\n```{r, message=FALSE}\nmodelRS = lme(ATTIT ~ 1 + AGE, \n              data=nys1,\n              random=~AGE|ID )\n```\n\nWe have separated our fixed and random components with `lme()`.\nWe first include a formula with only fixed effects, and then give a right-side-only formula with terms similar to what you'd put in parentheses with `lmer()` for the random effects.\n\nOur results:\n\n```{r}\nsummary(modelRS)\ngetVarCov(modelRS,type=\"marginal\", individual=3)\nsummary(modelRS)$AIC\n```\n\nThe first thing to note is the residual covariance matrix comes from the structure of the random intercept and random slope.\nIf you squint hard enough at it, you can begin to see the linear structures in its diagonal and off-diagonal elements.\nIf you graphed it, those structures would jump out more clearly.\nBut in practice, it's much easier to think of things in terms of the hierarchical model, not in terms of linear structures in a covariance matrix.\n\nNote also that the AIC has dropped by another 60 points or so; we're continuing to improve the model.\n\nAlso note that this is just using a different package to fit the exact same model we would fit using `lmer`; so far we haven't taken advantage of the `lme` command's additional flexibility.\n\n\n### Random slopes with heteroskedasticity\n\nRelaxing the homoskedasticity assumption in the random slopes model leaves us a bit in between worlds.\nWe're not fully into the world of GLS, because there are still random effects; but we're not fully in the world of hierarchical models because there is structure in the residuals within groups.\nWe'll talk more about this compromise below; for now, let's just do it.\n\n```{r, message=FALSE}\nmodelRSH = lme(ATTIT ~ AGE, \n              data=nys1,\n              random=~AGE|ID,\n              weights=varIdent(form=~1|agefac) )\n```\n\nThe key line is the `varIdent` line: we are saying each age factor level gets its own weight (rescaling) of the residuals---this is heteroskedasticity.\nIn particular, the above says our residual variance will be weighted by a weight for each age factor, so each age level effectively gets its own variance.\nThis is where these models start to get a bit exciting---we have random slopes, and then heteroskedastic residuals (homoskedastic for any given age level), all together.\nOur fit model:\n\n```{r}\nsummary(modelRSH)\n```\n\nNote how we have 5 parameter estimates for the residuals, listed under `agefac`.\nIt appears as if we have more variation in age 13 than other ages.\nAge 11, the baseline, is 1.0; it is our reference scaling.\nThese numbers are all scaling the overall residual variance parameter $\\sigma^2$ of $0.1405^2$.\n\nFor looking at the covariance structure of the residuals, we use `getVarCov()` again:\n\n```{r}\nmyVarCov = getVarCov(modelRSH,type=\"marginal\", individual=3)\nmyVarCov\n```\n\nWe get lists of matrices back from our call.\nWe can convert any one to a correlation matrix:\n\n```{r}\ncov2cor(myVarCov[[1]])\n```\n\nNo amount of squinting will show the structure in the original covariance matrix.\nBut when you convert to a correlation matrix, you can again squint and begin to see the linear structures in its diagonal and off-diagonal elements.\nThe same comment as above still applies: in practice, it's much easier to think of things in terms of the hierarchical model, and only read the diagonals of the covariance matrix.\n\nWe can also get our AIC:\n\n```{r}\nsummary(modelRSH)$AIC\n```\n\nThe AIC has dropped by only another 2.5 points or so; that corresponds to the idea that if one of these two models were exactly true, the odds are about $e^{2.5/2}\\cong 3.5$ in favor of the more complex model.\nAside from the fact that that premise is silly -- we are pretty sure that neither of these models is the exact truth; and in that case, something like BIC would probably be better than AIC -- those odds are also pretty weak; the simpler model is probably better here.\n\nHere's the reported BICs, by the way: `r summary(modelRS)$BIC` for the homoskedastic one, and `r summary(modelRSH)$BIC` for the heteroskedastic.\nAs we expected, the simpler model wins that fight.\n(Though what $N$ to use for BIC is sometimes not obvious with hierarchical models, so you can't trust those numbers too much; see the unit on AIC and BIC and model building.)\n\n### Fully unrestricted model\n\nOK, let's go whole hog, and fit the unrestricted model.\nAgain, this means leaving the world of hierarchical models and using gls.\n\n```{r unrestricted, message=FALSE}\n\nmodelUnrestricted = gls(ATTIT ~ AGE, \n               data=nys1,\n               correlation=corSymm(form=~1|ID),\n               weights=varIdent(form=~1|agefac) )\n\n\nsummary(modelUnrestricted)\n```\n\nAnd our residual structure:\n\n```{r}\nmyvc = getVarCov(modelUnrestricted,type=\"marginal\", individual=3)\nmyvc\n```\n\nAnd AIC:\n\n```{r}\nAIC( modelUnrestricted )\n```\n\nThis unrestricted covariance and correlation matrices have the same structures discussed in the book and in class.\nThe AIC has improved by another 6 or 7 points; that's marginally \"significant\", but in practice probably not substantial enough to make up for the massive loss of interpretability.\nThe lesson we should take from that is that there's not a whole lot of room for improvement just by tinkering with the residual covariance structure; if we want a much better model, we would have to add new fixed or random effects; perhaps other covariates or perhaps a quadratic term in time.\n\n## Having both AR\\[1\\] and Random Slopes\n\nLet's look at an AR1 residual structure along with some covariates in our main model.\nThe following has AR\\[1\\] and *also* a random intercept and slope:\n\n```{r,  message=FALSE}\nnys1$AGE11 = nys1$AGE - 11\nctrl <- lmeControl(opt='optim');\nmodel1 = lme(fixed=ATTIT ~ AGE11 + EXPO + FEMALE + MINORITY + log(INCOME + 1), \n              data=nys1,\n              random=~1 + AGE11|ID,\n              correlation=corAR1(),\n             control = ctrl )\n\nsummary(model1)\n```\n\nIn order to get this model to converge, we had to use the `lmeControl` command above; without it, the model doesn't converge due to not reaching a max in the given number of iterations.  The `lmeControl` with `optim` apparently turns up the juice so it converges without complaint.\n\nLet's compare our fit model to the same model without AR1 correlation\n\n```{r,  message=FALSE}\nmodel1simple = lme(fixed=ATTIT ~ AGE11 + EXPO + FEMALE + MINORITY + log(INCOME + 1), \n             data=nys1,\n             random=~1 + AGE11|ID )\nscreenreg( list( AR=model1, noAR=model1simple ) )\n```\n\nThe AR1 model has a notably lower AIC and thus is significantly better:\n\n\n```{r}\nanova( model1simple, model1 )\n```\n\nAutoregression involves only a single extra parameter--the autoregressive correlation coefficient.\n\nOur hybrid model is actually kind of mixed up, conceptually.\nWe allowed a random slope on age, and also an autoregressive component by age.\nThus, we effectively allowed the covariance matrix to vary in two different ways, at two different levels of our modeling.\n\nIn fact, as we've seen in class, any random effects, whether they be on slope or intercept, are actually equivalent to certain ways of varying the variance-covariance matrix of the residuals within each group.\nFor instance, random intercepts are equivalent to compound symmetry.\nThus, by including both random intercepts and AR1 correlation in the above model, we've effectively fit a model that allows any covariance matrix that can be expressed as a sum of a random slope covariance matrix (with 2 parameters plus a scaling factor) and an AR1 covariance matrix (with 1 parameter plus a scaling factor).\nThat makes 5 degrees of freedom total for our covariance matrix.\nThis is many fewer than the 15 for a fully unconstrained matrix, for comparison.\n\nConceptually this model is nice: people have linear growth trends, but vary around those growth trends in an autoregressive way.\n\n\n## The Kitchen sink: building complex models\n\nWhich brings us to the next point: how do you actually use this stuff in practice?\nIdeally, you'd like both the interpretability (and robustness against MAR missingness) of hierarchical models, along with the ability to add additional residual structure such as AR\\[1\\] and/or heteroskedastic residuals.\nThe good news is, you can get both.\nThe bad news is, there's a bit of a potential for bias due to overfitting.\n\nFor instance, imagine you use both random effects and AR\\[1\\].\nSay that for a given subject you have 5 time points, and all of them are above the values you would have predicted based on fixed effects alone.\nThat might be explained by an above-average random effect, or by a set of correlated residuals that all came in high.\nWhichever one of these is the \"true\" explanation, the MLE will tend to parcel it out between the two.\nThis can lead to downward bias in variance and/or correlation parameter estimates, especially with small numbers of observations per subject--the variation gets pushed into just assuming the residuals are correlated due to the auto-regressive structure.\n\nStill, as long as your focus is on location parameters such as true means or slopes, having hybrid models can be a good way to proceed.\nLet's explore this by first fitting a \"kitchen sink\" model for this data, in which we use all available covariates; and seeing how adding heteroskedasticity, AR\\[1\\] structure, or both changes it (or doesn't).\n\nWhat do we want in this \"kitchen sink\" model?\nLet's first fit a very simple random intercept model with fixed effects for gender, minority status, \"exposure\", and log(income), to see which of these covariates to focus on.\nWe use the `lmerTest` package to get some early $p$-values for these fixed effects.\n\n```{r, message=FALSE}\nmodelKS0 = lmerTest::lmer(ATTIT ~ FEMALE + MINORITY + log(INCOME + 1) + EXPO + (1|ID), data=nys1)\nsummary(modelKS0, correlation=FALSE)\n```\n\n(The `correlation=FALSE` shortens the printout.)\n\nApparently, MINORITY and EXPO are the covariates with significant effects; minority status is correlated with a lower tolerance for deviance, while \"deviant\" friends are of course correlated positively with tolerance of deviance.\nLet's build a few hierarchical models including these in various specifications (can you identify what models are what? Some of these models are not necessarily good choices).\nWe first center our age so we have meaningful intercepts.\n\n```{r kitchenSink, message=FALSE}\nnys1$age13 = nys1$AGE - 13\n\nmodelKS1 = lme(ATTIT ~ MINORITY + age13,\n              data=nys1,\n              random=~age13 + EXPO|ID )\n\nmodelKS2 = lme(ATTIT ~ MINORITY + age13, \n              data=nys1,\n              random=~age13 + EXPO|ID )\n\nmodelKS3 = lme(ATTIT ~ MINORITY + age13, \n              data=nys1,\n              random=~EXPO|ID )\n\nmodelKS4 = lme(ATTIT ~ MINORITY + age13 + EXPO, \n              data=nys1,\n              random=~1|ID )\n```\n\nAnd now we examine them:\n\n```{r}\nlibrary( texreg )\nscreenreg( list( modelKS1, modelKS2, modelKS3, modelKS4 ))\n```\n\nOK, Number 4 seems like a pretty good model.\nLet's see how much it improves when we add AR\\[1\\]:\n\n```{r, message=FALSE}\nmodelKS5 = lme(ATTIT ~ MINORITY + age13 + EXPO, \n              data=nys1,\n              random=~1|ID,\n              correlation=corAR1(form=~AGE|ID) )\nAIC( modelKS5 )\nfixef( modelKS4 )\nfixef( modelKS5 )\n```\n\nNote that the estimates for all the effects are essentially unchanged.\nHowever, the AIC is almost 40 points better.\nAlso, because the model has done a better job explaining residual variance, the $p$-value for the coefficient on MINORITY has dropped from 0.032 to 0.029, as we can see on the summary display below.\nThis is not a large drop, but a noticeable one:\n\n```{r}\nsummary( modelKS5 )\n```\n\nIs any of this drop in the $p$-value due to overfitting?\nGiven the size of the change in AIC, it seems doubtful that that's a significant factor.\n\nLet's try including heteroskedasticity, without AR\\[1\\]:\n\n```{r, message=FALSE}\nmodelKS6 = lme(ATTIT ~ MINORITY + age13 + EXPO, \n              data=nys1,\n              random=~1|ID,\n              weights=varIdent(form=~1|agefac) )\nAIC( modelKS6 )\n```\n\nThis did not improve AIC in this case, so we can avoid looking at this model further.\n\nFor completeness, let's look at a model with both AR(1) and heteroskedasticity:\n\n```{r, message=FALSE}\nmodelKS7 = lme(ATTIT ~ MINORITY + age13 + EXPO, \n              data=nys1,\n              random=~1|ID,\n              correlation=corAR1(form=~AGE|ID),\n              weights=varIdent(form=~1|agefac) )\nAIC( modelKS7 )\n```\n\nAgain, no improvement.\nSo we settle with our AR\\[1\\] model with a random intercept to get overall level of a student.\n\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"pygments","output-file":"complex_error.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.553","bibliography":["references.bib"],"output_dir":"docs","editor":{"markdown":{"wrap":"sentence"}},"theme":"cosmo","code-copy":true,"title":"An overview of complex error structures","author":"Luke Miratrix"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","highlight-style":"pygments","output-file":"complex_error.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"block-headings":true,"bibliography":["references.bib"],"output_dir":"docs","editor":{"markdown":{"wrap":"sentence"}},"documentclass":"scrreprt","title":"An overview of complex error structures","author":"Luke Miratrix"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}