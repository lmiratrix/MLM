{"title":"Intro to Regression","markdown":{"yaml":{"title":"Intro to Regression","author":"Luke Miratrix","editor":{"markdown":{"wrap":"sentence"}}},"headingText":"clear memory","containsRefs":false,"markdown":"\n\n```{r setup, include=FALSE}\nlibrary(tidyverse)\nlibrary(Lock5Data)\nlibrary(knitr)\nlibrary(broom)\nlibrary(ggeffects)\nlibrary(sjPlot)\n\nknitr::opts_chunk$set(echo = TRUE)\noptions( digits = 3 )\nopts_knit$set(progress = TRUE)\nopts_chunk$set(progress = TRUE, verbose = TRUE, prompt=FALSE,echo=TRUE,\n               fig.align=\"center\", fig.width=8, fig.height=5, \n               out.width=\"0.7\\\\linewidth\", size=\"scriptsize\")\n\nrm(list = ls())\n\n# set ggplot theme\ntheme_set(theme_bw())\n```\n\nThis walkthrough shows how to fit simple linear regression models in R.\nLinear regression is the main way researchers tend to examine the relationships between multiple variables.\nThis document runs through some code without too much discussion, with the assumption that you are already familiar with interpretation of such models.\n\n## Simple Regression\n\nWe are going to use an example dataset, `RestaurantTips`, that records tip amounts for a series of bills.\nLet's first regress `Tip` on `Bill`.\nBefore doing regression, we should plot the data to make sure using simple linear regression is reasonable.\nFor kicks, we add in an automatic regression line as well by taking advantage of ggplot's `geom_smooth()` method:\n\n```{r RegressionCheck,echo=TRUE, warning=FALSE, message=FALSE }\n# load the data into memory\ndata(RestaurantTips)\n\n# plot Tip on Bill\nggplot( RestaurantTips, aes(x = Bill, y = Tip) ) +\n    geom_point() +\n    geom_smooth( method=\"lm\", se=FALSE ) +\n    geom_smooth( method=\"loess\", se=FALSE, col=\"orange\" ) +\n    labs(title = \"Tip given Bill\")\n```\n\nThat looks pretty darn linear!\nThere are a few unusually large tips, but no extreme outliers, and variability appears to be constant at all levels of `Bill` , so we proceed:\n\n```{r}\n# fit the linear model\nmod <- lm(Tip ~ Bill, data = RestaurantTips)\nsummary(mod)\n```\n\nThe first line tells R to fit the regression.\nThe thing on the left of the `~` is our outcome, the things on the right are our covariates or predictors.\nR then saves the results of all that work under the name `mod` (short for model - you can call it anything you want).\nOnce we fit the model, we used `summary()` command to print the output to the screen.\n\nResults relevant to the intercept are in the `(Intercept)` row and results relevant to the slope are in the `Bill` row (`Bill` is the explanatory variable).\nThe `Estimate` column gives the estimated coefficients, the `Std. Error` column gives the standard error for these estimates, the `t value` is simply estimate/SE, and the p-value is the result of a hypothesis test testing whether that coefficient is significantly different from 0.\n\nWe also see the RMSE as `Residual standard error` and $R^2$ as `Multiple R-squared`.\nThe last line of the regression output gives details relevant to an ANOVA table for testing our model against no model.\nIt has the F-statistic, degrees of freedom, and p-value.\n\nYou can pull the coefficients of your model out with the `coef()` command:\n\n```{r GettingLmStuff,echo=TRUE }\ncoef(mod)\ncoef(mod)[1] # intercept\ncoef(mod)[2] # slope\ncoef(mod)[\"Bill\"] # alternate way.\n```\n\nAlternatively, you can use the `tidy()` function from `broom` to turn the regression results into a tidy data frame, which makes it easier to work with:\n\n```{r}\ntidy(mod)\ntidy(mod)[[2,2]] # slope\n```\n\nWe can plot our regression line on top of the scatterplot manually using the `geom_abline()` layer in ggplot:\n\n```{r LinearRegressionPlot, echo=TRUE }\nggplot( RestaurantTips, aes( Bill, Tip ) ) +\n  geom_point() +\n  geom_abline( intercept = -0.292, slope =  0.182, col=\"red\" )\n```\n\n## Multiple Regression\n\nWe now include the additional explanatory variables of number in party (`Guests`) and whether or not they pay with a credit card (`Credit`):\n\n```{r RegressionMultiple, echo=TRUE }\ntip.mod <- lm(Tip ~ Bill + Guests + Credit, data=RestaurantTips )\nsummary(tip.mod)\n```\n\nThis output should look very similar to the output for one variable, except now there is a row corresponding to each explanatory variable.\nOur two-category (y, n) `Credit` variable was automatically converted to a 0-1 dummy variable (with \"y\" being 1 and \"n\" our baseline).\n\n### Easy Tabulation and Graphing of Multiple Regression Models\n\nFor publication-ready tables and graphics, R has many wonderful packages to automate the process.\nI (JG) am partial to `tab_model` from `sjPlot` for regression tables, and `ggeffects` for regression graphs, as shown below.\nSee more in the other chapters.\n\n```{r}\n# tabulate the results of our two tip models\ntab_model(mod, tip.mod)\n\n# graph model 2, with Bill on X, Credit as color, and Guests held constant at the mean\nggeffect(tip.mod, terms = c(\"Bill\", \"Credit\")) |> \n  plot(add.data = TRUE, ci = FALSE)\n```\n\n## Categorical Variables (and Factors)\n\nYou can include any explanatory categorical variable in a multiple regression model, and R will automatically create corresponding 0/1 variables.\nFor example, if you were to include gender coded as male/female, R would create a variable GenderMale that is 1 for males and 0 for females.\n\n### Numbers Coding Categories.\n\nIf you have multiple levels of a category, but your levels are coded with numbers you have to be a bit careful because R can treat this as a quantitative (continuous) variable by mistake in some cases.\nYou will know it did this if you only see the single variable on one line of your output.\nFor categorical variables with $k$ categories, you should see $k-1$ lines.\n\nTo make a variable categorical, even if the levels are numbers, convert the variable to a factor with `as.factor` or `factor`:\n\n```{r MakeFactor, echo=TRUE, fig.keep='none' }\n# load the US states data\ndata( USStates )\n\n# convert Region to a factor\nUSStates <- USStates |> \n  mutate(Region = factor(Region))\n```\n\n### Setting new baselines.\n\nWe can reorder the levels if desired (the first is our baseline).\n\n```{r ReorderFactor, echo=TRUE, fig.keep='none' }\nlevels( USStates$Region )\nUSStates$Region = relevel(USStates$Region, \"S\" )\nlevels( USStates$Region )\n```\n\nNow any regression will use the south as baseline.\n\n### Testing for significance of a categorical variable.\n\nWhen deciding whether to keep a categorical variable, we need to test how important all the dummy variables for that category are to the model all at once.\nWe do this with ANOVA.\nHere we examine whether region is useful for predicting the percent vote for Clinton in 2016:\n\n```{r CatCheck, echo=TRUE, fig.keep='none'}\nmlm = lm( ClintonVote ~ Region, data=USStates)\nanova( mlm )\n```\n\nIt is quite important.\n\nWe can also compare for region beyond some other variable:\n\n```{r CatCheck2, echo=TRUE}\nmlm2 = lm( ClintonVote ~ HouseholdIncome + HouseholdIncome + HighSchool + \n               EighthGradeMath, data=USStates)\n\nmlm3 = lm( ClintonVote ~ HouseholdIncome + HouseholdIncome + HighSchool + \n               EighthGradeMath + Region, data=USStates)\nanova( mlm2, mlm3 )\n```\n\nRegion is still important, beyond including some further controls.\nInterpreting this mess of a regression is not part of this document; this document shows you how to run regressions but it doesn't discuss whether you should or not.\n\n### Missing levels in a factor\n\nR often treats categorical variables as factors.\nThis is often useful, but sometimes annoying.\nA factor has different **levels** which are the different values it can be.\nFor example:\n\n```{r CatLevels, echo=TRUE }\ndata(FishGills3)\nlevels(FishGills3$Calcium)\ntable(FishGills3$Calcium)\n```\n\nNote the weird nameless level; it also has no actual observations in it.\nNevertheless, if you make a boxplot, you will get an empty plot in addition to the other three.\nThis error was likely due to some past data entry issue.\nYou can drop the unused level:\n\n```{r DropCatLevels, echo=TRUE }\nFishGills3$Calcium = droplevels(FishGills3$Calcium)\n```\n\nYou can also turn a categorical variable into a numeric one like so:\n\n```{r CatToNum, echo=TRUE }\nsummary( FishGills3$Calcium )\nasnum = as.numeric( FishGills3$Calcium )\nasnum\n```\n\nRegression on only a categorical variable is fine:\n\n```{r CatLevelsLM, echo=TRUE }\nmylm = lm( GillRate ~ Calcium, data=FishGills3 )\nmylm\n```\n\nR has made you a bunch of dummy variables automatically.\nHere \"high\" is the baseline, selected automatically.\nWe can also force it so there is no baseline by removing the intercept, in which case the coefficients are the means of each group.\n\n```{r CatLevelsLM2, echo=TRUE }\nmymm = lm( GillRate ~ 0 + Calcium, data=FishGills3 )\nmymm\n```\n\n## Some extra stuff (optional)\n\n### Confidence Intervals\n\nTo get confidence intervals around each parameter in your model, try this:\n\n```{r RegrConfInt}\nconfint(tip.mod)\n```\n\nYou can also create them easily using `tidy` and `mutate`:\n\n```{r}\ntip.mod |> \n  tidy() |> \n  mutate(upper = estimate + 1.96*std.error,\n         lower = estimate - 1.96*std.error)\n```\n\n### Prediction\n\nSuppose a server at this bistro is about to deliver a \\$20 bill, and wants to predict their tip.\nThey can get a predicted value and 95% (this is the default level, change with level) prediction interval with\n\n```{r RegrPrediction}\nnew.dat = data.frame( Bill = c(20) )\npredict(mod,new.dat,interval = \"prediction\")\n```\n\nThey should expect a tip somewhere between \\$1.41 and \\$5.30.\n\nIf we know a bit more we can use our more complex model called `tip.mod` from above:\n\n```{r RegrPrediction2}\nnew.dat = data.frame( Bill = c(20), Guests=c(1), Credit=c(\"n\") )\npredict(tip.mod,new.dat,interval = \"prediction\")\n```\n\nThis is the predicted tip for one guest paying with cash for a \\$20 tip.\nIt is wider than our original interval because our model is a bit more unstable (it turns out guest number and credit card aren't that relevant or helpful).\n\nCompare the prediction interval to the confidence interval\n\n```{r RegrPredictionCI}\nnew.dat = data.frame( Bill = c(20), Guests=c(1), Credit=c(\"n\") )\npredict(tip.mod, new.dat, interval = \"confidence\")\n```\n\nThis predicts the mean tip for all single guests who pay a \\$20 bill with cash.\nOur interval is smaller because we are generating a confidence interval for where the mean is, and are ignoring that individuals will vary around that mean.\nConfidence intervals are different from prediction intervals.\n\n### Removing Outliers\n\nIf you can identify which rows the outliers are on, you can do this by hand (say the rows are 5, 10, 12).\n\n```{r eval=FALSE }\nnew.data = old.data[ -c(5,10,12), ]\nlm( Y ~ X, data=new.data )\n```\n\nSome technical details: The `c(5,10,12)` is a list of 3 numbers.\nThe `c()` is the concatenation function that takes things makes lists out of them.\nThe \"-list\" notation means give me my old data, but without rows 5, 10, and 12.\nNote the comma after the list.\nThis is because we identify elements in a dataframe with row, column notation.\nSo `old.data[1,3]` would be row 1, column 3.\n\nIf you notice your points all have X bigger than some value, say 20.5, you could use filtering to keep everything less than some value:\n\n```{r eval=FALSE }\nnew.data = filter( old.data, X <= 20.5 )\n```\n\n### Missing data\n\nIf you have missing data, `lm` will automatically drop those cases because it doesn't know what else to do.\nIt will tell you this, however, with the `summary` command.\n\n```{r echo=TRUE }\ndata(AllCountries)\ndev.lm = lm( BirthRate ~ Rural + Health + ElderlyPop, data=AllCountries )\nsummary( dev.lm  )\n```\n\n### Residual plots and model fit\n\nIf we throw out model into the `plot` function, we get some nice regression diagnostics.\n\n```{r}\nplot(tip.mod)\n```\n\nTo generate classic model fit diagnostics with more control, we need to calculate residuals, make a residual versus fitted values plot, and make a histogram of the residuals.\nWe can make some quick and dirty plots with `qplot` (standing for \"quick plot\") like so:\n\n```{r ConditionsForRegression, echo=TRUE}\nqplot(tip.mod$fit, tip.mod$residuals )\n```\n\nand\n\n```{r}\nqplot(tip.mod$residuals, bins=30)\n```\n\nWe see no real pattern other than some extreme outliers.\nThe residual histogram suggests we are not really normally distributed, so we should treat our SEs and $p$-values with caution.\nThese plots are the canonical \"model-checking'' plots you might use.\n\nAnother is the \"fitted outcomes vs. actual outcomes'' plot of:\n\n```{r ConditionsForRegression2, echo=TRUE }\npredicted = predict( dev.lm )\nactual = dev.lm$model$BirthRate\nqplot( actual, predicted, main=\"Fit vs. actual Birth Rate\" )\n```\n\nNote the `dev.lm` variable has a `model` variable inside it.\nThis is a data frame of the **used** data for the model (i.e., if cases were dropped due to missingness, they will not be in the model).\nWe then grab the birth rates from this, and make a scatterplot.\nIf we tried to skip this, and use the original data, we would get an error because our original data set has some observations that were dropped.\n\nNote we can't just add our predictions to `AllCountries` since we would get an error due to this dropped data issue:\n\n```{r, eval=FALSE}\nAllCountries$predicted = predict( dev.lm )\n```\n\n```         \nError in `$<-.data.frame`(`*tmp*`, predicted, value = c(`1` = 31.630301617421,  : \n  replacement has 179 rows, data has 217\n```\n\nWe can, however, predict like this:\n\n```{r}\nAllCountries$predicted = predict( dev.lm, newdata=AllCountries )\n```\n\nThe `newdata` tells predict to generate a prediction for each row in AllCountries rather than each row in the left over data after `lm` dropped cases with missing values.\n","srcMarkdownNoYaml":"\n\n```{r setup, include=FALSE}\nlibrary(tidyverse)\nlibrary(Lock5Data)\nlibrary(knitr)\nlibrary(broom)\nlibrary(ggeffects)\nlibrary(sjPlot)\n\nknitr::opts_chunk$set(echo = TRUE)\noptions( digits = 3 )\nopts_knit$set(progress = TRUE)\nopts_chunk$set(progress = TRUE, verbose = TRUE, prompt=FALSE,echo=TRUE,\n               fig.align=\"center\", fig.width=8, fig.height=5, \n               out.width=\"0.7\\\\linewidth\", size=\"scriptsize\")\n\n# clear memory\nrm(list = ls())\n\n# set ggplot theme\ntheme_set(theme_bw())\n```\n\nThis walkthrough shows how to fit simple linear regression models in R.\nLinear regression is the main way researchers tend to examine the relationships between multiple variables.\nThis document runs through some code without too much discussion, with the assumption that you are already familiar with interpretation of such models.\n\n## Simple Regression\n\nWe are going to use an example dataset, `RestaurantTips`, that records tip amounts for a series of bills.\nLet's first regress `Tip` on `Bill`.\nBefore doing regression, we should plot the data to make sure using simple linear regression is reasonable.\nFor kicks, we add in an automatic regression line as well by taking advantage of ggplot's `geom_smooth()` method:\n\n```{r RegressionCheck,echo=TRUE, warning=FALSE, message=FALSE }\n# load the data into memory\ndata(RestaurantTips)\n\n# plot Tip on Bill\nggplot( RestaurantTips, aes(x = Bill, y = Tip) ) +\n    geom_point() +\n    geom_smooth( method=\"lm\", se=FALSE ) +\n    geom_smooth( method=\"loess\", se=FALSE, col=\"orange\" ) +\n    labs(title = \"Tip given Bill\")\n```\n\nThat looks pretty darn linear!\nThere are a few unusually large tips, but no extreme outliers, and variability appears to be constant at all levels of `Bill` , so we proceed:\n\n```{r}\n# fit the linear model\nmod <- lm(Tip ~ Bill, data = RestaurantTips)\nsummary(mod)\n```\n\nThe first line tells R to fit the regression.\nThe thing on the left of the `~` is our outcome, the things on the right are our covariates or predictors.\nR then saves the results of all that work under the name `mod` (short for model - you can call it anything you want).\nOnce we fit the model, we used `summary()` command to print the output to the screen.\n\nResults relevant to the intercept are in the `(Intercept)` row and results relevant to the slope are in the `Bill` row (`Bill` is the explanatory variable).\nThe `Estimate` column gives the estimated coefficients, the `Std. Error` column gives the standard error for these estimates, the `t value` is simply estimate/SE, and the p-value is the result of a hypothesis test testing whether that coefficient is significantly different from 0.\n\nWe also see the RMSE as `Residual standard error` and $R^2$ as `Multiple R-squared`.\nThe last line of the regression output gives details relevant to an ANOVA table for testing our model against no model.\nIt has the F-statistic, degrees of freedom, and p-value.\n\nYou can pull the coefficients of your model out with the `coef()` command:\n\n```{r GettingLmStuff,echo=TRUE }\ncoef(mod)\ncoef(mod)[1] # intercept\ncoef(mod)[2] # slope\ncoef(mod)[\"Bill\"] # alternate way.\n```\n\nAlternatively, you can use the `tidy()` function from `broom` to turn the regression results into a tidy data frame, which makes it easier to work with:\n\n```{r}\ntidy(mod)\ntidy(mod)[[2,2]] # slope\n```\n\nWe can plot our regression line on top of the scatterplot manually using the `geom_abline()` layer in ggplot:\n\n```{r LinearRegressionPlot, echo=TRUE }\nggplot( RestaurantTips, aes( Bill, Tip ) ) +\n  geom_point() +\n  geom_abline( intercept = -0.292, slope =  0.182, col=\"red\" )\n```\n\n## Multiple Regression\n\nWe now include the additional explanatory variables of number in party (`Guests`) and whether or not they pay with a credit card (`Credit`):\n\n```{r RegressionMultiple, echo=TRUE }\ntip.mod <- lm(Tip ~ Bill + Guests + Credit, data=RestaurantTips )\nsummary(tip.mod)\n```\n\nThis output should look very similar to the output for one variable, except now there is a row corresponding to each explanatory variable.\nOur two-category (y, n) `Credit` variable was automatically converted to a 0-1 dummy variable (with \"y\" being 1 and \"n\" our baseline).\n\n### Easy Tabulation and Graphing of Multiple Regression Models\n\nFor publication-ready tables and graphics, R has many wonderful packages to automate the process.\nI (JG) am partial to `tab_model` from `sjPlot` for regression tables, and `ggeffects` for regression graphs, as shown below.\nSee more in the other chapters.\n\n```{r}\n# tabulate the results of our two tip models\ntab_model(mod, tip.mod)\n\n# graph model 2, with Bill on X, Credit as color, and Guests held constant at the mean\nggeffect(tip.mod, terms = c(\"Bill\", \"Credit\")) |> \n  plot(add.data = TRUE, ci = FALSE)\n```\n\n## Categorical Variables (and Factors)\n\nYou can include any explanatory categorical variable in a multiple regression model, and R will automatically create corresponding 0/1 variables.\nFor example, if you were to include gender coded as male/female, R would create a variable GenderMale that is 1 for males and 0 for females.\n\n### Numbers Coding Categories.\n\nIf you have multiple levels of a category, but your levels are coded with numbers you have to be a bit careful because R can treat this as a quantitative (continuous) variable by mistake in some cases.\nYou will know it did this if you only see the single variable on one line of your output.\nFor categorical variables with $k$ categories, you should see $k-1$ lines.\n\nTo make a variable categorical, even if the levels are numbers, convert the variable to a factor with `as.factor` or `factor`:\n\n```{r MakeFactor, echo=TRUE, fig.keep='none' }\n# load the US states data\ndata( USStates )\n\n# convert Region to a factor\nUSStates <- USStates |> \n  mutate(Region = factor(Region))\n```\n\n### Setting new baselines.\n\nWe can reorder the levels if desired (the first is our baseline).\n\n```{r ReorderFactor, echo=TRUE, fig.keep='none' }\nlevels( USStates$Region )\nUSStates$Region = relevel(USStates$Region, \"S\" )\nlevels( USStates$Region )\n```\n\nNow any regression will use the south as baseline.\n\n### Testing for significance of a categorical variable.\n\nWhen deciding whether to keep a categorical variable, we need to test how important all the dummy variables for that category are to the model all at once.\nWe do this with ANOVA.\nHere we examine whether region is useful for predicting the percent vote for Clinton in 2016:\n\n```{r CatCheck, echo=TRUE, fig.keep='none'}\nmlm = lm( ClintonVote ~ Region, data=USStates)\nanova( mlm )\n```\n\nIt is quite important.\n\nWe can also compare for region beyond some other variable:\n\n```{r CatCheck2, echo=TRUE}\nmlm2 = lm( ClintonVote ~ HouseholdIncome + HouseholdIncome + HighSchool + \n               EighthGradeMath, data=USStates)\n\nmlm3 = lm( ClintonVote ~ HouseholdIncome + HouseholdIncome + HighSchool + \n               EighthGradeMath + Region, data=USStates)\nanova( mlm2, mlm3 )\n```\n\nRegion is still important, beyond including some further controls.\nInterpreting this mess of a regression is not part of this document; this document shows you how to run regressions but it doesn't discuss whether you should or not.\n\n### Missing levels in a factor\n\nR often treats categorical variables as factors.\nThis is often useful, but sometimes annoying.\nA factor has different **levels** which are the different values it can be.\nFor example:\n\n```{r CatLevels, echo=TRUE }\ndata(FishGills3)\nlevels(FishGills3$Calcium)\ntable(FishGills3$Calcium)\n```\n\nNote the weird nameless level; it also has no actual observations in it.\nNevertheless, if you make a boxplot, you will get an empty plot in addition to the other three.\nThis error was likely due to some past data entry issue.\nYou can drop the unused level:\n\n```{r DropCatLevels, echo=TRUE }\nFishGills3$Calcium = droplevels(FishGills3$Calcium)\n```\n\nYou can also turn a categorical variable into a numeric one like so:\n\n```{r CatToNum, echo=TRUE }\nsummary( FishGills3$Calcium )\nasnum = as.numeric( FishGills3$Calcium )\nasnum\n```\n\nRegression on only a categorical variable is fine:\n\n```{r CatLevelsLM, echo=TRUE }\nmylm = lm( GillRate ~ Calcium, data=FishGills3 )\nmylm\n```\n\nR has made you a bunch of dummy variables automatically.\nHere \"high\" is the baseline, selected automatically.\nWe can also force it so there is no baseline by removing the intercept, in which case the coefficients are the means of each group.\n\n```{r CatLevelsLM2, echo=TRUE }\nmymm = lm( GillRate ~ 0 + Calcium, data=FishGills3 )\nmymm\n```\n\n## Some extra stuff (optional)\n\n### Confidence Intervals\n\nTo get confidence intervals around each parameter in your model, try this:\n\n```{r RegrConfInt}\nconfint(tip.mod)\n```\n\nYou can also create them easily using `tidy` and `mutate`:\n\n```{r}\ntip.mod |> \n  tidy() |> \n  mutate(upper = estimate + 1.96*std.error,\n         lower = estimate - 1.96*std.error)\n```\n\n### Prediction\n\nSuppose a server at this bistro is about to deliver a \\$20 bill, and wants to predict their tip.\nThey can get a predicted value and 95% (this is the default level, change with level) prediction interval with\n\n```{r RegrPrediction}\nnew.dat = data.frame( Bill = c(20) )\npredict(mod,new.dat,interval = \"prediction\")\n```\n\nThey should expect a tip somewhere between \\$1.41 and \\$5.30.\n\nIf we know a bit more we can use our more complex model called `tip.mod` from above:\n\n```{r RegrPrediction2}\nnew.dat = data.frame( Bill = c(20), Guests=c(1), Credit=c(\"n\") )\npredict(tip.mod,new.dat,interval = \"prediction\")\n```\n\nThis is the predicted tip for one guest paying with cash for a \\$20 tip.\nIt is wider than our original interval because our model is a bit more unstable (it turns out guest number and credit card aren't that relevant or helpful).\n\nCompare the prediction interval to the confidence interval\n\n```{r RegrPredictionCI}\nnew.dat = data.frame( Bill = c(20), Guests=c(1), Credit=c(\"n\") )\npredict(tip.mod, new.dat, interval = \"confidence\")\n```\n\nThis predicts the mean tip for all single guests who pay a \\$20 bill with cash.\nOur interval is smaller because we are generating a confidence interval for where the mean is, and are ignoring that individuals will vary around that mean.\nConfidence intervals are different from prediction intervals.\n\n### Removing Outliers\n\nIf you can identify which rows the outliers are on, you can do this by hand (say the rows are 5, 10, 12).\n\n```{r eval=FALSE }\nnew.data = old.data[ -c(5,10,12), ]\nlm( Y ~ X, data=new.data )\n```\n\nSome technical details: The `c(5,10,12)` is a list of 3 numbers.\nThe `c()` is the concatenation function that takes things makes lists out of them.\nThe \"-list\" notation means give me my old data, but without rows 5, 10, and 12.\nNote the comma after the list.\nThis is because we identify elements in a dataframe with row, column notation.\nSo `old.data[1,3]` would be row 1, column 3.\n\nIf you notice your points all have X bigger than some value, say 20.5, you could use filtering to keep everything less than some value:\n\n```{r eval=FALSE }\nnew.data = filter( old.data, X <= 20.5 )\n```\n\n### Missing data\n\nIf you have missing data, `lm` will automatically drop those cases because it doesn't know what else to do.\nIt will tell you this, however, with the `summary` command.\n\n```{r echo=TRUE }\ndata(AllCountries)\ndev.lm = lm( BirthRate ~ Rural + Health + ElderlyPop, data=AllCountries )\nsummary( dev.lm  )\n```\n\n### Residual plots and model fit\n\nIf we throw out model into the `plot` function, we get some nice regression diagnostics.\n\n```{r}\nplot(tip.mod)\n```\n\nTo generate classic model fit diagnostics with more control, we need to calculate residuals, make a residual versus fitted values plot, and make a histogram of the residuals.\nWe can make some quick and dirty plots with `qplot` (standing for \"quick plot\") like so:\n\n```{r ConditionsForRegression, echo=TRUE}\nqplot(tip.mod$fit, tip.mod$residuals )\n```\n\nand\n\n```{r}\nqplot(tip.mod$residuals, bins=30)\n```\n\nWe see no real pattern other than some extreme outliers.\nThe residual histogram suggests we are not really normally distributed, so we should treat our SEs and $p$-values with caution.\nThese plots are the canonical \"model-checking'' plots you might use.\n\nAnother is the \"fitted outcomes vs. actual outcomes'' plot of:\n\n```{r ConditionsForRegression2, echo=TRUE }\npredicted = predict( dev.lm )\nactual = dev.lm$model$BirthRate\nqplot( actual, predicted, main=\"Fit vs. actual Birth Rate\" )\n```\n\nNote the `dev.lm` variable has a `model` variable inside it.\nThis is a data frame of the **used** data for the model (i.e., if cases were dropped due to missingness, they will not be in the model).\nWe then grab the birth rates from this, and make a scatterplot.\nIf we tried to skip this, and use the original data, we would get an error because our original data set has some observations that were dropped.\n\nNote we can't just add our predictions to `AllCountries` since we would get an error due to this dropped data issue:\n\n```{r, eval=FALSE}\nAllCountries$predicted = predict( dev.lm )\n```\n\n```         \nError in `$<-.data.frame`(`*tmp*`, predicted, value = c(`1` = 31.630301617421,  : \n  replacement has 179 rows, data has 217\n```\n\nWe can, however, predict like this:\n\n```{r}\nAllCountries$predicted = predict( dev.lm, newdata=AllCountries )\n```\n\nThe `newdata` tells predict to generate a prediction for each row in AllCountries rather than each row in the left over data after `lm` dropped cases with missing values.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"pygments","output-file":"intro_linear_regression.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.353","bibliography":["references.bib"],"output_dir":"docs","editor":{"markdown":{"wrap":"sentence"}},"theme":"cosmo","code-copy":true,"title":"Intro to Regression","author":"Luke Miratrix"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","highlight-style":"pygments","output-file":"intro_linear_regression.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"block-headings":true,"bibliography":["references.bib"],"output_dir":"docs","editor":{"markdown":{"wrap":"sentence"}},"documentclass":"scrreprt","title":"Intro to Regression","author":"Luke Miratrix"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}