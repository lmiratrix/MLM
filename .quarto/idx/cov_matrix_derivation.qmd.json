{"title":"Covariance Derivation","markdown":{"yaml":{"title":"Covariance Derivation","author":"Luke Miratrix","editor":{"markdown":{"wrap":"sentence"}}},"headingText":"The student-level residual matrix","containsRefs":false,"markdown":"\n\nIn this chapter we lay out some of the derivations on residual matrices.\nWe use the running example of the NYS data (see Packet 7).\n\n\nFollowing Packet 7.1, let's think about a generic regression equation for a linear growth model with 5 timepoints (this is a simplified version of the NYS model, where each time point is a year of age, 11--15).\n\nIn particular, consider $$\nY_{ti} = \\beta_0 + \\beta_1 age_{ti} + u_{ti}\n$$ where $age_{ti}$ is our age from 11 (so an observation at 11 years old would have `age11 = 0`).\nThis means our intercept correspond to our first timepoint, with $a_1 = 0, a_2 = 1, ..., a_5 = 4$.\nI.e., our $age_{ti}$ is number of years since onset of study.\n\nIn this model, $\\beta_{0}$ is the average outcome across our population at the onset of the study and $\\beta_{1}$ is the average rate of growth (per year) in the population.\n\nNow we have 5 observations for each student $i$, so the residuals $(u_{1i}, \\ldots, u_{5i})$ are likely correlated with each other.\nFor example, a student might just generally have higher levels of outcome, or lower levels, which means the overall residual of one time point would be related to the reisduals of other time points.\nIn math, we can write that for any randomly subject $i$, the covariance matrix of their residuals is $$\\begin{aligned}\n\\begin{pmatrix} u_{i1} \\\\\nu_{i2} \\\\\nu_{i3} \\\\\nu_{i4} \\\\\nu_{i5}\n\\end{pmatrix} &\\sim  N\n\\begin{bmatrix}\n\\begin{pmatrix}\n0 \\\\\n0 \\\\\n0\\\\\n0 \\\\\n0\n\\end{pmatrix}\\!\\!,&\n\\begin{pmatrix}\n\\delta_{11} & \\delta_{12} & \\delta_{13} & \\delta_{14} & \\delta_{15} \\\\\n           & \\delta_{22} & \\delta_{23} & \\delta_{24} & \\delta_{25} \\\\\n         &              & \\delta_{33} & \\delta_{34} & \\delta_{35} \\\\\n         &              &             & \\delta_{44} & \\delta_{45} \\\\\n         &              &              &            & \\delta_{55} \n\\end{pmatrix}\n\\end{bmatrix}\n\\end{aligned} = N( 0, \\Sigma_i)$$\n\nThis matrix of residuals for student $i$ is one of the blocks in our $N \\times N$ block-diagonal matrix for all our residuals (this would be the giant matrix plugged into our sandwich formula to get standard errors for $\\beta_0$ and $\\beta_1$), where $N$ is the number of observations.\nAssuming 5 observations per student, multilevel and generalized linear modeling (which we are talking about here) make the assumption that this matrix is the same across students; cluster robust standard errors would not make this assumption.\n(More broadly, MLM and generalized linear modeling make the assumption that we can represent all the $\\Sigma_i$ in terms of measured covariates and pre-specified parameters, but in this case we end up with the same matrix for all students with 5 time points. Students with fewer than 5 would be subsets of this matrix corresponding to the time points observed.)\n\nThe diagonal of $\\Sigma_i$ are our variances at each timepoint (this means, for example, that if our model is good, that if we took the variance of all the observations where $t=5$ across our dataset we should get something close to $\\delta_{55}$).\n\nIn the remainder of this document, we look at how MLM gives expressions for this matrix.\n\n## Covariance matrix for a random intercept model\n\nFollowing Packet 7.1, we start with a random intercept model with a completely pooled growth component with 5 timepoints (this is a simplified version of the NYS model, where each time point is a year of age, 11--15).\nIn particular, take the model represented by this `lmer()` command:\n\n```{r, eval=FALSE}\nM = lmer( Y ~ 1 + age11 + (1|id), data=nys )\n```\n\nwhere `age11` is our age from 11 (so an observation at 11 years old would have `age11 = 0`).\n\nIn math, this model is $$\\begin{aligned}\nY_{ti} &= \\pi_{0i} + \\beta_{1} age_ + \\epsilon_{ti} \\\\\n\\epsilon_{ti} &\\sim N( 0, \\sigma^2 ) \\\\\n\\pi_{0i} &=  \\beta_{0} + r_{0i} \\\\\n r_{0j} & \\sim N( 0, \\tau_{00} ) \\\\\n\\end{aligned}$$\n\nThe reduced form is $$\\begin{aligned}\n Y_{ti} &= \\beta_{0}  + \\beta_{1}  a_t  + r_{0i} + \\epsilon_{ti} \\\\ \n &= \\beta_{0}  + \\beta_{1}  a_t  + u_{ti} \n\\end{aligned}$$ with $u_{ti} = r_{0i} + \\epsilon_{ti}$.\n\nNote that $\\epsilon_{ti}$ is the specific time-individual residual after the individual random effects, and $u_{ti}$ is the *overall* residual (deviation from what we expect from the population, or the difference between our observed outcome and the *population* model, not student latent growth curve).\n\nUsing our model, let's calculate some variances and covariances of the residuals.\n\nFirst the variance of a residual at time point $t$: $$\n\\begin{aligned}\nvar( u_{ti} ) &= var( r_{i} + \\epsilon_{ti} ) \\\\\n  &= var( r_{i} ) + var( \\epsilon_{ti} ) + cov( r_i, \\epsilon_{ti} ) \\\\\n  &= \\tau_{00} + \\sigma^2 \n\\end{aligned}\n$$ because the residuals are independent, so all covariances of different residuals, such as $r_i$ and $\\epsilon_{ti}$ are 0.\nThe second line is using the identity $Var( A + B ) = Var( A ) + Var( B ) + 2 Cov( A, B )$.\n\nSecond, the covariance of $u_{1i}$ and $u_{2i}$, i.e., time 1 and time 2 for the same person: $$\n\\begin{aligned}\ncov( u_{1i}, u_{2i} ) &= cov( r_{i} + \\epsilon_{1i}, r_{i} + \\epsilon_{2i},  ) \\\\\n  &= cov( r_{i}, r_{i} ) + cov( r_{i}, \\epsilon_{2i} ) + cov( \\epsilon_{1i}, r_i ) + cov( \\epsilon_{1i}, \\epsilon_{2i} ) \\\\\n  &= \\tau_{00}\n\\end{aligned}\n$$ The last bit is again because the covariances of different residuals are 0.\nThe covariance of something with itself is just the variance.\nThe second line comes from $$cov( A + B, C + D ) = cov( A, C ) + cov( A, D ) + cov( B, C ) + cov( B, D ),$$ i.e., you multiply all the bits out.\nThe above clearly generalizes so the covariance of any two time points within a student has covariance of $\\tau_{00}$.\n\nFinally, looking at two different students, we have $$\n\\begin{aligned}\ncov( u_{ti}, u_{t'j} ) &= cov( r_{i} + \\epsilon_{ti}, r_{j} + \\epsilon_{t'j},  ) \\\\\n  &= cov( r_{i}, r_{j} ) + cov( r_{i}, \\epsilon_{t'j} ) + cov( \\epsilon_{ti}, r_j ) + cov( \\epsilon_{ti}, \\epsilon_{t'j} ) \\\\\n  &= 0 ,\n\\end{aligned}\n$$ because all of the residuals are independent, according to our model.\nThis says that all our population residuals from different students are not correlated.\nThis gives us our block diagonal structure on our $N \\times N$ matrix of residuals.\nFor student $i$, the first two results tell us that: $$\\begin{aligned}\n\\begin{pmatrix} u_{i1} \\\\\nu_{i2} \\\\\nu_{i3} \\\\\nu_{i4} \\\\\nu_{i5}\n\\end{pmatrix} &\\sim  N\n\\begin{bmatrix}\n\\begin{pmatrix}\n0 \\\\\n0 \\\\\n0\\\\\n0 \\\\\n0\n\\end{pmatrix}\\!\\!,&\n\\begin{pmatrix}\n\\tau_{00} + \\sigma^2 & \\tau_{00} & \\tau_{00} & \\tau_{00} & \\tau_{00} \\\\\n           & \\tau_{00} + \\sigma^2 & \\tau_{00} & \\tau_{00} & \\tau_{00} \\\\\n         &              & \\tau_{00} + \\sigma^2 & \\tau_{00} & \\tau_{00} \\\\\n         &              &             & \\tau_{00} + \\sigma^2 & \\tau_{00} \\\\\n         &              &              &            & \\tau_{00} + \\sigma^2 \n\\end{pmatrix}\n\\end{bmatrix}\n\\end{aligned} .$$\n\nOur multilevel model has given us a specific structure for our student-level residual covariance matrix $\\Sigma_i$.\nWe could just fit a regression at the population level with this matrix specified, without talking about random intercepts or anything.\nWe can also tweak this matrix in ways that capture other kinds of variation.\nThis is the key to this approach to modeling clustered or non-independent data.\n\nIn the next section we repeat this for a random slope model.\nSame idea, more messy math.\n\n## And now for a random slope model\n\nTake a random slopes model with 5 timepoints (this is the NYS model, each time point is a year of age, 11--15):\n\n$$\n\\begin{aligned}\nY_{ti} &= \\pi_{0i} + \\pi_{1i} age_{ti} + \\epsilon_{ti} \\\\\n\\pi_{0i} &=  \\beta_{0} + r_{0i} \\\\\n\\pi_{1i} &= \\beta_{1} + r_{1i} \\\\\n\\begin{pmatrix} r_{0j} \\\\\nr_{1j}\n\\end{pmatrix} &\\sim  N\n\\begin{bmatrix}\n\\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix}\\!\\!,&\n\\begin{pmatrix}\n\\tau_{00} & \\tau_{01} \\\\\n        & \\tau_{11} \n\\end{pmatrix}\n\\end{bmatrix}\n\\end{aligned}\n$$\n\nLet $\\epsilon_i \\sim N(0, \\sigma^2)$.\nLet our intercept correspond to our first timepoint, so $a_1 = 0, a_2 = 1, ..., a_5 = 4$.\nI.e., our $age_{ti}$ is number of years since onset of study.\nThen $\\beta_{0}$ is the average outcome at onset of the study and $\\beta_{1}$ is the rate of growth (per year) in the population.\n\nThe reduced form is $$\\begin{aligned}\n Y_{ti} &= \\beta_{0}  + \\beta_{1}  age_{ti}  + r_{0i} + r_{1i} age_{ti} + \\epsilon_{ti} \\\\ \n &= \\beta_{0}  + \\beta_{1}  age_{ti}  + u_{ti} \n\\end{aligned}$$ with $u_{ti} = r_{0i} + r_{1i} age_{ti} + \\epsilon_{ti}$.\n\nNow let's use this definition of $u_{ti}$ to calculate all the $\\delta_{tt'}$ values in the student level covariance matrix $\\Sigma_i$.\n\n### Calculating the $\\delta_{tt'}$ {#calculating-the-delta_tt .unnumbered}\n\nLet's calculate $\\delta_{13} = cov( \\epsilon_{i1}, \\epsilon_{i2} )$.\n\nFirst we need a math fact about random quantities $A$, $B$, and $C$: $$cov( A + B, C ) = cov( A, C ) + cov( B, C ) .$$ Also if you multiply something by a constant $k$ you have $$cov( k_1 A, k_2 B ) = k_1 k_2 cov( A, B ) .$$\n\nAlso note that $a_1 = 0$ and $a_3 = 2$, given our coding of age (\\$a_1\\$ is the time covariate at age 11, which is 0, for example).\nThen we have, plugging in those values: $$\\begin{aligned}\n\\delta_{13} &= cov( u_{i1}, u_{i3} ) \\\\\n   &= cov(  r_{0i} + r_{1i} a_1 + \\epsilon_{1i},  r_{0i} + r_{1i} a_3 + \\epsilon_{3i} ) \\\\\n   &= cov(  r_{0i}  + \\epsilon_{1i},  r_{0i} + 2 r_{1i} + \\epsilon_{3i} ) \\\\\n   &= cov(  r_{0i}, r_{0i} ) + cov( r_{0i}, 2 r_{1i} ) + cov( r_{0i}, \\epsilon_{3i} ) + cov( \\epsilon_{1i}, r_{0i}) + cov( \\epsilon_{1i}, 2 r_{1i} )  + cov( \\epsilon_{1i}, \\epsilon_{3i}) \\\\\n   &= \\tau_{00} + 2\\tau_{01} + 0 + 0 + 0 + 0 \\\\\n   &= \\tau_{00} + 2\\tau_{01} \n\\end{aligned}$$\n\nNote how we multiple out the individual components, and this gives an expression for the overall covariance of our two residuals.\nIf we did this for each $\\delta_{tt'}$ we could fill in our $5 \\times 5$ matrix.\nFun!\n\nA core idea here is the independence of the different residual pieces makes a lot of the terms go to 0, giving short(er) expressions than we might have otherwise.\nThe random slope model dictates the overall covariance of the residuals.\n\n### Calculating the diagonal terms.\n\nFor the variances, you would just calculate covariance of a quantity with itself.\nLet's do $\\delta_{11}$, the variance of timepoint 1: $$\\begin{aligned}\n\\delta_{11} &= var( u_{1i} ) = cov( u_{1i}, u_{1i} ) \\\\\n   &= cov(  r_{0i} + r_{1i} a_1 + \\epsilon_{1i},  r_{0i} + r_{1i} a_1 + \\epsilon_{1i} ) \\\\\n   &= cov(  r_{0i}  + \\epsilon_{1i},  r_{0i} + \\epsilon_{1i} ) \\\\\n   &= cov(  r_{0i}, r_{0i} ) + cov( r_{0i},  \\epsilon_{1i} ) + cov( \\epsilon_{1i}, r_{0i}) + cov( \\epsilon_{1i},\\epsilon_{1i} )  \\\\\n   &= \\tau_{00} + 0 + 0 + \\sigma^2 =  \\tau_{00} + \\sigma^2\n\\end{aligned}$$\n\nNow let's do $\\delta_{55}$, the variance of timepoint 5: $$\\begin{aligned}\n\\delta_{55} &= var( u_{5i} ) = cov( u_{5i}, u_{5i} ) \\\\\n   &= cov(  r_{0i} + r_{1i} a_5 + \\epsilon_{5i},  r_{0i} + r_{5i} a_5 + \\epsilon_{5i} ) \\\\\n   &= cov(  r_{0i} + 4 r_{1i} + \\epsilon_{5i},  r_{0i} + 4 r_{1i} + \\epsilon_{5i} ) \\\\\n   &= cov(  r_{0i}, r_{0i} ) + cov(  r_{0i}, 4 r_{1i} )  + cov( r_{0i},  \\epsilon_{5i} ) + \\\\\n    &\\qquad cov( 4 r_{1i}, r_{0i} ) + cov( 4 r_{1i}, 4 r_{1i} )  + cov( 4 r_{1i}, \\epsilon_{5i} ) \\\\\n    & \\qquad cov( \\epsilon_{1i}, r_{0i}) + cov( \\epsilon_{1i}, 4 r_{1i} )  + cov( \\epsilon_{1i},\\epsilon_{1i} )  \\\\\n   &= \\tau_{00} + 4 \\tau_{01} + 0 + 4 \\tau_{01} + 16 \\tau_{11} + 0 + 0 + 0 + \\sigma^2 \\\\\n   &= \\tau_{00} + 16 \\tau_{11} + 8 \\tau_{01} + \\sigma^2 .\n\\end{aligned}$$\n\nNote how the variance around the intercept (at time 1 where $a_1 = 0$) looks like it would be smaller than the variance further out.\nThat being said, the covariance $\\tau_{01}$ could be large and negative, causing the variance at the intercept to be less.\nBut, if $\\tau_{01}$ is positive, the overall variance increases as we move away from the intercept point.\n\nOne interesting aspect of random slope models is the marginal (at each time point) variance changes at each time point.\nThis is heteroskedasticity: the variances are each time point can be different because the lines can spread or gather.\n","srcMarkdownNoYaml":"\n\nIn this chapter we lay out some of the derivations on residual matrices.\nWe use the running example of the NYS data (see Packet 7).\n\n## The student-level residual matrix\n\nFollowing Packet 7.1, let's think about a generic regression equation for a linear growth model with 5 timepoints (this is a simplified version of the NYS model, where each time point is a year of age, 11--15).\n\nIn particular, consider $$\nY_{ti} = \\beta_0 + \\beta_1 age_{ti} + u_{ti}\n$$ where $age_{ti}$ is our age from 11 (so an observation at 11 years old would have `age11 = 0`).\nThis means our intercept correspond to our first timepoint, with $a_1 = 0, a_2 = 1, ..., a_5 = 4$.\nI.e., our $age_{ti}$ is number of years since onset of study.\n\nIn this model, $\\beta_{0}$ is the average outcome across our population at the onset of the study and $\\beta_{1}$ is the average rate of growth (per year) in the population.\n\nNow we have 5 observations for each student $i$, so the residuals $(u_{1i}, \\ldots, u_{5i})$ are likely correlated with each other.\nFor example, a student might just generally have higher levels of outcome, or lower levels, which means the overall residual of one time point would be related to the reisduals of other time points.\nIn math, we can write that for any randomly subject $i$, the covariance matrix of their residuals is $$\\begin{aligned}\n\\begin{pmatrix} u_{i1} \\\\\nu_{i2} \\\\\nu_{i3} \\\\\nu_{i4} \\\\\nu_{i5}\n\\end{pmatrix} &\\sim  N\n\\begin{bmatrix}\n\\begin{pmatrix}\n0 \\\\\n0 \\\\\n0\\\\\n0 \\\\\n0\n\\end{pmatrix}\\!\\!,&\n\\begin{pmatrix}\n\\delta_{11} & \\delta_{12} & \\delta_{13} & \\delta_{14} & \\delta_{15} \\\\\n           & \\delta_{22} & \\delta_{23} & \\delta_{24} & \\delta_{25} \\\\\n         &              & \\delta_{33} & \\delta_{34} & \\delta_{35} \\\\\n         &              &             & \\delta_{44} & \\delta_{45} \\\\\n         &              &              &            & \\delta_{55} \n\\end{pmatrix}\n\\end{bmatrix}\n\\end{aligned} = N( 0, \\Sigma_i)$$\n\nThis matrix of residuals for student $i$ is one of the blocks in our $N \\times N$ block-diagonal matrix for all our residuals (this would be the giant matrix plugged into our sandwich formula to get standard errors for $\\beta_0$ and $\\beta_1$), where $N$ is the number of observations.\nAssuming 5 observations per student, multilevel and generalized linear modeling (which we are talking about here) make the assumption that this matrix is the same across students; cluster robust standard errors would not make this assumption.\n(More broadly, MLM and generalized linear modeling make the assumption that we can represent all the $\\Sigma_i$ in terms of measured covariates and pre-specified parameters, but in this case we end up with the same matrix for all students with 5 time points. Students with fewer than 5 would be subsets of this matrix corresponding to the time points observed.)\n\nThe diagonal of $\\Sigma_i$ are our variances at each timepoint (this means, for example, that if our model is good, that if we took the variance of all the observations where $t=5$ across our dataset we should get something close to $\\delta_{55}$).\n\nIn the remainder of this document, we look at how MLM gives expressions for this matrix.\n\n## Covariance matrix for a random intercept model\n\nFollowing Packet 7.1, we start with a random intercept model with a completely pooled growth component with 5 timepoints (this is a simplified version of the NYS model, where each time point is a year of age, 11--15).\nIn particular, take the model represented by this `lmer()` command:\n\n```{r, eval=FALSE}\nM = lmer( Y ~ 1 + age11 + (1|id), data=nys )\n```\n\nwhere `age11` is our age from 11 (so an observation at 11 years old would have `age11 = 0`).\n\nIn math, this model is $$\\begin{aligned}\nY_{ti} &= \\pi_{0i} + \\beta_{1} age_ + \\epsilon_{ti} \\\\\n\\epsilon_{ti} &\\sim N( 0, \\sigma^2 ) \\\\\n\\pi_{0i} &=  \\beta_{0} + r_{0i} \\\\\n r_{0j} & \\sim N( 0, \\tau_{00} ) \\\\\n\\end{aligned}$$\n\nThe reduced form is $$\\begin{aligned}\n Y_{ti} &= \\beta_{0}  + \\beta_{1}  a_t  + r_{0i} + \\epsilon_{ti} \\\\ \n &= \\beta_{0}  + \\beta_{1}  a_t  + u_{ti} \n\\end{aligned}$$ with $u_{ti} = r_{0i} + \\epsilon_{ti}$.\n\nNote that $\\epsilon_{ti}$ is the specific time-individual residual after the individual random effects, and $u_{ti}$ is the *overall* residual (deviation from what we expect from the population, or the difference between our observed outcome and the *population* model, not student latent growth curve).\n\nUsing our model, let's calculate some variances and covariances of the residuals.\n\nFirst the variance of a residual at time point $t$: $$\n\\begin{aligned}\nvar( u_{ti} ) &= var( r_{i} + \\epsilon_{ti} ) \\\\\n  &= var( r_{i} ) + var( \\epsilon_{ti} ) + cov( r_i, \\epsilon_{ti} ) \\\\\n  &= \\tau_{00} + \\sigma^2 \n\\end{aligned}\n$$ because the residuals are independent, so all covariances of different residuals, such as $r_i$ and $\\epsilon_{ti}$ are 0.\nThe second line is using the identity $Var( A + B ) = Var( A ) + Var( B ) + 2 Cov( A, B )$.\n\nSecond, the covariance of $u_{1i}$ and $u_{2i}$, i.e., time 1 and time 2 for the same person: $$\n\\begin{aligned}\ncov( u_{1i}, u_{2i} ) &= cov( r_{i} + \\epsilon_{1i}, r_{i} + \\epsilon_{2i},  ) \\\\\n  &= cov( r_{i}, r_{i} ) + cov( r_{i}, \\epsilon_{2i} ) + cov( \\epsilon_{1i}, r_i ) + cov( \\epsilon_{1i}, \\epsilon_{2i} ) \\\\\n  &= \\tau_{00}\n\\end{aligned}\n$$ The last bit is again because the covariances of different residuals are 0.\nThe covariance of something with itself is just the variance.\nThe second line comes from $$cov( A + B, C + D ) = cov( A, C ) + cov( A, D ) + cov( B, C ) + cov( B, D ),$$ i.e., you multiply all the bits out.\nThe above clearly generalizes so the covariance of any two time points within a student has covariance of $\\tau_{00}$.\n\nFinally, looking at two different students, we have $$\n\\begin{aligned}\ncov( u_{ti}, u_{t'j} ) &= cov( r_{i} + \\epsilon_{ti}, r_{j} + \\epsilon_{t'j},  ) \\\\\n  &= cov( r_{i}, r_{j} ) + cov( r_{i}, \\epsilon_{t'j} ) + cov( \\epsilon_{ti}, r_j ) + cov( \\epsilon_{ti}, \\epsilon_{t'j} ) \\\\\n  &= 0 ,\n\\end{aligned}\n$$ because all of the residuals are independent, according to our model.\nThis says that all our population residuals from different students are not correlated.\nThis gives us our block diagonal structure on our $N \\times N$ matrix of residuals.\nFor student $i$, the first two results tell us that: $$\\begin{aligned}\n\\begin{pmatrix} u_{i1} \\\\\nu_{i2} \\\\\nu_{i3} \\\\\nu_{i4} \\\\\nu_{i5}\n\\end{pmatrix} &\\sim  N\n\\begin{bmatrix}\n\\begin{pmatrix}\n0 \\\\\n0 \\\\\n0\\\\\n0 \\\\\n0\n\\end{pmatrix}\\!\\!,&\n\\begin{pmatrix}\n\\tau_{00} + \\sigma^2 & \\tau_{00} & \\tau_{00} & \\tau_{00} & \\tau_{00} \\\\\n           & \\tau_{00} + \\sigma^2 & \\tau_{00} & \\tau_{00} & \\tau_{00} \\\\\n         &              & \\tau_{00} + \\sigma^2 & \\tau_{00} & \\tau_{00} \\\\\n         &              &             & \\tau_{00} + \\sigma^2 & \\tau_{00} \\\\\n         &              &              &            & \\tau_{00} + \\sigma^2 \n\\end{pmatrix}\n\\end{bmatrix}\n\\end{aligned} .$$\n\nOur multilevel model has given us a specific structure for our student-level residual covariance matrix $\\Sigma_i$.\nWe could just fit a regression at the population level with this matrix specified, without talking about random intercepts or anything.\nWe can also tweak this matrix in ways that capture other kinds of variation.\nThis is the key to this approach to modeling clustered or non-independent data.\n\nIn the next section we repeat this for a random slope model.\nSame idea, more messy math.\n\n## And now for a random slope model\n\nTake a random slopes model with 5 timepoints (this is the NYS model, each time point is a year of age, 11--15):\n\n$$\n\\begin{aligned}\nY_{ti} &= \\pi_{0i} + \\pi_{1i} age_{ti} + \\epsilon_{ti} \\\\\n\\pi_{0i} &=  \\beta_{0} + r_{0i} \\\\\n\\pi_{1i} &= \\beta_{1} + r_{1i} \\\\\n\\begin{pmatrix} r_{0j} \\\\\nr_{1j}\n\\end{pmatrix} &\\sim  N\n\\begin{bmatrix}\n\\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix}\\!\\!,&\n\\begin{pmatrix}\n\\tau_{00} & \\tau_{01} \\\\\n        & \\tau_{11} \n\\end{pmatrix}\n\\end{bmatrix}\n\\end{aligned}\n$$\n\nLet $\\epsilon_i \\sim N(0, \\sigma^2)$.\nLet our intercept correspond to our first timepoint, so $a_1 = 0, a_2 = 1, ..., a_5 = 4$.\nI.e., our $age_{ti}$ is number of years since onset of study.\nThen $\\beta_{0}$ is the average outcome at onset of the study and $\\beta_{1}$ is the rate of growth (per year) in the population.\n\nThe reduced form is $$\\begin{aligned}\n Y_{ti} &= \\beta_{0}  + \\beta_{1}  age_{ti}  + r_{0i} + r_{1i} age_{ti} + \\epsilon_{ti} \\\\ \n &= \\beta_{0}  + \\beta_{1}  age_{ti}  + u_{ti} \n\\end{aligned}$$ with $u_{ti} = r_{0i} + r_{1i} age_{ti} + \\epsilon_{ti}$.\n\nNow let's use this definition of $u_{ti}$ to calculate all the $\\delta_{tt'}$ values in the student level covariance matrix $\\Sigma_i$.\n\n### Calculating the $\\delta_{tt'}$ {#calculating-the-delta_tt .unnumbered}\n\nLet's calculate $\\delta_{13} = cov( \\epsilon_{i1}, \\epsilon_{i2} )$.\n\nFirst we need a math fact about random quantities $A$, $B$, and $C$: $$cov( A + B, C ) = cov( A, C ) + cov( B, C ) .$$ Also if you multiply something by a constant $k$ you have $$cov( k_1 A, k_2 B ) = k_1 k_2 cov( A, B ) .$$\n\nAlso note that $a_1 = 0$ and $a_3 = 2$, given our coding of age (\\$a_1\\$ is the time covariate at age 11, which is 0, for example).\nThen we have, plugging in those values: $$\\begin{aligned}\n\\delta_{13} &= cov( u_{i1}, u_{i3} ) \\\\\n   &= cov(  r_{0i} + r_{1i} a_1 + \\epsilon_{1i},  r_{0i} + r_{1i} a_3 + \\epsilon_{3i} ) \\\\\n   &= cov(  r_{0i}  + \\epsilon_{1i},  r_{0i} + 2 r_{1i} + \\epsilon_{3i} ) \\\\\n   &= cov(  r_{0i}, r_{0i} ) + cov( r_{0i}, 2 r_{1i} ) + cov( r_{0i}, \\epsilon_{3i} ) + cov( \\epsilon_{1i}, r_{0i}) + cov( \\epsilon_{1i}, 2 r_{1i} )  + cov( \\epsilon_{1i}, \\epsilon_{3i}) \\\\\n   &= \\tau_{00} + 2\\tau_{01} + 0 + 0 + 0 + 0 \\\\\n   &= \\tau_{00} + 2\\tau_{01} \n\\end{aligned}$$\n\nNote how we multiple out the individual components, and this gives an expression for the overall covariance of our two residuals.\nIf we did this for each $\\delta_{tt'}$ we could fill in our $5 \\times 5$ matrix.\nFun!\n\nA core idea here is the independence of the different residual pieces makes a lot of the terms go to 0, giving short(er) expressions than we might have otherwise.\nThe random slope model dictates the overall covariance of the residuals.\n\n### Calculating the diagonal terms.\n\nFor the variances, you would just calculate covariance of a quantity with itself.\nLet's do $\\delta_{11}$, the variance of timepoint 1: $$\\begin{aligned}\n\\delta_{11} &= var( u_{1i} ) = cov( u_{1i}, u_{1i} ) \\\\\n   &= cov(  r_{0i} + r_{1i} a_1 + \\epsilon_{1i},  r_{0i} + r_{1i} a_1 + \\epsilon_{1i} ) \\\\\n   &= cov(  r_{0i}  + \\epsilon_{1i},  r_{0i} + \\epsilon_{1i} ) \\\\\n   &= cov(  r_{0i}, r_{0i} ) + cov( r_{0i},  \\epsilon_{1i} ) + cov( \\epsilon_{1i}, r_{0i}) + cov( \\epsilon_{1i},\\epsilon_{1i} )  \\\\\n   &= \\tau_{00} + 0 + 0 + \\sigma^2 =  \\tau_{00} + \\sigma^2\n\\end{aligned}$$\n\nNow let's do $\\delta_{55}$, the variance of timepoint 5: $$\\begin{aligned}\n\\delta_{55} &= var( u_{5i} ) = cov( u_{5i}, u_{5i} ) \\\\\n   &= cov(  r_{0i} + r_{1i} a_5 + \\epsilon_{5i},  r_{0i} + r_{5i} a_5 + \\epsilon_{5i} ) \\\\\n   &= cov(  r_{0i} + 4 r_{1i} + \\epsilon_{5i},  r_{0i} + 4 r_{1i} + \\epsilon_{5i} ) \\\\\n   &= cov(  r_{0i}, r_{0i} ) + cov(  r_{0i}, 4 r_{1i} )  + cov( r_{0i},  \\epsilon_{5i} ) + \\\\\n    &\\qquad cov( 4 r_{1i}, r_{0i} ) + cov( 4 r_{1i}, 4 r_{1i} )  + cov( 4 r_{1i}, \\epsilon_{5i} ) \\\\\n    & \\qquad cov( \\epsilon_{1i}, r_{0i}) + cov( \\epsilon_{1i}, 4 r_{1i} )  + cov( \\epsilon_{1i},\\epsilon_{1i} )  \\\\\n   &= \\tau_{00} + 4 \\tau_{01} + 0 + 4 \\tau_{01} + 16 \\tau_{11} + 0 + 0 + 0 + \\sigma^2 \\\\\n   &= \\tau_{00} + 16 \\tau_{11} + 8 \\tau_{01} + \\sigma^2 .\n\\end{aligned}$$\n\nNote how the variance around the intercept (at time 1 where $a_1 = 0$) looks like it would be smaller than the variance further out.\nThat being said, the covariance $\\tau_{01}$ could be large and negative, causing the variance at the intercept to be less.\nBut, if $\\tau_{01}$ is positive, the overall variance increases as we move away from the intercept point.\n\nOne interesting aspect of random slope models is the marginal (at each time point) variance changes at each time point.\nThis is heteroskedasticity: the variances are each time point can be different because the lines can spread or gather.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","highlight-style":"pygments","output-file":"cov_matrix_derivation.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.353","bibliography":["references.bib"],"output_dir":"docs","editor":{"markdown":{"wrap":"sentence"}},"theme":"cosmo","code-copy":true,"title":"Covariance Derivation","author":"Luke Miratrix"},"extensions":{"book":{"multiFile":true}}},"pdf":{"identifier":{"display-name":"PDF","target-format":"pdf","base-format":"pdf"},"execute":{"fig-width":5.5,"fig-height":3.5,"fig-format":"pdf","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"pdf","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"pdf-engine":"xelatex","standalone":true,"variables":{"graphics":true,"tables":true},"default-image-extension":"pdf","to":"pdf","highlight-style":"pygments","output-file":"cov_matrix_derivation.pdf"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"block-headings":true,"bibliography":["references.bib"],"output_dir":"docs","editor":{"markdown":{"wrap":"sentence"}},"documentclass":"scrreprt","title":"Covariance Derivation","author":"Luke Miratrix"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","pdf"]}