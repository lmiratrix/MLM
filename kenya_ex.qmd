---
title: "Example of a three-level longitudinal model"
author: "Luke Miratrix"
editor: 
  markdown: 
    wrap: sentence
---

This will illustrate fitting a three level model (where we have time variation and then clusters) and extracting the various components from it.
This is a rough document, but hopefully will be useful.
This example is based on a dataset used in Rabe-Hesketh and Skrondal, chapter 8.10, but you don't really need that text.

```{r loadlibs, echo=FALSE, message=FALSE, warning=FALSE}
library( tidyverse )
library( lme4 )
library(foreign)
library(ggplot2)
library( arm )
options( digits=3 )
```

## Load the data

We first load the data.
Shoving a lot of things under the rug, we have five measurements on a collection of kids in Kenya across time.
We are interested in the impact of improved nutrition.
The children are clustered in schools.
This gives a three-level structure.
The schools were treated with different nutrition programs.

In the following we load the data and look at the first few lines.
Lots of variables!
The main ones are id (the identifier of the kid), treatment (the kind of treatment given to the school), schoolid (the identifier of the school), gender (the gender of the kid), and rn (the time variable).
Our outcome is ravens (Raven's colored progressive matrices asssessment).

```{r}
kenya = read.dta( "data/kenya.dta" )

# look at first 9 variables
head( kenya[1:9], 3 )

# what times do we have?
table( kenya$rn ) #time

length( unique( kenya$id ) )
length( unique( kenya$schoolid) )
```

We see we have 546 kids and 12 schools.

## Plot the data

We can look at the data.

```{r}
ggplot( data=kenya, aes( x=rn, y=ravens, group=id )  )+ 
            facet_wrap( ~ gender ) + 
            geom_line( alpha=0.3 )
```

or

```{r}
ggplot( data=kenya, aes( x=rn, y=ravens, group=id )  )+ 
            facet_wrap( ~ schoolid ) + 
            geom_line( alpha=0.3 )
```

```{r}
id.sub = sample( unique( kenya$id), 12 )
ken.sub = subset( kenya, id %in% id.sub )
ggplot( data=ken.sub, aes( x=rn, y=ravens, group=id )  )+ 
            facet_wrap( ~ id ) + 
            geom_line( alpha=0.3 )
```

We have lots of noise!
But there is also a trend.

The progression of marginal means show there is growth over time, on average:

```{r}
mosaic::favstats( ravens ~ rn, data=kenya )
```

(Using the mosaic package we can do this.)

The above also shows that we have some missing data, more as the study progresses.

We drop these missing observations:

```{r}
kenya = subset( kenya, !is.na( ravens ) & !is.na( rn ) )
```

We have some treatments, which we order so control is first

```{r}
str( kenya$treatment )
levels( kenya$treatment )
kenya$treatment = relevel( kenya$treatment, ref = "control" )
levels( kenya$treatment )
```

## The mathematical model

Let's fit a random slope model.

*Level 1:* We have for individual $i$ in school $j$ at time $t$: \[ Y\_{ijt} = \beta*{0ij} +* \beta{1ij} (t-L) + \epsilon\_{ijt} \]

*Level 2:* Each individual has their own growth curve.
Their curve's slope and intercepts varies around the school means: \[ \beta*{0ij} =* \gamma{00j} + \gamma*{01} gender*{ij} + u\_{0ij} \] \[ \beta*{1ij} =* \gamma{10j} + \gamma*{11} gender*{ij} + u\_{1ij} \] We also have that $(u_{0ij}, u_{1ij})$ are normally distributed with some 2x2 covariance matrix.
We are forcing the impact of gender to be constant across schools.

*Level 3:* Finally our school mean slope and intercepts are \[ \gamma*{0j} =* \mu{00} + w\_{0i} \] \[ \gamma*{1j} =* \mu{10} + \mu*{11} meat_j +* \mu{12} milk_j + \mu*{13} calorie_j + w*{1i} \] For the rate of growth at a school we allow different slopes for different treatments (compared to baseline).
The milk, meat, and calorie are the three different treatments applied.
We also have that $(w_{0j}, w_{1j})$ are normally distributed with some 2x2 covariance matrix: \[

```{=tex}
\begin{pmatrix}w_{j0}\\
w_{j1}
\end{pmatrix}
```
\sim N\left(\left(

```{=tex}
\begin{array}{c}
0 \\
0
\end{array}
```
\right), \left[ 
\begin{array}{cc}
\tau_{11} & \tau_{12} \\
\tau_{12} & \tau_{22} 
\end{array}
\right]\right) = \sim N\left(\left(

```{=tex}
\begin{array}{c}
0 \\
0
\end{array}
```
\right), \Sigma\_{sch} \right) \]

The $\mu_0$ and $\mu_1$ are the slope and intercept for the overall population growth (this is what defines our marginal model).

We will use $L = 1$ to center the data at the first time point (so our intercept is expected ravens score at onset of the study).

*Conceptual question:* Why do we not have treatment in the intercept for school?
What would changing $L$ do to our model and this reasoning?

## Fit the model

```{r}
library( lme4 )
kenya$rn = kenya$rn - 1 # center by L=1
M1 = lmer( ravens ~ 1 + rn + gender*rn + treatment:rn + (1+rn|schoolid) + (1+rn|id:schoolid), 
           data=kenya )
display( M1 )
```

Now let's connect some pieces:

-   $\mu_{00} = 17.41$ and $\mu_{11} = 0.59$. The initial score for boys is 17.4, on average, with an average gain of 0.59 per year for control schools.
-   $\gamma_{01} = -0.30$ and $\gamma_{11} = -0.14$, giving estimates that girls score lower and gain slower than boys.
-   The school-level variation in initial expected Raven scores is 0.45 (this is the standard deviation of $w_{0i}$), relatively small compared to the individual variation of 1.40 (this is the standard deviation of $u_{0ij}$).
-   The correlation of the $u_{0ij}$ and $u_{1ij}$ is basically zero (estimated at -0.09).
-   The random effects for school has a covariance matrix $\Sigma_{sch}$ of \[ \widehat{\Sigma}\_{sch} = \left[ 
    \begin{array}{cc}
    0.45^2 & 0.45 \times 0.09 \times -0.99 \\
    . & 0.09^2 
    \end{array} 
    \right] \] The very negative correlation suggests an extrapolation effect, and that perhaps we could drop the random slope for schools.
-   The treatment effects are estimated as $\mu_{11}=0.17, \mu_{12}=-0.13$, and $\mu_{13}=-0.02$.\
-   P-values for these will not be small, however, as the standard errors are all 0.09.

We could try to look at uncertainty on our parameters using the `confint( M1 )` command, but it turns out that it crashes for this model.
This can happen, and our -0.99 correlation gives a hint as to why.
Let's first drop the random slope and then try:

```{r, cache=TRUE, warning=FALSE, message=FALSE }
M1B = lmer( ravens ~ rn + gender*rn + treatment:rn + (1|schoolid) + (1+rn|id:schoolid), 
           data=kenya )
display( M1B )
confint( M1B )
```

We then have to puzzle out which confidence interval goes with what.
The `.sig01` is the variance of the kid (`id:schoolid`), which we can tell by the range it covers.
Then the next must be correlation, and then the slope.
This tells us we have no confidence the school random intercept is away from 0 (`.sig04`).

## Some quick plots

We can look at the empirical bayes intercepts:

```{r}
schools = data.frame( resid = ranef( M1 )$schoolid$`(Intercept)` )
kids = data.frame( resid = ranef( M1 )$id$`(Intercept)` )
resid = data.frame( resid = resid( M1 ) )
resids = bind_rows( school=schools, child=kids, residual=resid, .id="type" )
resids$type = factor( resids$type, levels = c("school","child", "residual" ) )

ggplot( resids, aes( x = type, y = resid ) ) +
  geom_boxplot() +
  coord_flip()
```

This shows that the variation in occasion is much larger than kid, which is much larger than school.

We can calculate all the individual growth curves and plot those:

```{r}
kenya$predictions = predict( M1 )
ggplot( data=kenya, aes( x=rn, y=predictions, group=id ) ) +
    facet_wrap( ~ schoolid, labeller = label_both) +
    geom_line( alpha=0.3 )
```

Generally individual curves are estimated to have positive slopes.
The schools visually look quite similar; any school variation is small compared to individual variation.
