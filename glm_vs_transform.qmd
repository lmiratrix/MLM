---
title: "GLMs vs. Transformations"
author: "Josh Gilbert"
editor: 
  markdown: 
    wrap: sentence
---

```{r, echo = FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)

set.seed(2023)
```

Those of you coming from S40 and S52 may recall that when we have non-linear relationships between X and Y, we can apply a transformation, such as taking the log, to linearize the relationship.
In the words of Jimmy Kim, "with transformations, we use the *machinery of linear regression to model non-linear relationships*".
If that's the case, then what is Poisson regression about, which deals with log counts?
This is a topic that confused me for many years so hopefully I can clear it up here.

## Making and Graphing the Data

Let's start by making some fake data.
Here's the data-generating function, which suggests that a 1-unit increase in `x` will increase the expected count by $e^.5 = 1.65$.

$$
y = Poisson(e^{0.5x})
$$

In the graph, we can see that the relationship between `x` and `y` is clearly non linear!

```{r}
library(tidyverse)
library(sjPlot)
library(ggeffects)

theme_set(theme_classic())

rm(list = ls())

dat <- tibble(
  x = runif(1000, 0, 5),
  y = rpois(1000, exp(0.5*x))
)

ggplot(dat, aes(x = x, y = y)) +
  geom_point() +
  geom_smooth(se = FALSE)
```

Let's plot `log_y + 1` on `x`.
Amazing!
The relationship is basically linear, which suggests that a 1-unit increase in `x` has some multiplicative effect on `y`.

```{r}
ggplot(dat, aes(x = x, y = log(y + 1))) +
  geom_point() +
  geom_smooth(se = FALSE)
```

## Fitting the Regression Models

Let's use OLS and Poisson regression to fit the data.
We see a few things:

1.  The Poisson model fits drastically better, both in terms of $R^2$ and the coefficients are close to the data-generating values
2.  The transformed OLS model understates the slope
3.  Both models have (seemingly) similar interpretations: a 1-unit increase in `x` causes an $e^\beta$ increase in `y`. How is this possible?

So what's going on?

The answer is that there is a very subtle difference between transformed OLS and Poisson regression.
In transformed OLS, we are modeling the mean of the log of Y, or $E(ln(y|x))$.
In Poisson, we're modeling the log of the mean of Y, or $ln(E(y|x))$.
These are not equivalent!
In essence, Poisson regression is a model for the arithmetic mean, whereas OLS is a model for the geometric mean.
This means that when we exponentiate the Poisson model, we can get predicted counts, but this is *not* true of the OLS model.

```{r}
m1 <- lm(log(y + 1) ~ x, dat)
m2 <- glm(y ~ x, dat, family = poisson)

tab_model(m1, m2,
          p.style = "stars",
          show.ci = FALSE,
          show.se = TRUE,
          digits = 3,
          transform = NULL,
          dv.labels = c("Log(Y+1)", "Poisson"))
```

## More Intuition: An Example with Means

Let's create a super simple data set, `s`.

```{r}
s <- c(1, 10, 100)
```

It's clearly skewed.
But I can still take the mean.
I could take the arithmetic mean, or the geometric mean.
These are clearly different quantities.

```{r}
mean(s) # arithmetic
exp(mean(log((s)))) # geometric
```

The idea of Poisson is to take the log of the mean and fit a linear model for that:

```{r}
log_mean <- log(mean(s))
log_mean
```

The idea of transformed OLS is to take the mean of the log and fit a linear model for that:

```{r}
mean_log <- mean(log(s))
mean_log
```

When I exponentiate the log of the mean, I get back the original arithmetic mean.
This is what Poisson is doing:

```{r}
exp(log_mean)
```

When I exponentiate the mean of the log, I get back the original geometric mean.
This is what transformed OLS is doing:

```{r}
exp(mean_log)
```

## Further Reading

<https://www.theanalysisfactor.com/the-difference-between-link-functions-and-data-transformations/>
