---
title: "Extrating information from fitted `lmer` models using base R"
author: "Luke Miratrix"
editor: 
  markdown: 
    wrap: sentence
---

```{r loadlibs, echo=FALSE, message=FALSE, warning=FALSE}
library( lme4 )
library( foreign )
library( arm )
library( tidyverse )
knitr::opts_chunk$set(comment = "")
knitr::opts_chunk$set(width=100)

```

This chapter follows Chapter @sec-broom, and provides an alternate set of ways of pulling information from a fit `lmer` model.
In particular, this document walks through various R code to pull information out of a multilevel model (and OLS models as well, since the methods generally work on everything).
For illustration, we will use a random-slope model on the HS&B dataset with some level 1 and level 2 fixed effects.

We use the following libraries in this file:

```{r}
library( lme4 )
library( foreign ) ## to load data
library( arm )
library( tidyverse )
```

Loading the data is simple.
We read student and school level data and merge:

```{r, warning=FALSE}
dat = read.spss( "data/hsb1.sav", to.data.frame=TRUE )
sdat = read.spss( "data/hsb2.sav", to.data.frame=TRUE )
dat = merge( dat, sdat, by="id", all.x=TRUE )
head( dat, 3 )
```

## Fitting and viewing the model

Now we fit the random slope model with the level-2 covariates:

```{r}
M1 = lmer( mathach ~ 1 + ses + meanses + (1 + ses|id), data=dat )
```

If we just print the object, e.g., by typing the name of the model on the console, we get minimal information:

```{r}
M1
```

### The `display()` method

The `arm` package's `display()` method gives an overview of what our fitted model is:

```{r}
display( M1 )
```

### The `summary()` method

We can also look at the messier default `summary()` command, which gives you more output.
The real win is if we use the `lmerTest` library and fit our model with that package loaded, our `summary()` is more exciting and has $p$-values:

```{r, warnings=FALSE, message=FALSE}
library( lmerTest )
M1 = lmer( mathach ~ 1 + ses + meanses + (1 + ses|id), data=dat )
summary( M1 )
```

## Obtaining Fixed Effects

R thinks of all models in reduced form.
Thus when we get the fixed effects we get both the level-1 and level-2 fixed effects all together:

```{r}
fixef( M1 )
```

The above is a vector of numbers.
Each element is named, but we can index them as so:

```{r}
fixef( M1 )[2]
```

We can also use the `[[]]` which means "give me that element not as a list but as just the element!" When in doubt, if you want one thing out of a list or vector, use `[[]]` instead of `[]`:

```{r}
fixef( M1 )[[2]]
```

See how it gives you the number without the name here?

## Obtaining Variance and Covariance estimates

We can get the Variance-Covariance matrix of the random effects with `VarCorr`.

```{r}
VarCorr( M1 )
```

It displays nicely if you just print it out, but inside it are covariance matrices for each random effect group.
(In our model we only have one group, `id`.) These matrices also have correlation matrices for reference.
Here is how to get these pieces:

```{r}
vc = VarCorr( M1 )$id
vc
```

You might be wondering what all the `attr` stuff is.
R can "tack on" extra information to a variable via "attributes".
Attributes are not part of the variable exactly, but they follows their variable around.
The `attr` (for attribute) method is a way to get these extra bits of information.
In the above, R is tacking the correlation matrix on to the variance-covariance matrix to save you the trouble of calculating it yourself.
Get it as follows:

```{r}
attr( vc, "correlation" )
```

You can also just use the `vc` object as a matrix.
Here we take the diagonal of it

```{r}
diag( vc )
```

If you want an element from a matrix use row-column indexing like so:

```{r}
vc[1,2]
```

for row 1 and column 2.

#### The `sigma.hat()` and `sigma()` methods

If you just want the variances and standard deviations of your random effects, use `sigma.hat()`.
This also gives you the residual standard deviation as well.
The output is a weird object, with a list of things that are themselves lists in it.
Let's examine it.
First we look at what the whole thing is:

```{r}
sigma.hat( M1 )
names( sigma.hat( M1 ) )
sigma.hat( M1 )$sigma
```

Our standard deviations of the random effects are

```{r}
sigma.hat( M1 )$sigma$id
```

We can get our residual variance by this weird thing (we are getting `data` from the `sigma` inside of `sigma.hat( M1 )`):

```{r}
sigma.hat( M1 )$sigma$data
```

But here is an easier way using the `sigma()` utility function:

```{r}
sigma( M1 )
```

## Obtaining Empirical Bayes Estimates of the Random Effects

Random effects come out of the `ranef()` method.
Each random effect is its own object inside the returned object.
You refer to these sets of effects by name.
Here our random effect is called `id`.

```{r}
ests = ranef( M1 )$id
head( ests )
```

Generally, what you get back from these calls is a new data frame with a row for each group.
The rows are named with the original id codes for the groups, but if you want to connect it back to your group-level information you are going to want to merge stuff.
To do this, and to keep things organized, I recommend adding the id as a column to your dataframe:

```{r}
names(ests) = c( "u0", "u1" )
ests$id = rownames( ests )
head( ests )
```

We also renamed our columns of our dataframe to give them names nicer than `(Intercept)`.
You can use these names if you wish, however.
You just need to quote them with back ticks (this code is not run):

```{r, eval=FALSE}
head( ests$`(Intercept)` )
```

### The `coef()` method

We can also get a slighly different (but generally easier to use) version these things through `coef()`.
What `coef()` does is give you the estimated regression lines for each group in your data by combining the random effect for each group with the corresponding fixed effects.
Note how in the following the `meanses` coefficient is the same, but the others vary due to the random slope and random intercept.

```{r}
coefs = coef( M1 )$id
head( coefs )
```

Note that if we have level 2 covariates in our model, they are not incorperated in the intercept and slope via `coef()`.
We have to do that by hand:

```{r}
names( coefs ) = c( "beta0.adj", "beta.ses", "beta.meanses" )
coefs$id = rownames( coefs )
coefs = merge( coefs, sdat, by="id" )
coefs = mutate( coefs, beta0 = beta0.adj + beta.meanses * meanses )
coefs$beta.meanses = NULL
```

Here we added in the impact of mean ses to the intercept (as specified by our model).
Now if we look at the intercepts (the beta0 variables) they will incorperate the level 2 covariate effects.
If we then plotted a line using beta0 and beta.ses for each school, we would get the estimated lines for each school including the school-level covariate impacts.

## Obtaining standard errors

We can get an object with all the standard errors of the coefficients, including the individual Emperical Bayes estimates for the individual random effects.
This is a lot of information.
We first look at the Standard Errors for the fixed effects, and then for the random effects.
Standard errors for the variance terms are not given (this is tricker to calculate).

### Fixed effect standard errors

```{r}
ses = se.coef( M1 )
names( ses )
```

Our fixed effect standard errors:

```{r}
ses$fixef
```

You can also get the uncertainty estimates of your fixed effects as a variance-covariance matrix:

```{r}
vcov( M1 )
```

The standard errors are the diagonal of this matrix, square-rooted.
See how they line up?:

```{r}
sqrt( diag( vcov( M1 ) ) )
```

### Random effect standard errors

Our random effect standard errors for our EB estimates:

```{r}
head( ses$id )
```

Warning: these come as a matrix, not data frame.
It is probably best to do this:

```{r}
SEs = as.data.frame( se.coef( M1 )$id )
head( SEs )
```

## Generating confidence intervals

We can compute profile confidence intervals (warnings have been suppressed)

```{r, warning=FALSE, message=FALSE, cache=TRUE}
confint( M1 )
```

## Obtaining fitted values

Fitted values are the predicted value for each individual given the model.

```{r}
yhat = fitted( M1 )
head( yhat )
```

Residuals are the difference between predicted and observed:

```{r}
resids = resid( M1 )
head( resids )
```

We can also predict for hypothetical new data.
Here we predict the outcome for a random student with ses of -1, 0, and 1 in a school with mean ses of 0:

```{r}
ndat = data.frame( ses = c( -1, 0, 1 ), meanses=c(0,0,0), id = -1 )
predict( M1, newdata=ndat, allow.new.levels=TRUE )
```

The `allow.new.levels=TRUE` bit says to predict for a new school (our fake school id of -1 in `ndat` above).
In this case it assumes the new school is typical, with 0s for the random effect residuals.

If we predict for a current school, the random effect estimates are incorporated:

```{r}
ndat$id = 1296
predict( M1, newdata=ndat )
```

## Appendix: the guts of the object

When we fit our model and store it in a variable, R stores *a lot* of stuff.
The following lists some other functions that pull out bits and pieces of that stuff.

First, to get the model matrix (otherwise called the design matrix)

```{r}
mm = model.matrix( M1 )
head( mm )
```

This can be useful for predicting individual group mean outcomes, for example.

We can also ask questions such as number of groups, number of individuals:

```{r}
ngrps( M1 )
nobs( M1 )
```

We can list all methods for the object (`merMod` is a more generic version of `lmerMod` and has a lot of methods we can use)

```{r}
class( M1 )
methods(class = "lmerMod")
methods(class = "merMod")
```
